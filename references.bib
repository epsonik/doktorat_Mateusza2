@article{10.1145/3545572,
 address = {New York, NY, USA},
 articleno = {76},
 author = {Jabeen, Summaira and Li, Xi and Amin, Muhammad Shoib and Bourahla, Omar and Li, Songyuan and Jabbar, Abdul},
 doi = {10.1145/3545572},
 file = {:PDF/2202.09195v1.pdf:PDF},
 groups = {review},
 issn = {1551-6857},
 issue_date = {April 2023},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 keywords = {Deep learning, multimedia, multimodal learning, datasets, neural networks, survey},
 number = {2s},
 numpages = {41},
 publisher = {Association for Computing Machinery},
 title = {A Review on Methods and Applications in Multimodal Deep Learning},
 volume = {19},
 year = {2023}
}

@inproceedings{1Chen2017,
 address = {New York, NY, USA},
 author = {Chen, Fuhai and Ji, Rongrong and Su, Jinsong and Wu, Yongjian and Wu, Yunsheng},
 booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
 doi = {10.1145/3123266.3123275},
 file = {:PDF/3123266.3123275.pdf:PDF},
 groups = {global CNN features},
 isbn = {9781450349062},
 keywords = {visual relation, structured learning, image captioning, deep learning},
 location = {Mountain View, California, USA},
 numpages = {9},
 pages = {46–54},
 publisher = {Association for Computing Machinery},
 series = {MM '17},
 title = {StructCap: Structured Semantic Embedding for Image Captioning},
 url = {https://doi.org/10.1145/3123266.3123275},
 year = {2017}
}

@inproceedings{7410648,
 author = {Mao, Junhua and Wei, Xu and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
 booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2015.291},
 file = {:PDF/Learning_Like_a_Child_Fast_Novel_Visual_Concept_Learning_from_Sentence_Descriptions_of_Images.pdf:PDF},
 groups = {review},
 issn = {2380-7504},
 keywords = {Adaptation models;Visualization;Semantics;Dictionaries;Computer vision;Computational modeling;Training},
 pages = {2533-2541},
 title = {Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images},
 year = {2015}
}

@inproceedings{9428453,
 author = {Li, Tong and Hu, Yunhui and Wu, Xinxiao},
 booktitle = {2021 IEEE International Conference on Multimedia and Expo (ICME)},
 comment = {https://github.com/ezeli/InSentiCap_model
https://github.com/ezeli/BUTD_model},
 doi = {10.1109/ICME51207.2021.9428453},
 file = {:PDF/Image Captioning with Inherent Sentiment.pdf:PDF},
 pages = {1-6},
 title = {Image Captioning with Inherent Sentiment},
 year = {2021}
}

@inproceedings{9431465,
 author = {Atliha, Viktar and Šešok, Dmitrij},
 booktitle = {2021 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)},
 doi = {10.1109/eStream53087.2021.9431465},
 file = {:PDF/Pretrained_Word_Embeddings_for_Image_Captioning.pdf:PDF},
 pages = {1-4},
 title = {Pretrained Word Embeddings for Image Captioning},
 year = {2021}
}

@inproceedings{Abella1999From,
 author = {A. {Abella} and J. R. {Kender}},
 booktitle = {Proceedings Integration of Speech and Image Understanding},
 doi = {10.1109/ISIU.1999.824875},
 file = {:PDF/From_images_to_sentences_Abella_1999.pdf:PDF},
 pages = {117-146},
 title = {From images to sentences via spatial relations},
 year = {1999}
}

@article{Afridi2020Multimodal,
 author = {Tariq Habib Afridi and Aftab Alam and Muhammad Numan Khan and Jawad Khan and Young{-}Koo Lee},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2009-08395.bib},
 eprint = {2009.08395},
 eprinttype = {arXiv},
 file = {:PDF/A Multimodal Memes Classification A Surveyand Open Research Issues.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 23 Sep 2020 15:51:46 +0200},
 title = {A Multimodal Memes Classification: {A} Survey and Open Research Issues},
 url = {https://arxiv.org/abs/2009.08395},
 volume = {abs/2009.08395},
 year = {2020}
}

@article{Agarwal2018Recent,
 archiveprefix = {arXiv},
 author = {Shivang Agarwal and Jean Ogier du Terrail and Frederic Jurie},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1809-03193.bib},
 eprint = {1809.03193},
 file = {:PDF/Recent_Advances_Agarval_2018.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 18 Sep 2019 08:24:15 +0200},
 title = {Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks},
 url = {http://arxiv.org/abs/1809.03193},
 volume = {abs/1809.03193},
 year = {2018}
}

@article{Agarwal2023,
 author = {Agarwal, Lakshita and Verma, Bindu},
 doi = {10.1007/s11042-023-16560-x},
 file = {:PDF/s11042-023-16560-x-1.pdf:PDF},
 groups = {review},
 journal = {Multimedia Tools and Applications},
 pages = {1-47},
 title = {From methods to datasets: A survey on Image-Caption Generators},
 volume = {83},
 year = {2023}
}

@misc{aggarwal2019aiautomateendtoenddata,
 archiveprefix = {arXiv},
 author = {Charu Aggarwal and Djallel Bouneffouf and Horst Samulowitz and Beat Buesser and Thanh Hoang and Udayan Khurana and Sijia Liu and Tejaswini Pedapati and Parikshit Ram and Ambrish Rawat and Martin Wistuba and Alexander Gray},
 eprint = {1910.14436},
 file = {:PDF/1910.14436v1.pdf:PDF;:PDF/1910.14436v1.pdf:PDF},
 primaryclass = {cs.AI},
 title = {How can AI Automate End-to-End Data Science?},
 url = {https://arxiv.org/abs/1910.14436},
 year = {2019}
}

@article{Agrawal2019Nocaps,
 author = {Harsh Agrawal and Karan Desai and Yufei Wang and Xinlei Chen and Rishabh Jain and Mark Johnson and Dhruv Batra and Devi Parikh and Stefan Lee and Peter Anderson},
 journal = {International Conference on Computer Vision},
 pages = {8947-8956},
 title = {nocaps: novel object captioning at scale},
 year = {2019}
}

@inproceedings{Aker2010Generating,
 author = {Aker, Ahmet and Gaizauskas, Robert},
 booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
 file = {:PDF/Generating image descriptions using dependency relational patterns.pdf:PDF},
 location = {Uppsala, Sweden},
 numpages = {9},
 pages = {1250–1258},
 series = {ACL '10},
 title = {Generating Image Descriptions Using Dependency Relational Patterns},
 year = {2010}
}

@misc{akkus2023multimodaldeeplearning,
 archiveprefix = {arXiv},
 author = {Cem Akkus and Luyang Chu and Vladana Djakovic and Steffen Jauch-Walser and Philipp Koch and Giacomo Loss and Christopher Marquardt and Marco Moldovan and Nadja Sauter and Maximilian Schneider and Rickmer Schulte and Karol Urbanczyk and Jann Goschenhofer and Christian Heumann and Rasmus Hvingelby and Daniel Schalk and Matthias Aßenmacher},
 eprint = {2301.04856},
 file = {:PDF/2301.04856v1.pdf:PDF},
 groups = {review, word2vec},
 primaryclass = {cs.CL},
 title = {Multimodal Deep Learning},
 url = {https://arxiv.org/abs/2301.04856},
 year = {2023}
}

@article{Al2024Review,
 author = {Al-Shamayleh, Ahmad and Adwan, Omar and Alsharaiah, Mohammad and Hussein, Abdelrahman and Kharma, Qasem and Eke, Christopher},
 doi = {10.1007/s11042-024-18307-8},
 file = {:PDF/s11042-024-18307-8-1.pdf:PDF},
 groups = {review},
 journal = {Multimedia Tools and Applications},
 pages = {1-50},
 title = {A comprehensive literature review on image captioning methods and metrics based on deep learning technique},
 volume = {83},
 year = {2024}
}

@article{Alassi2013Effectiveness,
 address = {USA},
 author = {Alassi, Derar and Alhajj, Reda},
 doi = {10.1016/j.ins.2012.07.022},
 file = {:PDF/Effectiveness of template detection on noise reductionand websites summarization.pdf:PDF},
 issn = {0020-0255},
 issue_date = {January, 2013},
 journal = {Inf. Sci.},
 keywords = {Template detection, Noise detection, Website summarization, Web mining},
 numpages = {32},
 pages = {41–72},
 publisher = {Elsevier Science Inc.},
 title = {Effectiveness of Template Detection on Noise Reduction and Websites Summarization},
 url = {https://doi.org/10.1016/j.ins.2012.07.022},
 volume = {219},
 year = {2013}
}

@article{Allen1983Maintaining,
 address = {New York, NY, USA},
 author = {Allen, James F.},
 doi = {10.1145/182.358434},
 file = {:PDF/Knowledge_temporal_Allen_1983.pdf:PDF},
 issn = {0001-0782},
 issue_date = {Nov. 1983},
 journal = {Commun. ACM},
 keywords = {interval reasoning, interval representation, temporal interval},
 number = {11},
 numpages = {12},
 pages = {832–843},
 publisher = {Association for Computing Machinery},
 title = {Maintaining Knowledge about Temporal Intervals},
 url = {https://doi.org/10.1145/182.358434},
 volume = {26},
 year = {1983}
}

@article{Almutiri2022Markov,
 author = {Talal Almutiri, Farrukh Nadeem},
 file = {:PDF/IJITCS-V14-N2-1.pdf:PDF},
 groups = {lm},
 journal = {Int. J. Inf. Technol. Comput. Sci.},
 number = {2},
 pages = {1--16},
 publisher = {MECS Publisher},
 title = {Markov Models applications in natural language processing: A survey},
 volume = {14},
 year = {2022}
}

@article{Alshattnawi2024,
 article-number = {2254},
 author = {Alshattnawi, Sawsan and Shatnawi, Amani and AlSobeh, Anas M.R. and Magableh, Aws A.},
 doi = {10.3390/app14062254},
 file = {:PDF/applsci-14-02254-v3.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {6},
 title = {Beyond Word-Based Model Embeddings: Contextualized Representations for Enhanced Social Media Spam Detection},
 url = {https://www.mdpi.com/2076-3417/14/6/2254},
 volume = {14},
 year = {2024}
}

@misc{Amirian2019ASR,
 author = {Amirian,Soheyla and Rasheed,Khaled and Taha,Thiab R. and Arabnia,Hamid R.},
 file = {:PDF/A Short Review on Image Caption Generation with Deep Learning.pdf:PDF},
 journal = {Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV)},
 keywords = {Computers; Research; Language; International conferences; Software; Search engines; Datasets; Deep learning; Science; Researchers; Machine translation; Machine learning; Domains; Pattern recognition; Natural language; Visual impairment; Big Data; Neural networks; Images; Methods; Linguistics; Vision systems; Semantics; Disabled people},
 language = {English},
 note = {Name - Google Inc; Copyright - Copyright The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp) 2019; Last updated - 2020-01-27},
 pages = {10-18},
 title = {A Short Review on Image Caption Generation with Deep Learning},
 url = {https://www.proquest.com/conference-papers-proceedings/short-review-on-image-caption-generation-with/docview/2277982436/se-2?accountid=27375},
 year = {2019}
}

@article{An2020Data,
 author = {Qinglong An and Zhengrui Tao and Xingwei Xu and Mohamed {El Mansori} and Ming Chen},
 doi = {10.1016/j.measurement.2019.107461},
 file = {:PDF/A data-driven model for milling tool remaining useful life prediction with convolutional and stacked LSTM network.pdf:PDF},
 issn = {0263-2241},
 journal = {Measurement},
 keywords = {Tool condition monitoring, Long short-term memory network, Convolutional neural network, Remaining useful life, Cyber-physical system},
 pages = {107461},
 title = {A data-driven model for milling tool remaining useful life prediction with convolutional and stacked LSTM network},
 url = {https://www.sciencedirect.com/science/article/pii/S0263224119313284},
 volume = {154},
 year = {2020}
}

@inproceedings{Anderson2016Spice,
author="Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="SPICE: Semantic Propositional Image Caption Evaluation",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="382--398",
abstract="There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?",
isbn="978-3-319-46454-1"
}

@inproceedings{Anderson2017GuidedOV,
 author = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
 booktitle = {EMNLP},
 file = {:PDF/Guided Open Vocabulary Image Captioning with Constrained Beam Search.pdf:PDF},
 title = {Guided Open Vocabulary Image Captioning with Constrained Beam Search},
 year = {2017}
}

@article{Anderson2018BottomUpAT,
 author = {Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
 comment = {https://github.com/MILVLG/bottom-up-attention.pytorch},
 file = {:PDF/Bottom-Up_and_Top-Down_Attention_for_Image_Captioning_and_Visual_Question_Answering.pdf:PDF},
 groups = {Attention Over Visual Regions, Two-layers and additive attention, attention taxonomy},
 journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {6077-6086},
 printed = {yes},
 title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
 year = {2018}
}

@misc{Anderson2018partiallysupervisedimagecaptioning,
 archiveprefix = {arXiv},
 author = {Peter Anderson and Stephen Gould and Mark Johnson},
 eprint = {1806.06004},
 file = {:PDF/NeurIPS-2018-partially-supervised-image-captioning-Paper.pdf:PDF},
 groups = {Other deep learning methods},
 primaryclass = {cs.CV},
 title = {Partially-Supervised Image Captioning},
 url = {https://arxiv.org/abs/1806.06004},
 year = {2018}
}

@Article{Andrzej2005kultura,
  author  = {Markowski, Andrzej},
  journal = {Teoria. Zagadnienia leksykalne, Wydawnictwo Naukowe PWN, Warszawa},
  title   = {Kultura j{\k{e}}zyka polskiego},
  year    = {2005},
}

@InProceedings{Aneja2018CVPR,
  title={Convolutional image captioning},
  author={Aneja, Jyoti and Deshpande, Aditya and Schwing, Alexander G},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5561--5570},
  year={2018}
}

@article{apidianaki-2023-word,
 address = {Cambridge, MA},
 author = {Apidianaki, Marianna},
 doi = {10.1162/coli_a_00474},
 file = {:PDF/2023.cl-2.7.pdf:PDF},
 journal = {Computational Linguistics},
 number = {2},
 pages = {465--523},
 publisher = {MIT Press},
 title = {From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation},
 url = {https://aclanthology.org/2023.cl-2.7},
 volume = {49},
 year = {2023}
}

@InProceedings{Arneffe2014Universal,
  author    = {de Marneffe, Marie and Dozat, Timothy and Silveira, Natalia and Haverinen, Katri and Ginter, Filip and Nivre, Joakim and Manning, Christopher D.},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}`14)},
  title     = {Universal {S}tanford dependencies: A cross-linguistic typology},
  year      = {2014},
  address   = {Reykjavik, Iceland},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  pages     = {4585--4592},
  publisher = {European Language Resources Association (ELRA)},
  file      = {:PDF/1062_Paper.pdf:PDF},
  url       = {https://aclanthology.org/L14-1045/},
}

@Article{Arshi2024Multimedia,
  author  = {Arshi, Oroos and Dadure, Pankaj},
  journal = {Multimedia Tools and Applications},
  title   = {A comprehensive review of image caption generation},
  year    = {2024},
  pages   = {1-53},
  doi     = {10.1007/s11042-024-20095-0},
}

@article{Atliha2022Compresion,
 article-number = {1638},
 author = {Atliha, Viktar and Šešok, Dmitrij},
 doi = {10.3390/app12031638},
 file = {:PDF/Image-Captioning Model Compression.pdf:PDF},
 groups = {review},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {3},
 title = {Image-Captioning Model Compression},
 url = {https://www.mdpi.com/2076-3417/12/3/1638},
 volume = {12},
 year = {2022}
}

@techreport{Bach2001,
 author = {Bach, Francis R. and Jordan, Michael I.},
 file = {:PDF/kernelICA-jmlr.pdf:PDF},
 number = {UCB/CSD-01-1166},
 title = {Kernel Independent Component Analysis},
 url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2001/5721.html},
 year = {2001}
}

@article{Bach2003Kernel,
 author = {Bach, Francis R. and Jordan, Michael I.},
 doi = {10.1162/153244303768966085},
 file = {:PDF/153244303768966085.pdf:PDF},
 issn = {1532-4435},
 issue_date = {3/1/2003},
 journal = {J. Mach. Learn. Res.},
 keywords = {Stiefel manifold, blind source separation, canonical correlations, gram matrices, incomplete Cholesky decomposition, independent component analysis, integral equations, kernel methods, mutual information, semiparametric models},
 number = {null},
 numpages = {48},
 pages = {1–48},
 publisher = {JMLR.org},
 title = {Kernel independent component analysis},
 volume = {3},
 year = {2003}
}

@misc{Bahdanau2015NeuralMT,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@Article{Bai2018Survey,
  author     = {Bai, Shuang and An, Shan},
  journal    = {Neurocomput.},
  title      = {A survey on automatic image caption generation},
  year       = {2018},
  issn       = {0925-2312},
  number     = {C},
  pages      = {291–304},
  volume     = {311},
  address    = {NLD},
  doi        = {10.1016/j.neucom.2018.05.080},
  file       = {:PDF/elsarticle-template-num.pdf:PDF},
  groups     = {review},
  issue_date = {Oct 2018},
  keywords   = {Image captioning, Sentence template, Deep neural networks, Multimodal embedding, Encoder–decoder framework, Attention mechanism},
  numpages   = {14},
  publisher  = {Elsevier Science Publishers B. V.},
}

@article{Bai2021,
 author = {Cong Bai and Anqi Zheng and Yuan Huang and Xiang Pan and Nan Chen},
 doi = {10.1016/j.displa.2021.102069},
 file = {:PDF/ 0141-9382\:© 2021 Published by Elsevier B.V.Boosting convolutional image captioning with semantic content and visual relationship.pdf:PDF},
 issn = {0141-9382},
 journal = {Displays},
 keywords = {Image captioning, Generative adversarial network, Graph convolution network},
 pages = {102069},
 title = {Boosting convolutional image captioning with semantic content and visual relationship},
 url = {https://www.sciencedirect.com/science/article/pii/S0141938221000792},
 volume = {70},
 year = {2021}
}

@article{Balakrishnan2014Stemming,
 author = {Balakrishnan, Vimala and Ethel, Lloyd-Yemoh},
 doi = {10.7763/LNSE.2014.V2.134},
 file = {:PDF/Stemming and Lemmatization- A Comparison of Retrieval Performances.pdf:PDF},
 journal = {Lecture Notes on Software Engineering},
 pages = {262-267},
 title = {Stemming and Lemmatization: A Comparison of Retrieval Performances},
 volume = {2},
 year = {2014}
}

@inproceedings{Baldi2013Dropout,
 author = {Baldi, Pierre and Sadowski, Peter J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 file = {:PDF/NIPS-2013-understanding-dropout-Paper.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Dropout},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},
 volume = {26},
 year = {2013}
}

@InProceedings{Banerjee2005METEOR,
  author    = {Lavie, Alon and Agarwal, Abhaya},
  booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
  title     = {Meteor: an automatic metric for MT evaluation with high levels of correlation with human judgments},
  year      = {2007},
  address   = {USA},
  pages     = {228–231},
  publisher = {Association for Computational Linguistics},
  series    = {StatMT '07},
  abstract  = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.},
  file      = {:PDF/METEOR- An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.pdf:PDF},
  location  = {Prague, Czech Republic},
  numpages  = {4},
}

@inproceedings{Banner2021Force,
 author = {Deléarde, Robin and Kurtz, Camille and Dejean, Philippe and Wendling, Laurent},
 booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
 doi = {10.1109/ICPR48806.2021.9412316},
 file = {:PDF/Force_Banner_for_the_recognition_of_spatial_relations.pdf:PDF},
 pages = {6065-6072},
 title = {Force Banner for the recognition of spatial relations},
 year = {2021}
}

@article{Barnard2003Matching,
 author = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and Blei, David and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
 doi = {10.1162/153244303322533214},
 file = {:PDF/matching.pdf:PDF},
 journal = {Journal of Machine Learning Research},
 title = {Matching Words and Pictures},
 volume = {3},
 year = {2003}
}

@inproceedings{barraco2022camel,
 author = {Barraco, Manuele and Stefanini, Matteo and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
 booktitle = {International Conference on Pattern Recognition},
 file = {:PDF/2202.10492.pdf:PDF},
 title = {{CaMEL: Mean Teacher Learning for Image Captioning}},
 year = {2022}
}

@article{Barron2017ELU,
 author = {Jonathan T. Barron},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/Barron17a.bib},
 eprint = {1704.07483},
 eprinttype = {arXiv},
 file = {:PDF/1704.07483v1.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
 title = {Continuously Differentiable Exponential Linear Units},
 url = {http://arxiv.org/abs/1704.07483},
 volume = {abs/1704.07483},
 year = {2017}
}

@inproceedings{Bartosiewicz2021Generating,
 author = {Bartosiewicz, Mateusz and Krupińska, Izabela and Bany, Maciej and Konieczna, Anna and Ostrowski, Mateusz and Zalewski, Maciej and Iwanowski, Marcin},
 booktitle = {2021 14th International Conference on Human System Interaction (HSI)},
 doi = {10.1109/HSI52170.2021.9538664},
 file = {:PDF/Generating_image_captions_in_Polish__experimental_study.pdf:PDF},
 groups = {moje},
 pages = {1-6},
 title = {Generating image captions in Polish – experimental study},
 year = {2021}
}

@inproceedings{Bartosiewicz2023Combining,
 author = {Bartosiewicz, Mateusz and Iwanowski, Marcin and Wiszniewska, Martika and Frączak, Karolina and Leśnowolski, Paweł},
 booktitle = {2023 18th Conference on Computer Science and Intelligence Systems (FedCSIS)},
 doi = {10.15439/2023F997},
 file = {:PDF/On_Combining_Image_Features_and_Word_Embeddings_for_Image_Captioning.pdf:PDF},
 groups = {moje},
 keywords = {Training;Measurement;Image coding;Image recognition;Computational modeling;Merging;Predictive models;image captioning;neural image feature extractors;embedding models;LSTM},
 pages = {355-365},
 title = {On Combining Image Features and Word Embeddings for Image Captioning},
 year = {2023}
}

@article{Bartosiewicz2024Improving,
 author = {Mateusz Bartosiewicz and Marcin Iwanowski and Piotr Szczepanski and Karol Zielinski and Albert Ziolkiewicz},
 file = {:PDF/Captioning_ICCVG.pdf:PDF},
 groups = {moje},
 title = {Improving the efficiency of "Show and Tell" encoder decoder image captioning model},
 year = {2024}
}

@article{Bartosiewicz2024Optimal,
 article-number = {504},
 author = {Bartosiewicz, Mateusz and Iwanowski, Marcin},
 doi = {10.3390/info15080504},
 file = {:PDF/information-15-00504.pdf:PDF},
 groups = {moje},
 issn = {2078-2489},
 journal = {Information},
 number = {8},
 title = {The Optimal Choice of the Encoder–Decoder Model Components for Image Captioning},
 volume = {15},
 year = {2024}
}

@misc{Bańko_Mirosław._Red._Narodowy,
 author = {Bańko, Mirosław. Red. and Górski, Rafał L. Red. and Przepiórkowski, Adam. Red. and Lewandowska-Tomaszczyk , Barbara. Red.},
 file = {:PDF/NKJP_ksiazka_poprawiony.pdf:PDF},
 groups = {opis polskiego},
 howpublished = {online},
 keywords = {narodowy korpus polszczyzny, przetwarzanie języków naturalnych},
 language = {pol},
 publisher = {Warszawa : Wydawnictwo Naukowe PWN},
 title = {Narodowy Korpus Języka Polskiego},
 type = {książka}
}

@book{Bellman1961AdaptiveControlProcesses,
 address = {Princeton},
 author = {Richard E. Bellman},
 doi = {doi:10.1515/9781400874668},
 isbn = {9781400874668},
 lastchecked = {2025-03-07},
 publisher = {Princeton University Press},
 title = {Adaptive Control Processes},
 url = {https://doi.org/10.1515/9781400874668},
 year = {1961}
}

@inproceedings{Bender2021StochasticsParrots,
 address = {New York, NY, USA},
 author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
 booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
 doi = {10.1145/3442188.3445922},
 groups = {bledy w SI},
 isbn = {9781450383097},
 location = {Virtual Event, Canada},
 numpages = {14},
 pages = {610–623},
 publisher = {Association for Computing Machinery},
 series = {FAccT 2021},
 title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ��},
 url = {https://doi.org/10.1145/3442188.3445922},
 year = {2021}
}

@article{Bengio2003ProbablisticLM,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
 file = {:PDF/944919.944966.pdf:PDF;:PDF/944919.944966.pdf:PDF},
 groups = {Probabilistic Neural Network Language Model},
 issn = {1532-4435},
 issue_date = {3/1/2003},
 journal = {J. Mach. Learn. Res.},
 number = {null},
 numpages = {19},
 pages = {1137–1155},
 publisher = {JMLR.org},
 title = {A neural probabilistic language model},
 volume = {3},
 year = {2003}
}

@article{Bengio2013RepresentationLearning,
 address = {USA},
 author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
 doi = {10.1109/TPAMI.2013.50},
 file = {:PDF/Representation_Learning_A_Review_and_New_Perspectives (1).pdf:PDF},
 groups = {representation learning},
 issn = {0162-8828},
 issue_date = {August 2013},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 keywords = {Abstracts, Boltzmann machine, Deep learning, Feature extraction, Learning systems, Machine learning, Manifolds, Neural networks, Speech recognition, autoencoder, feature learning, neural nets, representation learning, unsupervised learning},
 number = {8},
 numpages = {31},
 pages = {1798–1828},
 publisher = {IEEE Computer Society},
 title = {Representation Learning: A Review and New Perspectives},
 url = {https://doi.org/10.1109/TPAMI.2013.50},
 volume = {35},
 year = {2013}
}

@article{Beretti2001Efficient,
 author = {S. {Berretti} and A. {Del Bimbo} and E. {Vicario}},
 doi = {10.1109/34.954600},
 file = {:PDF/Efficient_Matching_and_Indexing_of_GraphModels_PAMI_Beretti2001.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {10},
 pages = {1089-1105},
 title = {Efficient matching and indexing of graph models in content-based retrieval},
 volume = {23},
 year = {2001}
}

@article{Bernardi2016,
 author = {Raffaella Bernardi and Ruket {\c{C}}akici and Desmond Elliott and Aykut Erdem and Erkut Erdem and Nazli Ikizler{-}Cinbis and Frank Keller and Adrian Muscat and Barbara Plank},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/BernardiCEEEIKM16.bib},
 eprint = {1601.03896},
 eprinttype = {arXiv},
 file = {:PDF/1601.03896.pdf:PDF},
 groups = {review},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:47:08 +0200},
 title = {Automatic Description Generation from Images: {A} Survey of Models, Datasets, and Evaluation Measures},
 url = {http://arxiv.org/abs/1601.03896},
 volume = {abs/1601.03896},
 year = {2016}
}

@article{Beyer2020Transformers,
 author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
 eprint = {2010.11929},
 eprinttype = {arXiv},
 file = {:PDF/2010.11929v2.pdf:PDF},
 groups = {Vision Transformer., attention taxonomy},
 journal = {CoRR},
 timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
 title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
 url = {https://arxiv.org/abs/2010.11929},
 volume = {abs/2010.11929},
 year = {2020}
}

@misc{Bhatnagar2023EnhancingIC,
 archiveprefix = {arXiv},
 author = {Pooja Bhatnagar and Sai Mrunaal and Sachin Kamnure},
 eprint = {2312.00435},
 file = {:PDF/2312.00435v1.pdf:PDF},
 primaryclass = {cs.CV},
 title = {Enhancing Image Captioning with Neural Models},
 url = {https://arxiv.org/abs/2312.00435},
 year = {2023}
}

@article{Biswas2020,
 author = {Biswas, Rajarshi and Barz, Michael and Sonntag, Daniel},
 day = {01},
 doi = {10.1007/s13218-020-00679-2},
 issn = {1610-1987},
 journal = {KI - K{\"u}nstliche Intelligenz},
 number = {4},
 pages = {571-584},
 title = {Towards Explanatory Interactive Image Captioning Using Top-Down and Bottom-Up Features, Beam Search and Re-ranking},
 url = {https://doi.org/10.1007/s13218-020-00679-2},
 volume = {34},
 year = {2020}
}

@article{Biten2019GoodNews,
 author = {Ali Furkan Biten and Llu{\'i}s G{\'o}mez and Marçal Rusi{\~n}ol and Dimosthenis Karatzas},
 file = {:PDF/Good News, Everyone! Context driven entity-aware captioning for news images.pdf:PDF},
 journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {12458-12467},
 title = {Good News, Everyone! Context Driven Entity-Aware Captioning for News Images},
 year = {2019}
}

@inproceedings{Biten2022Let,
 author = {Biten, Ali Furkan and G\'omez, Llu{\'\i}s and Karatzas, Dimosthenis},
 booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
 comment = {https://github.com/furkanbiten/object-bias},
 file = {:PDF/Let there be a clock on the beach- Reducing Object Hallucination in Image Captioning .pdf:PDF},
 pages = {1381-1390},
 title = {Let There Be a Clock on the Beach: Reducing Object Hallucination in Image Captioning},
 year = {2022}
}

@article{Bloch1999Fuzzy,
 author = {I. {Bloch}},
 doi = {10.1109/34.777378},
 file = {:PDF/Fuzzy_Relative_Position_PAMI_Bloch_1999.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {7},
 pages = {657-664},
 timestamp = {2021-09-13},
 title = {Fuzzy relative position between objects in image processing: a morphological approach},
 volume = {21},
 year = {1999}
}

@article{Bloch2005Fuzzy,
 author = {Isabelle Bloch},
 doi = {10.1016/j.imavis.2004.06.013},
 file = {:PDF/Fuzzy_spatial_review_Bloch2005.pdf:PDF},
 issn = {0262-8856},
 journal = {Image and Vision Computing},
 keywords = {Fuzzy spatial relationships, Degree of intersection, Degree of inclusion, Degree of adjacency, Distances, Directional relative position, Structural pattern recognition, Image interpretation, Spatial reasoning},
 note = {Discrete Geometry for Computer Imagery},
 number = {2},
 pages = {89-110},
 title = {Fuzzy spatial relationships for image processing and interpretation: a review},
 url = {https://www.sciencedirect.com/science/article/pii/S0262885604001490},
 volume = {23},
 year = {2005}
}

@incollection{Bloch2013Fuzziness,
 address = {Berlin, Heidelberg},
 author = {Bloch, Isabelle},
 booktitle = {On Fuzziness: A Homage to Lotfi A. Zadeh -- Volume 1},
 doi = {10.1007/978-3-642-35641-4_8},
 editor = {Seising, Rudolf and Trillas, Enric and Moraga, Claudio and Termini, Settimo},
 file = {:PDF/Bloch2013_Chapter_FuzzyModelsOfSpatialRelationsA.pdf:PDF},
 isbn = {978-3-642-35641-4},
 pages = {51--58},
 publisher = {Springer Berlin Heidelberg},
 title = {Fuzzy Models of Spatial Relations, Application to Spatial Reasoning},
 url = {https://doi.org/10.1007/978-3-642-35641-4_8},
 year = {2013}
}

@inproceedings{BMVC2015_93,
 articleno = {93},
 author = {Dahua Lin and Sanja Fidler and Chen Kong and Raquel Urtasun},
 booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
 doi = {10.5244/C.29.93},
 editor = {Xianghua Xie, Mark W. Jones, and Gary K. L. Tam},
 file = {:PDF/paper093.pdf:PDF},
 groups = {retrieval},
 isbn = {1-901725-53-7},
 numpages = {13},
 pages = {93.1-93.13},
 publisher = {BMVA Press},
 title = {Generating Multi-sentence Natural Language Descriptions of Indoor Scenes},
 url = {https://dx.doi.org/10.5244/C.29.93},
 year = {2015}
}

@article{Bojanowski2017Enriching,
 author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
 doi = {10.1162/tacl_a_00051},
 file = {:PDF/tacl_a_00051.pdf:PDF},
 groups = {word2vec},
 issn = {2307-387X},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {135-146},
 title = {Enriching Word Vectors with Subword Information},
 volume = {5},
 year = {2017}
}

@misc{bonillasalvador2024pixloredatasetdrivenapproachrich,
 archiveprefix = {arXiv},
 author = {Diego Bonilla-Salvador and Marcelino Martínez-Sober and Joan Vila-Francés and Antonio José Serrano-López and Pablo Rodríguez-Belenguer and Fernando Mateo},
 eprint = {2312.05349},
 file = {:PDF/2312.05349v2.pdf:PDF},
 primaryclass = {cs.CV},
 title = {PixLore: A Dataset-driven Approach to Rich Image Captioning},
 url = {https://arxiv.org/abs/2312.05349},
 year = {2024}
}

@article{Borji2023ACA,
 author = {Ali Borji},
 file = {:PDF/2302.03494v8.pdf:PDF},
 groups = {bledy w SI},
 journal = {ArXiv},
 title = {A Categorical Archive of ChatGPT Failures},
 url = {https://api.semanticscholar.org/CorpusID:256627571},
 volume = {abs/2302.03494},
 year = {2023}
}

@inproceedings{bugliarello-elliott-2021-role,
 address = {Online},
 author = {Bugliarello, Emanuele and Elliott, Desmond},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 comment = {https://github.com/e-bug/syncap},
 pages = {593--607},
 publisher = {Association for Computational Linguistics},
 title = {The Role of Syntactic Planning in Compositional Image Captioning},
 url = {https://www.aclweb.org/anthology/2021.eacl-main.48},
 year = {2021}
}

@misc{Cahyono2024automatedimagecaptioningcnns,
      title={Automated Image Captioning with CNNs and Transformers},
      author={Joshua Adrian Cahyono and Jeremy Nathan Jusuf},
      year={2024},
      eprint={2412.10511},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Cai2020PanOptic,
 article-number = {391},
 author = {Cai, Wenjie and Xiong, Zheng and Sun, Xianfang and Rosin, Paul L. and Jin, Longcun and Peng, Xinyi},
 doi = {10.3390/app10010391},
 file = {:PDF/applsci-10-00391.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {1},
 title = {Panoptic Segmentation-Based Attention for Image Captioning},
 url = {https://www.mdpi.com/2076-3417/10/1/391},
 volume = {10},
 year = {2020}
}

@inproceedings{callison-burch-etal-2007-meta,
 address = {Prague, Czech Republic},
 author = {Callison-Burch, Chris and Fordyce, Cameron and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
 booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
 editor = {Callison-Burch, Chris and Koehn, Philipp and Fordyce, Cameron Shaw and Monz, Christof},
 file = {:PDF/W07-0718.pdf:PDF},
 pages = {136--158},
 publisher = {Association for Computational Linguistics},
 title = {(Meta-) Evaluation of Machine Translation},
 url = {https://aclanthology.org/W07-0718},
 year = {2007}
}

@inproceedings{Callison2006Re-evaluating,
 address = {Trento, Italy},
 author = {Callison-Burch, Chris and Osborne, Miles and Koehn, Philipp},
 booktitle = {11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
 file = {:PDF/Re-evaluating the Role of {B}leu in Machine Translation Research.pdf:PDF},
 publisher = {Association for Computational Linguistics},
 title = {Re-evaluating the Role of {B}leu in Machine Translation Research},
 url = {https://www.aclweb.org/anthology/E06-1032},
 year = {2006}
}

@article{Cao2020Comprehensiv,
 author = {Cao, Wenming and Yan, Zhiyue and He, Zhiquan and He, Zhihai},
 doi = {10.1109/ACCESS.2020.2975067},
 file = {:PDF/A_Comprehensive_Survey_on_Geometric_Deep_Learning.pdf:PDF},
 journal = {IEEE Access},
 pages = {35929-35949},
 printed = {yes},
 title = {A Comprehensive Survey on Geometric Deep Learning},
 volume = {8},
 year = {2020}
}

@misc{Cao2022,
 author = {Cao, Min and Li, Shiping and Li, Juntao and Nie, Liqiang and Zhang, Min},
 copyright = {Creative Commons Attribution 4.0 International},
 doi = {10.48550/ARXIV.2203.14713},
 file = {:PDF/Image-text Retrieval- A Survey on Recent Research and Development.pdf:PDF;:PDF/2106.11342v5.pdf:PDF},
 groups = {review},
 keywords = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {Image-text Retrieval: A Survey on Recent Research and Development},
 url = {https://arxiv.org/abs/2203.14713},
 year = {2022}
}

@inproceedings{Carion2020End,
 address = {Cham},
 author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
 booktitle = {Computer Vision -- ECCV 2020},
 editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
 file = {:PDF/End-to-End_Object_Detection_with_Transformers.pdf:PDF},
 isbn = {978-3-030-58452-8},
 pages = {213--229},
 publisher = {Springer International Publishing},
 title = {End-to-End Object Detection with Transformers},
 year = {2020}
}

@article{Celikyilmaz2020EvaluationOT,
 author = {Asli Celikyilmaz and Elizabeth Clark and Jianfeng Gao},
 file = {:PDF/2006.14799v1.pdf:PDF},
 journal = {ArXiv},
 title = {Evaluation of Text Generation: A Survey},
 url = {https://api.semanticscholar.org/CorpusID:220128348},
 volume = {abs/2006.14799},
 year = {2020}
}

@article{Chai_2023,
 author = {Chai, Christine P.},
 doi = {10.1017/S1351324922000213},
 groups = {preprocessing},
 journal = {Natural Language Engineering},
 number = {3},
 pages = {509–553},
 title = {Comparison of text preprocessing methods},
 volume = {29},
 year = {2023}
}

@inproceedings{Changpinyo2021Cc12m,
 address = {Los Alamitos, CA, USA},
 author = {S. Changpinyo and P. Sharma and N. Ding and R. Soricut},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR46437.2021.00356},
 file = {:PDF/Conceptual 12M Pushing Web Scale Image Text PreTraining To Recognize Long-Tail Visual Concepts.pdf:PDF},
 keywords = {visualization;computer vision;image recognition;pipelines;benchmark testing;data collection;knowledge discovery},
 pages = {3557-3567},
 publisher = {IEEE Computer Society},
 title = {Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
 year = {2021}
}

@book{Chapman2010Handbook,
 author = {Indurkhya, Nitin and Damerau, Fred J.},
 edition = {2nd},
 file = {:PDF/Handbook Of Natural Language Processing, Second Edition Chapman and Hall Crc Machine Learning and Pattern Recognition 2010.pdf:PDF;:PDF/Handbook of Natural Language Processing 2nd edition 2010.pdf:PDF},
 groups = {preprocessing},
 isbn = {1420085921},
 publisher = {Chapman \& Hall/CRC},
 title = {Handbook of Natural Language Processing},
 year = {2010}
}

@article{Che2020Visual,
 author = {Che, Wenbin and Fan, Xiaopeng and Xiong, Ruiqin and Zhao, Debin},
 doi = {10.1109/TMM.2019.2954750},
 file = {:PDF/Visual Relationship Embedding Network for Image Paragraph Generation.pdf:PDF},
 journal = {IEEE Transactions on Multimedia},
 number = {9},
 pages = {2307-2320},
 title = {Visual Relationship Embedding Network for Image Paragraph Generation},
 volume = {22},
 year = {2020}
}

@article{Chen1998SmoothingTech,
 author = {Chen, Stanley F. and Joshua Goodman},
 file = {:PDF/tr-10-98.pdf:PDF},
 groups = {lm},
 journal = {Harvard Computer Science Group Technical Report TR-10-98.},
 title = {. An Empirical Study of Smoothing Techniquesfor Language Modeling.},
 year = {1998}
}

@Misc{Chen2015microsoftCOCO,
  author        = {Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
  title         = {Microsoft COCO Captions: Data Collection and Evaluation Server},
  year          = {2015},
  archiveprefix = {arXiv},
  eprint        = {1504.00325},
  file          = {:PDF/1504.00325v2.pdf:PDF},
  primaryclass  = {cs.CV},
}

@inproceedings{Chen2015Minds,
 address = {Los Alamitos, CA, USA},
 author = {Chen, Xinlei and Zitnick, C. Lawrence},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7298856},
 file = {:PDF/cvpr15_rnn.pdf:PDF},
 groups = {multimodal, global CNN features},
 issn = {1063-6919},
 pages = {2422-2431},
 publisher = {IEEE Computer Society},
 title = {{ Mind's eye: A recurrent visual representation for image caption generation }},
 year = {2015}
}

@InProceedings{Chen2017SCACNN,
  author    = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning}},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {6298-6306},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.667},
  file      = {:PDF/2101.09698v1.pdf:PDF;:PDF/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf:PDF},
  groups    = {single layer LSTM, Multi-level features},
  issn      = {1063-6919},
  keywords  = {Visualization;Semantics;Feature extraction;Image coding;Neural networks;Detectors},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.667},
}

@inproceedings{Chen2018GroupCap,
 author = {Chen, Fuhai and Ji, Rongrong and Sun, Xiaoshuai and Wu, Yongjian and Su, Jinsong},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2018.00146},
 file = {:PDF/GroupCap_Group-Based_Image_Captioning_with_Structured_Relevance_and_Diversity_Constraints.pdf:PDF},
 issn = {2575-7075},
 keywords = {Visualization;Correlation;Semantics;Feature extraction;Training;Adaptation models;Task analysis},
 pages = {1345-1353},
 title = {GroupCap: Group-Based Image Captioning with Structured Relevance and Diversity Constraints},
 year = {2018}
}

@inproceedings{Chen2018Regularizing,
 address = {Los Alamitos, CA, USA},
 author = {X. Chen and L. Ma and W. Jiang and J. Yao and W. Liu},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 comment = {https://github.com/chenxinpeng/ARNet},
 doi = {10.1109/CVPR.2018.00834},
 file = {:PDF/Chen_Regularizing_RNNs_for_CVPR_2018_paper.pdf:PDF},
 groups = {Additive attention over a grid of features, hidden state reconstruction},
 keywords = {computer vision;pattern recognition},
 pages = {7995-8003},
 publisher = {IEEE Computer Society},
 title = {Regularizing RNNs for Caption Generation by Reconstructing the Past with the Present},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00834},
 year = {2018}
}

@inproceedings{Chen2020Uniter,
 author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
 booktitle = {European Conference on Computer Vision},
 doi = {https://link.springer.com/chapter/10.1007/978-3-030-58577-8_7},
 file = {:PDF/Uniter Universal image-text representation learning.pdf:PDF},
 organization = {Springer International Publishing},
 pages = {104--120},
 title = {Uniter: Universal image-text representation learning},
 url = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750103.pdf},
 year = {2020}
}

@inproceedings{Chen2021BestPooling,
 author = {Chen, Jiacheng and Hu, Hexiang and Wu, Hao and Jiang, Yuning and Wang, Changhu},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/Learning the Best Pooling Strategy for Visual Semantic Embedding.pdf:PDF},
 pages = {15789-15798},
 title = {Learning the Best Pooling Strategy for Visual Semantic Embedding},
 year = {2021}
}

@inproceedings{Chen2021Human,
 author = {Chen, Long and Jiang, Zhihong and Xiao, Jun and Liu, Wei},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 comment = {)https://github.com/mad-red/VSR-guided-CIC
Zalety:
zróżnicowane opisy oraz balans miedzy jakoscia i zroznicowaniem
-wiele rozwiązań koder-dekoder osiągneło super wyniki, ale produkuja powtarzalne generyczne zdania
-nie ma mozliwosci skupienia sie na obszarze zainteresowania (tak jak to robi czlowiek)
-dzieki modelom CIC( Controllable Image Captioning) mozemy produkowac zróżnicowane opisy poprzez podanie różnych sygnałów sterujących
-standardowe modle CIC skupiaja sie na sentymencie, emocjach, osobie w zdaniu np. styl zdania(strona bierna). Jednak wciaz trudne jest scisle kontrolowanie
Nowe rozwiązanie:
Verb-specific Semantic Roles (VSR) - zorientowany na zdarzenia  obiektowy sygnal . Pozwala dostsowac sie do zdarzenia oraz specyficznego przykładu
Etapy:
1. trenowanie modelu GSRL(grounded semantic role labeling) - identyfikacja oraz oznaczenie wszystkich encji dla rół
2. SSP(semantic structure planner) -  uszeregowanie czasownikow i roli semantycznych w celu otrzymania  struktur semantycznych bliskich opisowi ludzkiemu
3. połączenie oznaczonych encji oraz struktur semantycznych oraz uzycie tego rezultatu w sieci RNN. Sieć RNNzamienia role poprzez sekwenycjne skupianie sie na roznych rolach
czasownik jedzenie
role semantyczne dla powyzszego czasownika - jedzenie(naleśnik), pojemnik(talerz), narzedzie(widelec)
encje - czlowiek, nalesnik,
Metryki:
sprawdzanie wygenerowanego podpisu i porownanie z referencyjnym
bleu-4 meteor, rouge, CIDEr-D, SPICE},
 file = {:PDF/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf:PDF},
 pages = {16846-16856},
 timestamp = {2021-10-28},
 title = {Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles},
 year = {2021}
}

@article{Chen2021MemorizedKnowledge,
 author = {Hui Chen and Guiguang Ding and Zijia Lin and Yuchen Guo and Caifeng Shan and Jungong Han},
 doi = {10.1007/s12559-019-09656-w},
 issn = {1866-9956},
 journal = {Cognitive Computation},
 keywords = {Attention, Encoder-decoder, Image captioning, Memory},
 language = {English},
 number = {4},
 pages = {807--820},
 publisher = {Springer International Publishing},
 title = {Image captioning with memorized knowledge},
 volume = {13},
 year = {2021}
}

@article{Chen2021Visualgpt,
 author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
 comment = {https://github.com/Vision-CAIR/VisualGPT},
 file = {:PDF/VisualGPT- Data-efficient Adaptation of Pretrained Language Models for Image Captioning.pdf:PDF},
 journal = {arXiv preprint arXiv:2102.10407},
 title = {VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning},
 year = {2021}
}

@inproceedings{chen2023pointgpt,
 address = {Red Hook, NY, USA},
 articleno = {1291},
 author = {Chen, Guangyan and Wang, Meiling and Yang, Yi and Yu, Kai and Yuan, Li and Yue, Yufeng},
 booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
 file = {:PDF/2511_pointgpt_auto_regressively_gen.pdf:PDF},
 location = {New Orleans, LA, USA},
 numpages = {13},
 publisher = {Curran Associates Inc.},
 series = {NIPS '23},
 title = {PointGPT: auto-regressively generative pre-training from point clouds},
 url = {https://openreview.net/forum?id=rqE0fEQDqs},
 year = {2024}
}

@article{Chen_2021,
 author = {Feng Chen and Xinyi Li and Jintao Tang and Shasha Li and Ting Wang},
 doi = {10.1088/1742-6596/1914/1/012053},
 file = {:PDF/A Survey on Recent Advances in Image Captioning.pdf:PDF},
 journal = {Journal of Physics: Conference Series},
 number = {1},
 pages = {012053},
 publisher = {{IOP} Publishing},
 title = {A Survey on Recent Advances in Image Captioning},
 url = {https://doi.org/10.1088/1742-6596/1914/1/012053},
 volume = {1914},
 year = {2021}
}

@article{Chen_Luo_2020,
 abstractnote = {Existing image-text matching approaches typically infer the similarity of an image-text pair by capturing and aggregating the affinities between the text and each independent object of the image. However, they ignore the connections between the objects that are semantically related. These objects may collectively determine whether the image corresponds to a text or not. To address this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN) which processes images and sentences symmetrically by recurrent neural networks (RNN). In particular, given an input image-text pair, our model reorders the image objects based on the positions of their most related words in the text. In the same way as extracting the hidden features from word embeddings, the model leverages RNN to extract high-level object features from the reordered object inputs. We validate that the high-level object features contain useful joint information of semantically related objects, which benefit the retrieval task. To compute the image-text similarity, we incorporate a Multi-attention Cross Matching Model into DP-RNN. It aggregates the affinity between objects and words with cross-modality guided attention and self-attention. Our model achieves the state-of-the-art performance on Flickr30K dataset and competitive performance on MS-COCO dataset. Extensive experiments demonstrate the effectiveness of our model.},
 author = {Chen, Tianlang and Luo, Jiebo},
 doi = {10.1609/aaai.v34i07.6631},
 file = {:PDF/Expressing Objects Just Like Words- Recurrent Visual Embedding for Image-Text Matching .pdf:PDF},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {07},
 pages = {10583-10590},
 title = {Expressing Objects Just Like Words: Recurrent Visual Embedding for Image-Text Matching},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/6631},
 volume = {34},
 year = {2020}
}

@misc{Cheng2021GeometryEntangled,
 archiveprefix = {arXiv},
 author = {Ling Cheng and Wei Wei and Feida Zhu and Yong Liu and Chunyan Miao},
 eprint = {2109.14137},
 file = {:PDF/Geometry-Entangled Visual Semantic Transformer for Image Captioning.pdf:PDF},
 primaryclass = {cs.CV},
 printed = {yes},
 title = {Geometry-Entangled Visual Semantic Transformer for Image Captioning},
 year = {2021}
}

@inproceedings{Cho2014Learning,
 address = {Doha, Qatar},
 author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
 doi = {10.3115/v1/D14-1179},
 file = {:PDF/Learning_Phrase_Representations_using_RNN_Encoder-.pdf:PDF},
 groups = {lm},
 pages = {1724--1734},
 publisher = {Association for Computational Linguistics},
 title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
 year = {2014}
}

@inproceedings{Cho2014Properties,
 address = {Doha, Qatar},
 author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
 booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
 doi = {10.3115/v1/W14-4012},
 editor = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
 file = {:PDF/W14-4012.pdf:PDF},
 groups = {lm},
 pages = {103--111},
 publisher = {Association for Computational Linguistics},
 title = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
 year = {2014}
}

@inproceedings{cho2021vlt5,
 author = {Jaemin Cho and Jie Lei and Hao Tan and Mohit Bansal},
 booktitle = {ICML},
 file = {:PDF/2102.02779.pdf:PDF},
 title = {Unifying Vision-and-Language Tasks via Text Generation},
 year = {2021}
}

@inproceedings{Chollet2017Xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}

@inproceedings{Chorowski2015AttentionBasedMF,
 author = {Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and Kyunghyun Cho and Yoshua Bengio},
 booktitle = {NIPS},
 file = {:PDF/Attention-Based Models for Speech Recognition.pdf:PDF},
 title = {Attention-Based Models for Speech Recognition},
 year = {2015}
}

@article{chu2020automatic,
 author = {Chu, Yan and Yue, Xiao and Yu, Lei and Sergei, Mikhailov and Wang, Zhengkui},
 file = {:PDF/Automatic Image Captioning Based on ResNet50 and LSTM with Soft Attention.pdf:PDF},
 journal = {Wireless Communications and Mobile Computing},
 pages = {1--7},
 publisher = {Hindawi Limited},
 title = {Automatic image captioning based on ResNet50 and LSTM with soft attention},
 volume = {2020},
 year = {2020}
}

@article{Ciaramella2006Fuzzy,
 author = {Ciaramella, A. and Tagliaferri, R. and Pedrycz, W. and Di Nola, A.},
 document_type = {Conference Paper},
 doi = {10.1016/j.ijar.2005.06.016},
 file = {:PDF/Fuzzy-relational-neural-network2006International-Journal-of-Approximate-Reasoning.pdf:PDF},
 journal = {International Journal of Approximate Reasoning},
 note = {cited By 45},
 number = {2},
 pages = {146-163},
 source = {Scopus},
 title = {Fuzzy relational neural network},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844466785&doi=10.1016%2fj.ijar.2005.06.016&partnerID=40&md5=4c5673f9003e0c659bd5d93d0bb44002},
 volume = {41},
 year = {2006}
}

@article{Clement2018Learning,
 author = {Clement, M. and Kurtz, C. and Wendling, L.},
 document_type = {Article},
 doi = {10.1016/j.patcog.2018.06.017},
 file = {:PDF/Learning-spatial-relations-and-shapes-for-structural-object-description-and-scene-recognition2018Pattern-Recognition.pdf:PDF},
 journal = {Pattern Recognition},
 note = {cited By 11},
 pages = {197-210},
 source = {Scopus},
 title = {Learning spatial relations and shapes for structural object description and scene recognition},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050152123&doi=10.1016%2fj.patcog.2018.06.017&partnerID=40&md5=0d989b79c5df6e6734909c32381cd7c0},
 volume = {84},
 year = {2018}
}

@inproceedings{Cohen2019Beam,
 author = {Cohen, Eldan and Beck, Christopher},
 booktitle = {Proceedings of the 36th International Conference on Machine Learning},
 editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
 groups = {beam},
 pages = {1290--1299},
 pdf = {http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models},
 url = {https://proceedings.mlr.press/v97/cohen19a.html},
 volume = {97},
 year = {2019}
}

@article{Cohn1997Qualititative,
 author = {Anthony G. Cohn and Brandon Bennett and John Gooday and Nicholas Mark Gotts},
 file = {:PDF/Cohn1997_Article_QualitativeSpatialRepresentati.pdf:PDF},
 journal = {Geoinformatica},
 pages = {275-316},
 title = {Qualitative Spatial Representation and Reasoning with the Region Connection Calculus},
 url = {https://www.researchgate.net/publication/2659953_Qualitative_Spatial_Representation_and_Reasoning_with_the_Region_Connection_Calculus},
 volume = {1},
 year = {1997}
}

@article{Cohn2001Qualititative,
 author = {Cohn, Anthony and Hazarika, Shyamanta},
 file = {:PDF/FI-paper-2001.pdf:PDF},
 journal = {Fundam. Inform.},
 pages = {1-29},
 title = {Qualitative Spatial Representation and Reasoning: An Overview},
 url = {https://www.researchgate.net/publication/220444929_Qualitative_Spatial_Representation_and_Reasoning_An_Overview},
 volume = {46},
 year = {2001}
}

@book{Cormen2024WprowadzenieDoAlgorytmow,
 author = {Cormen, Thomas H},
 edition = {2},
 editor = {Diks, Krzysztof and Malinowski, Adam},
 file = {:PDF/100000331082_Cormen_WprowadzenieDoAlgorytmow.pdf:PDF},
 isbn = {9788301229603},
 place = {Warszawa},
 publisher = {PWN},
 title = {Wprowadzenie do algorytmów},
 year = {2024}
}

@inproceedings{Cornia2016Saliency,
 author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
 booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
 doi = {10.1109/ICPR.2016.7900174},
 file = {:PDF/A_deep_multi-level_network_for_saliency_prediction.pdf:PDF},
 keywords = {Feature extraction;Computer architecture;Measurement;Encoding;Convolutional codes;Benchmark testing;Observers},
 pages = {3488-3493},
 title = {A deep multi-level network for saliency prediction},
 year = {2016}
}

@article{Cornia2018HumanFixation,
 author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
 doi = {10.1109/TIP.2018.2851672},
 issn = {1941-0042},
 journal = {IEEE Transactions on Image Processing},
 keywords = {Predictive models;Feature extraction;Computer architecture;Computational modeling;Task analysis;Deep learning;Visualization;Saliency;human eye fixations;convolutional neural networks;deep learning},
 number = {10},
 pages = {5142-5154},
 title = {Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model},
 volume = {27},
 year = {2018}
}

@misc{cornia2018payingattentionsaliencyimage,
 archiveprefix = {arXiv},
 author = {Marcella Cornia and Lorenzo Baraldi and Giuseppe Serra and Rita Cucchiara},
 eprint = {1706.08474},
 file = {:PDF/2018_TOMM.pdf:PDF},
 groups = {Exploiting human attention},
 primaryclass = {cs.CV},
 title = {Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention},
 url = {https://arxiv.org/abs/1706.08474},
 year = {2018}
}

@article{Cornia2019ShowCA,
 author = {Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
 comment = {https://github.com/aimagelab/show-control-and-tell},
 file = {:PDF/Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_Grounded_Captions.pdf:PDF},
 groups = {Visual sentinel},
 journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {8299-8308},
 title = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
 year = {2019}
}

@InProceedings{Cornia2020Meshed,
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={Meshed-Memory Transformer for Image Captioning},
  year={2020},
  volume={},
  number={},
  pages={10575-10584},
  abstract={Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M² - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M² Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the "Karpathy" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.},
  keywords={Decoding;Encoding;Visualization;Image coding;Computer architecture;Proposals;Task analysis},
  doi={10.1109/CVPR42600.2020.01059},
  ISSN={2575-7075},}


@inproceedings{Cornia2020SMART,
 author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
 booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/ICRA40945.2020.9196653},
 file = {:PDF/SMArT_Training_Shallow_Memory-aware_Transformers_for_Robotic_Explainability.pdf:PDF},
 groups = {Self-Attention Encoding},
 issn = {2577-087X},
 keywords = {Decoding;Computational modeling;Visualization;Magnetic heads;Robots;Natural languages;Encoding},
 pages = {1128-1134},
 title = {SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability},
 year = {2020}
}

@inproceedings{Cristianini2000AnIT,
 author = {Nello Cristianini and John Shawe-Taylor},
 file = {:PDF/512565724.pdf:PDF},
 title = {An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
 url = {https://api.semanticscholar.org/CorpusID:60486887},
 year = {2000}
}

@inproceedings{Dai2017Diverse,
 address = {Los Alamitos, CA, USA},
 author = {Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2017.323},
 file = {:PDF/Dai_Towards_Diverse_and_ICCV_2017_paper.pdf:PDF},
 groups = {Other deep learning methods},
 issn = {2380-7504},
 keywords = {Measurement;Buildings;Training;Cows;Gallium nitride;Generators;Visualization},
 pages = {2989-2998},
 publisher = {IEEE Computer Society},
 title = {{ Towards Diverse and Natural Image Descriptions via a Conditional GAN }},
 url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.323},
 year = {2017}
}

@inproceedings{Dalal2005HOG,
 author = {Dalal, N. and Triggs, B.},
 booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
 doi = {10.1109/CVPR.2005.177},
 file = {:PDF/Histograms_of_oriented_gradients_for_human_detection.pdf:PDF},
 issn = {1063-6919},
 keywords = {Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
 pages = {886-893 vol. 1},
 title = {Histograms of oriented gradients for human detection},
 volume = {1},
 year = {2005}
}

@article{Dale2021GPT3,
 author = {Dale, Robert},
 doi = {10.1017/S1351324920000601},
 file = {:PDF/GPT-3_Whats_it_good_for.pdf:PDF},
 groups = {bledy w SI},
 journal = {Natural Language Engineering},
 number = {1},
 pages = {113–118},
 title = {GPT-3: What’s it good for?},
 volume = {27},
 year = {2021}
}

@article{DBLP:journals/corr/abs-2001-01037,
 author = {Jiamei Sun and Sebastian Lapuschkin and Wojciech Samek and Alexander Binder},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2001-01037.bib},
 comment = {https://github.com/SunJiamei/LRP-imagecaptioning-pytorch},
 eprint = {2001.01037},
 eprinttype = {arXiv},
 file = {:PDF/Explain and Improve- LRP-Inference Fine-Tuning for Image Captioning Models.pdf:PDF},
 journal = {CoRR},
 timestamp = {Sat, 23 Jan 2021 01:19:29 +0100},
 title = {Understanding Image Captioning Models beyond Visualizing Attention},
 url = {http://arxiv.org/abs/2001.01037},
 volume = {abs/2001.01037},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2004-14451,
 author = {Allen Nie and Reuben Cohn{-}Gordon and Christopher Potts},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2004-14451.bib},
 comment = {https://github.com/windweller/Pragmatic-ISIC},
 eprint = {2004.14451},
 eprinttype = {arXiv},
 file = {:PDF/Pragmatic Issue-Sensitive Image Captioning.pdf:PDF},
 journal = {CoRR},
 timestamp = {Sun, 03 May 2020 17:39:04 +0200},
 title = {Pragmatic Issue-Sensitive Image Captioning},
 url = {https://arxiv.org/abs/2004.14451},
 volume = {abs/2004.14451},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2007-09580,
 author = {Chaorui Deng and Ning Ding and Mingkui Tan and Qi Wu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2007-09580.bib},
 comment = {https://github.com/bearcatt/LaBERT},
 eprint = {2007.09580},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
 title = {Length-Controllable Image Captioning},
 url = {https://arxiv.org/abs/2007.09580},
 volume = {abs/2007.09580},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2008-11009,
 author = {Jian Han Lim and Chee Seng Chan and KamWoh Ng and Lixin Fan and Qiang Yang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2008-11009.bib},
 comment = {https://github.com/jianhanlim/ipr-imagecaptioning},
 eprint = {2008.11009},
 eprinttype = {arXiv},
 file = {:PDF/Protect, show, attend and tell- Empowering image captioning models with ownership protection.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 08 Dec 2021 09:19:14 +0100},
 title = {Protect, Show, Attend and Tell: Image Captioning Model with Ownership Protection},
 url = {https://arxiv.org/abs/2008.11009},
 volume = {abs/2008.11009},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2110-07831,
 author = {Wenkai Yang and Yankai Lin and Peng Li and Jie Zhou and Xu Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2110-07831.bib},
 comment = {https://github.com/lancopku/RAP},
 eprint = {2110.07831},
 eprinttype = {arXiv},
 file = {:PDF/RAP- Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 15 Dec 2021 09:30:00 +0100},
 title = {{RAP:} Robustness-Aware Perturbations for Defending against Backdoor Attacks on {NLP} Models},
 url = {https://arxiv.org/abs/2110.07831},
 volume = {abs/2110.07831},
 year = {2021}
}

@article{DBLP:journals/corr/abs-2111-15015,
 author = {Zanyar Zohourianshahzadi and Jugal K. Kalita},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2111-15015.bib},
 eprint = {2111.15015},
 eprinttype = {arXiv},
 file = {:PDF/NeuralAttentionForImageCaption.pdf:PDF},
 groups = {review},
 journal = {CoRR},
 timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},
 title = {Neural Attention for Image Captioning: Review of Outstanding Methods},
 url = {https://arxiv.org/abs/2111.15015},
 volume = {abs/2111.15015},
 year = {2021}
}

@article{Delbrouck2018,
 author = {Jean{-}Benoit Delbrouck and St{\'{e}}phane Dupont},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1810-06245.bib},
 eprint = {1810.06245},
 eprinttype = {arXiv},
 file = {:PDF/1810.06245.pdf:PDF},
 journal = {CoRR},
 timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
 title = {Bringing back simplicity and lightliness into neural image captioning},
 url = {http://arxiv.org/abs/1810.06245},
 volume = {abs/1810.06245},
 year = {2018}
}

@article{Deng2020DensenetAdaptive,
 author = {Zhenrong Deng and Zhouqin Jiang and Rushi Lan and Wenming Huang and Xiaonan Luo},
 doi = {10.1016/j.image.2020.115836},
 file = {:PDF/1-s2.0-S092359652030059X-main.pdf:PDF},
 issn = {0923-5965},
 journal = {Signal Processing: Image Communication},
 keywords = {Image captioning, DenseNet, LSTM, Adaptive attention mechanism},
 pages = {115836},
 title = {Image captioning using DenseNet network and adaptive attention},
 volume = {85},
 year = {2020}
}

@inproceedings{Denkowski2014Meteor,
 address = {Baltimore, Maryland, USA},
 author = {Denkowski, Michael and Lavie, Alon},
 booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
 doi = {10.3115/v1/W14-3348},
 editor = {Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Monz, Christof and Post, Matt and Specia, Lucia},
 file = {:PDF/W14-3348.pdf:PDF},
 pages = {376--380},
 publisher = {Association for Computational Linguistics},
 title = {Meteor Universal: Language Specific Translation Evaluation for Any Target Language},
 year = {2014}
}

@article{Deorukhkar2021Detailed,
 author = {Deorukhkar, Kalpana and Ket, Satish},
 doi = {10.1007/s11042-021-11293-1},
 file = {:PDF/A detailed review of prevailing image captioning methods using deep learning techniques.pdf:PDF},
 journal = {Multimedia Tools and Applications},
 pages = {1--24},
 publisher = {Springer International Publishing},
 title = {A detailed review of prevailing image captioning methods using deep learning techniques},
 url = {https://link.springer.com/article/10.1007/s11042-021-11293-1},
 year = {2021}
}

@article{Dessi2021,
 article-number = {779},
 author = {Dessì, Danilo and Recupero, Diego Reforgiato and Sack, Harald},
 doi = {10.3390/electronics10070779},
 file = {:PDF/electronics-10-00779-v2.pdf:PDF},
 issn = {2079-9292},
 journal = {Electronics},
 number = {7},
 title = {An Assessment of Deep Learning Models and Word Embeddings for Toxicity Detection within Online Textual Comments},
 volume = {10},
 year = {2021}
}

@inproceedings{Devlin2015Language,
 address = {Beijing, China},
 author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 doi = {10.3115/v1/P15-2017},
 editor = {Zong, Chengqing and Strube, Michael},
 file = {:PDF/P15-2017.pdf:PDF},
 groups = {retrieval},
 pages = {100--105},
 publisher = {Association for Computational Linguistics},
 title = {Language Models for Image Captioning: The Quirks and What Works},
 url = {https://aclanthology.org/P15-2017/},
 year = {2015}
}

@inproceedings{Devlin2019BERTPO,
 author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
 booktitle = {NAACL},
 file = {:PDF/ BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@article{DHassani,
 author = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2104-05704.bib},
 comment = {https://github.com/lucidrains/vit-pytorch},
 eprint = {2104.05704},
 eprinttype = {arXiv},
 file = {:PDF/2104.05704.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 07 Nov 2022 08:34:14 +0100},
 title = {Escaping the Big Data Paradigm with Compact Transformers},
 url = {https://arxiv.org/abs/2104.05704},
 volume = {abs/2104.05704},
 year = {2021}
}

@article{DING2020520,
 author = {Songtao Ding and Shiru Qu and Yuling Xi and Shaohua Wan},
 doi = {10.1016/j.neucom.2019.04.095},
 file = {:PDF/1-s2.0-S0925231219310367-main.pdf:PDF},
 issn = {0925-2312},
 journal = {Neurocomputing},
 keywords = {Image captioning, Stimulus-driven, Concept-driven, Attention mechanism, LSTM},
 pages = {520-530},
 title = {Stimulus-driven and concept-driven analysis for image caption generation},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231219310367},
 volume = {398},
 year = {2020}
}

@article{Ding2023DeepLearningTechniques,
 article-number = {100},
 author = {Ding, Zifeng and Sun, Yuxuan and Xu, Sijin and Pan, Yan and Peng, Yanhong and Mao, Zebing},
 doi = {10.3390/robotics12040100},
 file = {:PDF/robotics-12-00100.pdf:PDF},
 issn = {2218-6581},
 journal = {Robotics},
 number = {4},
 title = {Recent Advances and Perspectives in Deep Learning Techniques for 3D Point Cloud Data Processing},
 url = {https://www.mdpi.com/2218-6581/12/4/100},
 volume = {12},
 year = {2023}
}

@inproceedings{Donadello2019Compensating,
 author = {Ivan Donadello and Luciano Serafini},
 booktitle = {{IJCNN}},
 comment = {https://github.com/ivanDonadello/Visual-Relationship-Detection-LTN},
 file = {:PDF/Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation.pdf:PDF},
 pages = {1--8},
 printed = {yes},
 publisher = {{IEEE}},
 title = {Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation},
 year = {2019}
}

@article{Donahue2017LongTerm,
 address = {USA},
 author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
 doi = {10.1109/TPAMI.2016.2599174},
 file = {:PDF/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf:PDF},
 groups = {encoder-decoder-lit, global CNN features, Two-layer LSTM},
 issn = {0162-8828},
 issue_date = {April 2017},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 number = {4},
 numpages = {15},
 pages = {677–691},
 publisher = {IEEE Computer Society},
 title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
 volume = {39},
 year = {2017}
}

@inproceedings{Dong2017,
 author = {Dong, Hao and Zhang, Jingqing and McIlwraith, Douglas and Guo, Yike},
 booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
 doi = {10.1109/ICIP.2017.8296635},
 file = {:PDF/1703.06676.pdf:PDF},
 groups = {inception},
 location = {Beijing, China},
 numpages = {5},
 pages = {2015–2019},
 publisher = {IEEE Press},
 title = {I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation},
 url = {https://doi.org/10.1109/ICIP.2017.8296635},
 year = {2017}
}

@article{Dong2020Novel,
 author = {Dong, Wenhui and Yau, Stephen S. -T.},
 doi = {10.1109/ACCESS.2020.2997857},
 file = {:PDF/A_Novel_Image_Description_With_the_Stretched_Natural_Vector_Method_Application_to_Face_Recognition.pdf:PDF},
 journal = {IEEE Access},
 pages = {100084-100094},
 title = {A Novel Image Description With the Stretched Natural Vector Method: Application to Face Recognition},
 volume = {8},
 year = {2020}
}

@article{Dubey2021LabelAttentionTW,
 author = {Shikha Dubey and Farrukh Olimov and Muhammad Aasim Rafique and Joonmo Kim and Moongu Jeon},
 comment = {https://github.com/uestc-nnlab/gat},
 file = {:PDF/geometry attention transformer with position-aware lstms for image captioning.pdf:PDF},
 journal = {ArXiv},
 title = {Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning},
 volume = {abs/2109.07799},
 year = {2021}
}

@article{Dubey2022ActivationDL,
 address = {NLD},
 author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
 doi = {10.1016/j.neucom.2022.06.111},
 file = {:PDF/1-s2.0-S0925231222008426-main.pdf:PDF},
 issn = {0925-2312},
 issue_date = {Sep 2022},
 journal = {Neurocomput.},
 keywords = {Activation Functions, Neural networks, Convolutional neural networks, Deep learning, Overview, Recurrent Neural Networks},
 number = {C},
 numpages = {17},
 pages = {92–108},
 publisher = {Elsevier Science Publishers B. V.},
 title = {Activation functions in deep learning: A comprehensive survey and benchmark},
 url = {https://doi.org/10.1016/j.neucom.2022.06.111},
 volume = {503},
 year = {2022}
}

@article{Dunning1993Accurate,
 address = {Cambridge, MA, USA},
 author = {Dunning, Ted},
 file = {:PDF/J93-1003.pdf:PDF},
 issn = {0891-2017},
 issue_date = {March 1993},
 journal = {Comput. Linguist.},
 number = {1},
 numpages = {14},
 pages = {61–74},
 publisher = {MIT Press},
 title = {Accurate methods for the statistics of surprise and coincidence},
 volume = {19},
 year = {1993}
}

@inproceedings{Dutta1988Approximate,
 address = {New York, NY, USA},
 author = {Dutta, Soumitra},
 booktitle = {Proceedings of the 1st International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 1},
 doi = {10.1145/51909.51925},
 file = {:PDF/Approximate_spatial_Dutta1988.pdf:PDF},
 isbn = {0897912713},
 location = {Tullahoma, Tennessee, USA},
 numpages = {15},
 pages = {126–140},
 publisher = {Association for Computing Machinery},
 series = {IEA/AIE '88},
 title = {Approximate Spatial Reasoning},
 url = {https://doi.org/10.1145/51909.51925},
 year = {1988}
}

@article{e26100876,
 article-number = {876},
 author = {Zhao, Fengzhi and Yu, Zhezhou and Wang, Tao and Lv, Yi},
 doi = {10.3390/e26100876},
 file = {:PDF/entropy-26-00876-v2-2.pdf:PDF},
 issn = {1099-4300},
 journal = {Entropy},
 number = {10},
 pubmedid = {39451952},
 title = {Image Captioning Based on Semantic Scenes},
 url = {https://www.mdpi.com/1099-4300/26/10/876},
 volume = {26},
 year = {2024}
}

@article{Egenhofer1991Point,
 author = {MAX J. EGENHOFER and ROBERT D. FRANZOSA},
 doi = {10.1080/02693799108927841},
 eprint = {https://doi.org/10.1080/02693799108927841},
 file = {:PDF/Egenhofer_Franzosa_1991.pdf:PDF},
 journal = {International Journal of Geographical Information Systems},
 number = {2},
 pages = {161-174},
 publisher = {Taylor \& Francis},
 title = {Point-set topological spatial relations},
 url = {https://doi.org/10.1080/02693799108927841},
 volume = {5},
 year = {1991}
}

@inproceedings{Egenhofer2015Qualitative,
 address = {Dordrecht},
 author = {Egenhofer, Max J.},
 booktitle = {Studying Visual and Spatial Reasoning for Design Creativity},
 editor = {Gero, John S.},
 file = {:PDF/Egenhofer2015_Chapter_QualitativeSpatial-RelationRea.pdf:PDF},
 isbn = {978-94-017-9297-4},
 pages = {153--175},
 publisher = {Springer Netherlands},
 title = {Qualitative Spatial-Relation Reasoning for Design},
 url = {https://www.researchgate.net/publication/228570304_Qualitative_Spatial-Relation_Reasoning_for_Design},
 year = {2015}
}

@inbook{ElAsnaoui2021AutomatedMethods,
 address = {Cham},
 author = {El Asnaoui, Khalid and Chawki, Youness and Idri, Ali},
 booktitle = {Artificial Intelligence and Blockchain for Future Cybersecurity Applications},
 doi = {10.1007/978-3-030-74575-2_14},
 editor = {Maleh, Yassine and Baddi, Youssef and Alazab, Mamoun and Tawalbeh, Loai and Romdhani, Imed},
 file = {:PDF/2003.14363v1.pdf:PDF},
 isbn = {978-3-030-74575-2},
 pages = {257--284},
 publisher = {Springer International Publishing},
 title = {Automated Methods for Detection and Classification Pneumonia Based on X-Ray Images Using Deep Learning},
 url = {https://doi.org/10.1007/978-3-030-74575-2_14},
 year = {2021}
}

@InProceedings{Elliott2013Image,
  author    = {Elliott, Desmond and Keller, Frank},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  title     = {Image Description using Visual Dependency Representations},
  year      = {2013},
  address   = {Seattle, Washington, USA},
  editor    = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
  month     = oct,
  pages     = {1292--1302},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/D13-1128.pdf:PDF},
  url       = {https://aclanthology.org/D13-1128/},
}

@inproceedings{Elliott2014Comparing,
 address = {Baltimore, Maryland},
 author = {Elliott, Desmond and Keller, Frank},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 doi = {10.3115/v1/P14-2074},
 editor = {Toutanova, Kristina and Wu, Hua},
 file = {:PDF/P14-2074.pdf:PDF},
 pages = {452--457},
 publisher = {Association for Computational Linguistics},
 title = {Comparing Automatic Evaluation Measures for Image Description},
 url = {https://aclanthology.org/P14-2074/},
 year = {2014}
}

@inproceedings{Elliott2015Describing,
 address = {Beijing, China},
 author = {Elliott, Desmond and de Vries, Arjen},
 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.3115/v1/P15-1005},
 editor = {Zong, Chengqing and Strube, Michael},
 file = {:PDF/P15-1005.pdf:PDF},
 groups = {template},
 pages = {42--52},
 publisher = {Association for Computational Linguistics},
 title = {Describing Images using Inferred Visual Dependency Representations},
 year = {2015}
}

@article{Elman1990Cognitive,
 author = {Jeffrey L. Elman},
 doi = {10.1016/0364-0213(90)90002-E},
 issn = {0364-0213},
 journal = {Cognitive Science},
 number = {2},
 pages = {179-211},
 title = {Finding structure in time},
 url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
 volume = {14},
 year = {1990}
}

@InProceedings{Etal2006generating,
  author    = {de Marneffe, Marie and MacCartney, Bill and Manning, Christopher D.},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},
  title     = {Generating Typed Dependency Parses from Phrase Structure Parses},
  year      = {2006},
  address   = {Genoa, Italy},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
  publisher = {European Language Resources Association (ELRA)},
  file      = {:PDF/440_pdf.pdf:PDF},
  url       = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf},
}

@article{Everingham2010,
 author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
 day = {01},
 doi = {10.1007/s11263-009-0275-4},
 file = {:PDF/Everingham2010_Article_ThePascalVisualObjectClassesVO.pdf:PDF},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 number = {2},
 pages = {303-338},
 title = {The Pascal Visual Object Classes (VOC) Challenge},
 url = {https://doi.org/10.1007/s11263-009-0275-4},
 volume = {88},
 year = {2010}
}

@article{Fan2020Fuzzy,
 author = {Fan, Hong and Yang, Mei and Wang, Yankun and Zeng, Jia},
 file = {:PDF/FUZZY POSITIONING MODELING OF NATURAL LANGUAGE LOCATION DESCRIPTION.pdf:PDF},
 journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
 pages = {41--46},
 publisher = {Copernicus GmbH},
 title = {Fuzzy Positioning Modeling of Natural Language Location Description},
 url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B4-2020/41/2020/isprs-archives-XLIII-B4-2020-41-2020.pdf},
 volume = {43},
 year = {2020}
}

@article{Fan2021TCICTC,
 author = {Zhihao Fan and Zhongyu Wei and Siyuan Wang and Ruize Wang and Zejun Li and Haijun Shan and Xuanjing Huang},
 file = {:PDF/0091.pdf:PDF},
 groups = {transformer},
 journal = {ArXiv},
 title = {TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning},
 url = {https://api.semanticscholar.org/CorpusID:235489710},
 volume = {abs/2106.10936},
 year = {2021}
}

@InProceedings{Fang2015CVPRObjList,
  author    = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Lawrence Zitnick, C. and Zweig, Geoffrey},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {From Captions to Visual Concepts and Back},
  year      = {2015},
  pages     = {1475-1476},
  file      = {:PDF/Fang_From_Captions_to_2015_CVPR_paper.pdf:PDF},
  groups    = {Compositional architectures, global CNN features},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf},
}

@misc{Fang2021Injecting,
 archiveprefix = {arXiv},
 author = {Zhiyuan Fang and Jianfeng Wang and Xiaowei Hu and Lin Liang and Zhe Gan and Lijuan Wang and Yezhou Yang and Zicheng Liu},
 eprint = {2112.05230},
 primaryclass = {cs.CV},
 title = {Injecting Semantic Concepts into End-to-End Image Captioning},
 year = {2021}
}

@inproceedings{Farhadi2010,
 address = {Berlin, Heidelberg},
 author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
 booktitle = {Computer Vision -- ECCV 2010},
 editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
 file = {:PDF/Every picture tells a story- generating sentences from images.pdf:PDF},
 groups = {retrieval},
 isbn = {978-3-642-15561-1},
 pages = {15--29},
 publisher = {Springer Berlin Heidelberg},
 title = {Every Picture Tells a Story: Generating Sentences from Images},
 year = {2010}
}

@article{Farhadi2013Phrasal,
 author = {Farhadi, Ali and Sadeghi, Mohammad Amin},
 doi = {10.1109/TPAMI.2013.168},
 file = {:PDF/Phrasal_Recognition.pdf:PDF},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Data visualization;Detectors;Decoding;Object recognition;Image processing;Complexity theory;Visual phrase;phrasal recognition;visual composites;object recognition;object interactions;scene understanding;single image activity recognition;object subcategories},
 number = {12},
 pages = {2854-2865},
 title = {Phrasal Recognition},
 volume = {35},
 year = {2013}
}

@misc{fei2019fastimagecaptiongeneration,
 archiveprefix = {arXiv},
 author = {Zheng-cong Fei},
 eprint = {1912.06365},
 file = {:PDF/1912.06365v1.pdf:PDF},
 groups = {Non-autoregressive Language Models},
 primaryclass = {cs.CV},
 title = {Fast Image Caption Generation with Position Alignment},
 url = {https://arxiv.org/abs/1912.06365},
 year = {2019}
}

@inproceedings{Fei2020,
 address = {New York, NY, USA},
 author = {Fei, Zhengcong},
 booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
 doi = {10.1145/3394171.3413901},
 file = {:PDF/3394171.3413901.pdf:PDF},
 groups = {Non-autoregressive Language Models},
 isbn = {9781450379885},
 keywords = {image captioning, iterative back modification, latent variables, non-autoregressive generation},
 location = {Seattle, WA, USA},
 numpages = {9},
 pages = {3182–3190},
 publisher = {Association for Computing Machinery},
 series = {MM '20},
 title = {Iterative Back Modification for Faster Image Captioning},
 year = {2020}
}

@article{Felzenszwalb2010Discriminatively,
 author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
 doi = {10.1109/TPAMI.2009.167},
 file = {:PDF/lsvm-pami.pdf:PDF},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Object detection;Deformable models;Support vector machines;Bicycles;Computer vision;Shape;Speech recognition;Computer Society;Iterative algorithms;Lighting;Object recognition;deformable models;pictorial structures;discriminative training;latent SVM.},
 number = {9},
 pages = {1627-1645},
 title = {Object Detection with Discriminatively Trained Part-Based Models},
 volume = {32},
 year = {2010}
}

@inproceedings{Ferraro2015Survey,
 address = {Lisbon, Portugal},
 author = {Ferraro, Francis and Mostafazadeh, Nasrin and Huang, Ting-Hao and Vanderwende, Lucy and Devlin, Jacob and Galley, Michel and Mitchell, Margaret},
 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D15-1021},
 editor = {M{\`a}rquez, Llu{\'\i}s and Callison-Burch, Chris and Su, Jian},
 file = {:PDF/D15-1021.pdf:PDF},
 pages = {207--213},
 publisher = {Association for Computational Linguistics},
 title = {A Survey of Current Datasets for Vision and Language Research},
 year = {2015}
}

@book{Firth1957ASynopsis,
 added-at = {2017-11-18T14:22:30.000+0100},
 author = {Firth, J.},
 biburl = {https://www.bibsonomy.org/bibtex/2b72c0e9f0c18ca5466a86ad391997923/thoni},
 file = {:PDF/lecture1-firth.pdf:PDF},
 interhash = {49b3d847512a3bff2a1d77cd5ea5391f},
 intrahash = {b72c0e9f0c18ca5466a86ad391997923},
 keywords = {context distributional semantics},
 publisher = {Longman},
 series = {Studies in Linguistic Analysis, Philological},
 timestamp = {2017-11-18T14:22:30.000+0100},
 title = {A synopsis of linguistic theory 1930-1955},
 year = {1957}
}

@article{Fox1989StopWords,
 address = {New York, NY, USA},
 author = {Fox, Christopher},
 doi = {10.1145/378881.378888},
 file = {:PDF/378881.378888.pdf:PDF},
 issn = {0163-5840},
 issue_date = {Fall 89/Winter 90},
 journal = {SIGIR Forum},
 number = {1–2},
 numpages = {3},
 pages = {19–21},
 publisher = {Association for Computing Machinery},
 title = {A stop list for general text},
 volume = {24},
 year = {1989}
}

@inproceedings{Francis2018Fuzzy,
 author = {J. {Francis} and F. {Rahbarnia} and P. {Matsakis}},
 booktitle = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 doi = {10.1109/FUZZ-IEEE.2018.8491654},
 file = {:PDF/Fuzzy_NLG_Francis_2018.pdf:PDF},
 pages = {1-8},
 title = {Fuzzy NLG System for Extensive Verbal Description of Relative Positions},
 year = {2018}
}

@inproceedings{French1993CatastrophicInference,
 address = {San Francisco, CA, USA},
 author = {French, Robert M.},
 booktitle = {Proceedings of the 7th International Conference on Neural Information Processing Systems},
 location = {Denver, Colorado},
 numpages = {2},
 pages = {1176–1177},
 publisher = {Morgan Kaufmann Publishers Inc.},
 series = {NIPS'93},
 title = {Catastrophic interference in connectionist networks: can it be predicted, can it be prevented?},
 year = {1993}
}

@article{Fu2017,
 author = {Fu, Kun and Jin, Junqi and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
 doi = {10.1109/TPAMI.2016.2642953},
 file = {:PDF/Aligning_Where_to_See_and_What_to_Tell_Image_Captioning_with_Region-Based_Attention_and_Scene-Specific_Contexts.pdf:PDF},
 groups = {Compositional architectures},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Visualization;Feature extraction;Image classification;Context modeling;Adaptation models;Computational modeling;Data mining;Image captioning;visual attention;scene-specific context;LSTM},
 number = {12},
 pages = {2321-2334},
 title = {Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts},
 volume = {39},
 year = {2017}
}

@article{Fu2020Rich,
 author = {Fu, Xin and Zhao, Yao and Wei, Yunchao and Zhao, Yufeng and Wei, Shikui},
 doi = {10.1109/TMM.2019.2957948},
 file = {:PDF/Rich Features Embedding for Cross-Modal Retrieval- A Simple Baseline .pdf:PDF},
 groups = {embeddings},
 journal = {IEEE Transactions on Multimedia},
 number = {9},
 pages = {2354-2365},
 title = {Rich Features Embedding for Cross-Modal Retrieval: A Simple Baseline},
 volume = {22},
 year = {2020}
}

@article{fudholi2022study,
 author = {Fudholi, Dhomas Hatta and Zahra, Annisa and Nayoan, Royan Abida N},
 file = {:PDF/1394-Article Text-124126475-1-10-20220329.pdf:PDF},
 journal = {Kinetik: Game Technology, Information System, Computer Network, Computing, Electronics, and Control},
 pages = {91--98},
 title = {A Study on Visual Understanding Image Captioning using Different Word Embeddings and CNN-Based Feature Extractions},
 year = {2022}
}

@inproceedings{Gader1997Fuzzy,
 author = {P. D. {Gader}},
 booktitle = {Proceedings of 6th International Fuzzy Systems Conference},
 doi = {10.1109/FUZZY.1997.622875},
 file = {:PDF/Fuzzy_spatial_relations_Gader_1997.pdf:PDF},
 pages = {1179-1183 vol.2},
 test = {dfddfdfdfdf},
 title = {Fuzzy spatial relations based on fuzzy morphology},
 volume = {2},
 year = {1997}
}

@InProceedings{Gan2017SemanticProbability,
  author    = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Semantic Compositional Networks for Visual Captioning},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {5630-5634},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.127},
  file      = {:PDF/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf:PDF},
  groups    = {global CNN features},
  issn      = {1063-6919},
  keywords  = {Semantics;Visualization;Pediatrics;Feature extraction;Mouth;Tensile stress;Training},
}

@inproceedings{Gan2017StyleNet,
 author = {Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2017.108},
 file = {:PDF/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf:PDF},
 groups = {encoder-decoder-lit},
 issn = {1063-6919},
 keywords = {Visualization;Training;Dogs;Flickr;Logic gates;Semantics},
 pages = {955-964},
 title = {StyleNet: Generating Attractive Visual Captions with Styles},
 year = {2017}
}

@inproceedings{Gao2018,
 address = {New York, NY, USA},
 author = {Gao, Lizhao and Wang, Bo and Wang, Wenmin},
 booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
 doi = {10.1145/3195106.3195114},
 file = {:PDF/Image Captioning with Scene-graph Based Semantic Concepts.pdf:PDF},
 groups = {graf},
 isbn = {9781450363532},
 keywords = {LSTM, Scene graph, Image captioning, CNN, Semantic representation},
 location = {Macau, China},
 numpages = {5},
 pages = {225–229},
 publisher = {Association for Computing Machinery},
 series = {ICMLC 2018},
 title = {Image Captioning with Scene-Graph Based Semantic Concepts},
 url = {https://doi.org/10.1145/3195106.3195114},
 year = {2018}
}

@article{Gao2019Deliberate,
 abstractnote = {In daily life, deliberation is a common behavior for human to improve or refine their work (e.g., writing, reading and drawing). To date, encoder-decoder framework with attention mechanisms has achieved great progress for image captioning. However, such framework is in essential an one-pass forward process while encoding to hidden states and attending to visual features, but lacks of the deliberation action. The learned hidden states and visual attention are directly used to predict the final captions without further polishing. In this paper, we present a novel Deliberate Residual Attention Network, namely DA, for image captioning. The first-pass residual-based attention layer prepares the hidden states and visual attention for generating a preliminary version of the captions, while the second-pass deliberate residual-based attention layer refines them. Since the second-pass is based on the rough global features captured by the hidden layer and visual attention in the first-pass, our DA has the potential to generate better sentences. We further equip our DA with discriminative loss and reinforcement learning to disambiguate image/caption pairs and reduce exposure bias. Our model improves the state-of-the-arts on the MSCOCO dataset and reaches 37.5% BELU-4, 28.5% METEOR and 125.6% CIDEr. It also outperforms the-state-ofthe-arts from 25.1% BLEU-4, 20.4% METEOR and 53.1% CIDEr to 29.4% BLEU-4, 23.0% METEOR and 66.6% on the Flickr30K dataset.},
 author = {Gao, Lianli and Fan, Kaixuan and Song, Jingkuan and Liu, Xianglong and Xu, Xing and Shen, Heng Tao},
 doi = {10.1609/aaai.v33i01.33018320},
 file = {:PDF/4845-Article Text-7911-1-10-20190709.pdf:PDF},
 groups = {attention taxonomy},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {01},
 pages = {8320-8327},
 title = {Deliberate Attention Networks for Image Captioning},
 volume = {33},
 year = {2019}
}

@misc{gao2019maskednonautoregressiveimagecaptioning,
      title={Masked Non-Autoregressive Image Captioning},
      author={Junlong Gao and Xi Meng and Shiqi Wang and Xia Li and Shanshe Wang and Siwei Ma and Wen Gao},
      year={2019},
      eprint={1906.00717},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{Gao2019SCST,
 address = {Los Alamitos, CA, USA},
 author = {Gao, Junlong and Wang, Shiqi and Wang, Shanshe and Ma, Siwei and Gao, Wen},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00646},
 file = {:PDF/a20191230181.pdf:PDF},
 keywords = {Training;Measurement;Monte Carlo methods;Computational modeling;Training data;Reinforcement learning;Benchmark testing},
 pages = {6293-6301},
 publisher = {IEEE Computer Society},
 title = {{ Self-Critical N-Step Training for Image Captioning }},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00646},
 year = {2019}
}

@InProceedings{Ge2019Exploring,
  author    = {Ge, Hongwei and Yan, Zehang and Zhang, Kai and Zhao, Mingde and Sun, Liang},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {{Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style}},
  year      = {2019},
  address   = {Los Alamitos, CA, USA},
  pages     = {1754-1763},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/ICCV.2019.00184},
  file      = {:PDF/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features, hidden state reconstruction},
  keywords  = {Feature extraction;Semantics;Visualization;Decoding;Training;Computational modeling;Cognition},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00184},
}

@inproceedings{Ge2021Structured,
 address = {New York, NY, USA},
 author = {Ge, Xuri and Chen, Fuhai and Jose, Joemon M. and Ji, Zhilong and Wu, Zhongqin and Liu, Xiao},
 booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
 doi = {10.1145/3474085.3475634},
 file = {:PDF/Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval .pdf:PDF},
 isbn = {9781450386517},
 keywords = {context-aware structured trees, image-sentence retrieval, semantics and structural consistency, multimodal retrieval},
 location = {Virtual Event, China},
 numpages = {9},
 pages = {5185–5193},
 publisher = {Association for Computing Machinery},
 series = {MM '21},
 title = {Structured Multi-Modal Feature Embedding and Alignment for Image-Sentence Retrieval},
 url = {https://doi.org/10.1145/3474085.3475634},
 year = {2021}
}

@inproceedings{Geetha2023,
 author = {U, Vijetha and V, Geetha},
 booktitle = {2023 13th International Conference on Cloud Computing, Data Science and Engineering (Confluence)},
 doi = {10.1109/Confluence56041.2023.10048861},
 file = {:PDF/Opportunities_and_Challenges_in_Development_of_Support_System_for_Visually_Impaired_A_Survey.pdf:PDF},
 groups = {review},
 pages = {684-690},
 title = {Opportunities and Challenges in Development of Support System for Visually Impaired: A Survey},
 year = {2023}
}

@misc{ghandi2022deep,
 archiveprefix = {arXiv},
 author = {Taraneh Ghandi and Hamidreza Pourreza and Hamidreza Mahyar},
 eprint = {2201.12944},
 file = {:PDF/Deep Learning Approaches on Image Captioning- A Review.pdf:PDF},
 groups = {review},
 primaryclass = {cs.CV},
 title = {Deep Learning Approaches on Image Captioning: A Review},
 year = {2022}
}

@article{Ghandi2023,
 address = {New York, NY, USA},
 articleno = {62},
 author = {Ghandi, Taraneh and Pourreza, Hamidreza and Mahyar, Hamidreza},
 doi = {10.1145/3617592},
 groups = {review},
 issn = {0360-0300},
 issue_date = {March 2024},
 journal = {ACM Comput. Surv.},
 keywords = {text generation, deep learning, Image captioning},
 number = {3},
 numpages = {39},
 publisher = {Association for Computing Machinery},
 title = {Deep Learning Approaches on Image Captioning: A Review},
 volume = {56},
 year = {2023}
}

@inproceedings{Gilberto2015Learning,
 address = {Denver, Colorado},
 author = {Gilberto Mateos Ortiz, Luis and Wolff, Clemens and Lapata, Mirella},
 booktitle = {Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.3115/v1/N15-1174},
 editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
 file = {:PDF/N15-1174.pdf:PDF;:PDF/N15-1174.pdf:PDF},
 groups = {retrieval},
 pages = {1505--1515},
 publisher = {Association for Computational Linguistics},
 title = {Learning to Interpret and Describe Abstract Scenes},
 url = {https://aclanthology.org/N15-1174},
 year = {2015}
}

@article{Girshick2013Rich,
 author = {Ross B. Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/GirshickDDM13.bib},
 eprint = {1311.2524},
 eprinttype = {arXiv},
 file = {:PDF/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
 title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
 url = {http://arxiv.org/abs/1311.2524},
 volume = {abs/1311.2524},
 year = {2013}
}

@inproceedings{Girshick2014Rich,
 address = {Los Alamitos, CA, USA},
 author = {R. Girshick and J. Donahue and T. Darrell and J. Malik},
 booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2014.81},
 file = {:PDF/Girshick2014.pdf:PDF},
 issn = {1063-6919},
 keywords = {proposals;feature extraction;training;visualization;object detection;vectors;support vector machines},
 pages = {580-587},
 publisher = {IEEE Computer Society},
 title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2014.81},
 year = {2014}
}

@inproceedings{Girshick2015Fast,
 author = {R. {Girshick}},
 booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2015.169},
 file = {:PDF/FastRCNN_Girshick2015.pdf:PDF},
 pages = {1440-1448},
 title = {Fast R-CNN},
 year = {2015}
}

@inproceedings{Gong2014,
 address = {Cham},
 author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia and Lazebnik, Svetlana},
 booktitle = {Computer Vision -- ECCV 2014},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 file = {:PDF/yunchao_eccv14_sentence.pdf:PDF},
 groups = {retrieval},
 isbn = {978-3-319-10593-2},
 pages = {529--545},
 publisher = {Springer International Publishing},
 title = {Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections},
 year = {2014}
}



@book{Goodfellow2018DL,
 author = {Goodfellow, Ian},
 language = {pl},
 title = {Deep Learning Wspolczesne systemy uczace sie},
 year = {2018}
}

@article{Goodman2001Progress,
 author = {Joshua Goodman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/cs-CL-0108005.bib},
 file = {:PDF/0108005v1.pdf:PDF},
 groups = {LM},
 journal = {CoRR},
 timestamp = {Thu, 29 Apr 2021 15:44:29 +0200},
 title = {A Bit of Progress in Language Modeling},
 url = {https://arxiv.org/abs/cs/0108005},
 volume = {cs.CL/0108005},
 year = {2001}
}

@incollection{Gordon1989Catastrophic,
 author = {Michael McCloskey and Neal J. Cohen},
 doi = {10.1016/S0079-7421(08)60536-8},
 editor = {Gordon H. Bower},
 issn = {0079-7421},
 pages = {109-165},
 publisher = {Academic Press},
 series = {Psychology of Learning and Motivation},
 title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
 volume = {24},
 year = {1989}
}

@Misc{Graff2003Gigaword,
  author    = {Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  title     = {English Gigaword Fifth Edition},
  year      = {2012},
  doi       = {10.57754/FDAT.8a40a-a6q96},
  publisher = {University of Tübingen},
}

@inproceedings{Grave2018WordWectors,
 address = {Miyazaki, Japan},
 author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
 booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
 editor = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
 file = {:PDF/L18-1550.pdf:PDF},
 publisher = {European Language Resources Association (ELRA)},
 title = {Learning Word Vectors for 157 Languages},
 url = {https://aclanthology.org/L18-1550/},
 year = {2018}
}

@article{Gu2017,
 author = {Jiuxiang Gu and G. Wang and Jianfei Cai and Tsuhan Chen},
 file = {:PDF/An_Empirical_Study_of_Language_CNN_for_Image_Captioning.pdf:PDF},
 groups = {vgg16, global CNN features},
 journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
 pages = {1231-1240},
 title = {An Empirical Study of Language CNN for Image Captioning},
 year = {2016}
}

@inproceedings{Gu2018,
 address = {Cham},
 author = {Gu, Jiayuan and Hu, Han and Wang, Liwei and Wei, Yichen and Dai, Jifeng},
 booktitle = {Computer Vision -- ECCV 2018},
 editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
 file = {:PDF/978-3-030-01258-8.pdf:PDF},
 isbn = {978-3-030-01258-8},
 pages = {392--406},
 publisher = {Springer International Publishing},
 title = {Learning Region Features for Object Detection},
 year = {2018}
}

@article{Gu2018RecentAdvances,
 address = {USA},
 author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
 doi = {10.1016/j.patcog.2017.10.013},
 file = {:PDF/recent-advances-in-convolutional-neural-networks-2017.pdf:PDF},
 groups = {cnn},
 issn = {0031-3203},
 issue_date = {May 2018},
 journal = {Pattern Recogn.},
 keywords = {Convolutional neural network, Deep learning},
 number = {C},
 numpages = {24},
 pages = {354–377},
 publisher = {Elsevier Science Inc.},
 title = {Recent advances in convolutional neural networks},
 volume = {77},
 year = {2018}
}

@InProceedings{Gu2018StackCaptioning,
  author    = {Gu, Jiuxiang and Cai, Jianfei and Wang, Gang and Chen, Tsuhan},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  title     = {Stack-captioning: coarse-to-fine learning for image captioning},
  year      = {2018},
  pages     = {1-8},
  publisher = {AAAI Press},
  series    = {AAAI'18/IAAI'18/EAAI'18},
  articleno = {837},
  file      = {:PDF/3504035.3504872.pdf:PDF},
  groups    = {Other deep learning methods, Additive attention over a grid of features, Multi-stage generation, attention taxonomy},
  isbn      = {978-1-57735-800-8},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {8},
}

@inproceedings{gu2019unpaired,
 author = {Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei and Zhao, Handong and Yang, Xu and Wang, Gang},
 booktitle = {ICCV},
 file = {:PDF/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf:PDF},
 groups = {graf},
 title = {Unpaired Image Captioning via Scene Graph Alignments},
 year = {2019}
}

@article{Guesgen2015Fuzzy,
 author = {Guesgen, H.W.},
 document_type = {Article},
 doi = {10.1007/978-3-319-14726-0_12},
 file = {:PDF/A-fuzzy-set-approach-to-expressing-preferences-in-spatial-reasoning2015Lecture-Notes-in-Computer-Science-including-subseries-Lecture-Notes-in-Artificial-Intelligence-and-Lecture-Notes-in-Bioinformatics.pdf:PDF},
 journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
 note = {cited By 1},
 pages = {173-185},
 source = {Scopus},
 title = {A fuzzy set approach to expressing preferences in spatial reasoning},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921769256&doi=10.1007%2f978-3-319-14726-0_12&partnerID=40&md5=43c3cf414eb068dfeed7c933560d399a},
 volume = {9060},
 year = {2015}
}

@inproceedings{Guler2018DensePose,
 author = {Güler, Rıza Alp and Neverova, Natalia and Kokkinos, Iasonas},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf:PDF},
 title = {DensePose: Dense Human Pose Estimation in the Wild},
 year = {2018}
}

@inproceedings{Guo2019Aligning,
 address = {New York, NY, USA},
 author = {Guo, Longteng and Liu, Jing and Tang, Jinhui and Li, Jiangwei and Luo, Wei and Lu, Hanqing},
 booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
 comment = {https://github.com/ltguo19/VSUA-Captioning},
 doi = {10.1145/3343031.3350943},
 file = {:PDF/Aligning Linguistic Words and Visual Semantic Units for Image Captioning.pdf:PDF},
 groups = {Spatial and semantic graphs.},
 isbn = {9781450368896},
 keywords = {image captioning, visual relationship, vision-language, graph convolutional networks},
 location = {Nice, France},
 numpages = {9},
 pages = {765–773},
 printed = {yes},
 publisher = {Association for Computing Machinery},
 series = {MM '19},
 title = {Aligning Linguistic Words and Visual Semantic Units for Image Captioning},
 url = {https://doi.org/10.1145/3343031.3350943},
 year = {2019}
}

@inproceedings{Guo2020Normalized,
 author = {Guo, Longteng and Liu, Jing and Zhu, Xinxin and Yao, Peng and Lu, Shichen and Lu, Hanqing},
 booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
 file = {:PDF/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf:PDF},
 groups = {Self-Attention Encoding, Transformer, attention taxonomy},
 pages = {10327--10336},
 title = {Normalized and geometry-aware self-attention network for image captioning},
 year = {2020}
}

@misc{guo2021fastsequencegenerationmultiagent,
 archiveprefix = {arXiv},
 author = {Longteng Guo and Jing Liu and Xinxin Zhu and Hanqing Lu},
 eprint = {2101.09698},
 file = {:PDF/2101.09698v1.pdf:PDF},
 groups = {Non-autoregressive Language Models},
 primaryclass = {cs.CL},
 title = {Fast Sequence Generation with Multi-Agent Reinforcement Learning},
 year = {2021}
}

@inproceedings{Guo2021NonAutoregresive,
 articleno = {107},
 author = {Guo, Longteng and Liu, Jing and Zhu, Xinxin and He, Xingjian and Jiang, Jie and Lu, Hanqing},
 booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
 file = {:PDF/3491440.3491547.pdf:PDF},
 groups = {Non-autoregressive Language Models},
 isbn = {9780999241165},
 location = {Yokohama, Yokohama, Japan},
 numpages = {7},
 series = {IJCAI'20},
 title = {Non-autoregressive image captioning with counterfactuals-critical multi-agent learning},
 year = {2021}
}

@inproceedings{Gupta2012Choosing,
 author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C. V.},
 booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
 file = {:PDF/8205-Article Text-11732-1-2-20201228-2.pdf:PDF},
 groups = {retrieval},
 location = {Toronto, Ontario, Canada},
 numpages = {7},
 pages = {606–612},
 publisher = {AAAI Press},
 series = {AAAI'12},
 title = {Choosing linguistics over vision to describe images},
 year = {2012}
}

@inproceedings{gupta2019vico,
 author = {Gupta, Tanmay and Schwing, Alexander and Hoiem, Derek},
 booktitle = {ICCV},
 file = {:PDF/ViCo- Word Embeddings from Visual Co-occurrences.pdf:PDF},
 groups = {embeddings},
 title = {ViCo: Word Embeddings from Visual Co-occurrences},
 year = {2019}
}

@article{gurari2020captioning,
 author = {Gurari, Danna and Zhao, Yinan and Zhang, Meng and Bhattacharya, Nilavra},
 comment = {https://github.com/Yinan-Zhao/AoANet_VizWiz},
 file = {:PDF/Captioning Images Taken by People Who Are Blind.pdf:PDF},
 journal = {arXiv preprint arXiv:2002.08565},
 title = {Captioning Images Taken by People Who Are Blind},
 year = {2020}
}

@article{Guyon2003Introduction,
 author = {Guyon, Isabelle and Elisseeff, Andr\'{e}},
 file = {:PDF/944919.944968.pdf:PDF},
 groups = {representation learning},
 issn = {1532-4435},
 issue_date = {3/1/2003},
 journal = {J. Mach. Learn. Res.},
 number = {null},
 numpages = {26},
 pages = {1157–1182},
 publisher = {JMLR.org},
 title = {An introduction to variable and feature selection},
 volume = {3},
 year = {2003}
}

@article{Hadosh2013Framing,
 address = {El Segundo, CA, USA},
 author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
 file = {:PDF/live-3994-7274-jair.pdf:PDF},
 groups = {retrieval},
 issn = {1076-9757},
 issue_date = {May 2013},
 journal = {J. Artif. Int. Res.},
 number = {1},
 numpages = {47},
 pages = {853–899},
 publisher = {AI Access Foundation},
 title = {Framing image description as a ranking task: data, models and evaluation metrics},
 volume = {47},
 year = {2013}
}

@article{Haller2023LSTMPerformance,
 article-number = {1432},
 author = {Bolboacă, Roland and Haller, Piroska},
 doi = {10.3390/math11061432},
 file = {:PDF/mathematics-11-01432-v2.pdf:PDF},
 groups = {lstm},
 issn = {2227-7390},
 journal = {Mathematics},
 number = {6},
 title = {Performance Analysis of Long Short-Term Memory Predictive Neural Networks on Time Series Data},
 url = {https://www.mdpi.com/2227-7390/11/6/1432},
 volume = {11},
 year = {2023}
}

@inproceedings{Han1995Sigmoid,
 address = {Berlin, Heidelberg},
 author = {Han, Jun and Moraga, Claudio},
 booktitle = {From Natural to Artificial Neural Computation},
 editor = {Mira, Jos{\'e} and Sandoval, Francisco},
 file = {:PDF/3-540-59497-3_175.pdf:PDF},
 isbn = {978-3-540-49288-7},
 pages = {195--201},
 publisher = {Springer Berlin Heidelberg},
 title = {The influence of the sigmoid function parameters on the speed of backpropagation learning},
 year = {1995}
}

@article{Han2021ImageSG,
 author = {Xiaotian Han and Jianwei Yang and Houdong Hu and Lei Zhang and Jianfeng Gao and Pengchuan Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2107-12604.bib},
 eprint = {2107.12604},
 eprinttype = {arXiv},
 file = {:PDF/Image Scene Graph Generation (SGG) Benchmark.pdf:PDF},
 journal = {CoRR},
 timestamp = {Fri, 30 Jul 2021 13:03:06 +0200},
 title = {Image Scene Graph Generation {(SGG)} Benchmark},
 url = {https://arxiv.org/abs/2107.12604},
 volume = {abs/2107.12604},
 year = {2021}
}

@article{Han2021PreTrainedPAstPresentFuture,
 author = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
 doi = {10.1016/j.aiopen.2021.08.002},
 file = {:PDF/Pre-trained models- Past, present and future.pdf:PDF},
 issn = {2666-6510},
 journal = {AI Open},
 keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
 pages = {225-250},
 title = {Pre-trained models: Past, present and future},
 url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
 volume = {2},
 year = {2021}
}

@inproceedings{Hao2018,
 author = {Hao, Yanlong and Xie, Jiyang and Lin, Zhiqing},
 booktitle = {2018 International Conference on Network Infrastructure and Digital Content (IC-NIDC)},
 doi = {10.1109/ICNIDC.2018.8525732},
 file = {:PDF/Image_Caption_via_Visual_Attention_Switch_on_DenseNet.pdf:PDF},
 groups = {encoder-decoder-lit},
 issn = {2575-4955},
 keywords = {Feature extraction;Decoding;Visualization;Switches;Training;Dictionaries;Data models;Image caption;Visual attention switch;Encoder-decoder architecture;DenseNet;LSTM},
 pages = {334-338},
 title = {Image Caption via Visual Attention Switch on DenseNet},
 year = {2018}
}

@article{Haque2021ImgCapt,
 author = {Haque, Anwar Ul and Ghani, Sayeed and Saeed, Muhammad},
 doi = {10.1109/ACCESS.2021.3131343},
 file = {:PDF/Image_Captioning_With_Positional_and_Geometrical_Semantics.pdf:PDF},
 journal = {IEEE Access},
 pages = {160917-160925},
 printed = {yes},
 title = {Image Captioning With Positional and Geometrical Semantics},
 volume = {9},
 year = {2021}
}

@article{hardoon2004canonical,
 author = {Hardoon, David R and Szedmak, Sandor and Shawe-Taylor, John},
 file = {:PDF/tech_report03.pdf:PDF},
 journal = {Neural computation},
 number = {12},
 pages = {2639--2664},
 publisher = {MIT Press},
 title = {Canonical correlation analysis: An overview with application to learning methods},
 volume = {16},
 year = {2004}
}

@article{Harris1954DistributionalS,
 author = {Zellig S. Harris},
 doi = {10.1080/00437956.1954.11659520},
 file = {:PDF/Distributional Structure-1.pdf:PDF},
 journal = {WORD},
 number = {2-3},
 pages = {146--162},
 publisher = {Routledge},
 title = {Distributional Structure},
 volume = {10},
 year = {1954}
}

@misc{havlik2024commonerrorsgenerativeai,
 archiveprefix = {arXiv},
 author = {Denis Havlik and Marcelo Pias},
 eprint = {2402.00830},
 file = {:PDF/2402.00830v1.pdf:PDF},
 groups = {bledy w SI},
 primaryclass = {cs.CY},
 title = {Common errors in Generative AI systems used for knowledge extraction in the climate action domain},
 url = {https://arxiv.org/abs/2402.00830},
 year = {2024}
}

@inproceedings{He2015Deep,
 address = {Los Alamitos, CA, USA},
 author = {K. He and X. Zhang and S. Ren and J. Sun},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.90},
 file = {:PDF/ResNet_He2015.pdf:PDF},
 issn = {1063-6919},
 keywords = {training;degradation;complexity theory;image recognition;neural networks;visualization;image segmentation},
 pages = {770-778},
 publisher = {IEEE Computer Society},
 title = {Deep Residual Learning for Image Recognition},
 year = {2016}
}

@inproceedings{He2017Mask,
 author = {K. {He} and G. {Gkioxari} and P. {Dollár} and R. {Girshick}},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2017.322},
 file = {:PDF/Mask_R_CNN_He_2017.pdf:PDF},
 pages = {2980-2988},
 title = {Mask R-CNN},
 year = {2017}
}

@inproceedings{He2019HumanAtt,
 author = {He, Sen and Tavakoli, Hamed Rezazadegan and Borji, Ali and Pugeault, Nicolas},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2019.00862},
 file = {:PDF/Human_Attention_in_Image_Captioning_Dataset_and_Analysis.pdf:PDF},
 groups = {attention},
 issn = {2380-7504},
 keywords = {Task analysis;Visualization;Data collection;Cows;Computer vision;Computational modeling;Adaptation models},
 pages = {8528-8537},
 title = {Human Attention in Image Captioning: Dataset and Analysis},
 year = {2019}
}

@inproceedings{He2021Image,
 address = {Cham},
 author = {He, Sen and Liao, Wentong and Tavakoli, Hamed R. and Yang, Michael and Rosenhahn, Bodo and Pugeault, Nicolas},
 booktitle = {Computer Vision -- ACCV 2020},
 comment = {https://github.com/wtliao/ImageTransformer},
 editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
 file = {:PDF/Image Captioning Through Image Transformer.pdf:PDF;:PDF/Image Captioning Through Image Transformer.pdf:PDF},
 isbn = {978-3-030-69538-5},
 pages = {153--169},
 printed = {yes},
 publisher = {Springer International Publishing},
 title = {Image Captioning Through Image Transformer},
 year = {2021}
}

@inproceedings{Heaton2016Empirical,
 author = {Heaton, Jeff},
 booktitle = {SoutheastCon 2016},
 doi = {10.1109/secon.2016.7506650},
 file = {:PDF/1701.07852v2.pdf:PDF},
 groups = {representation learning},
 pages = {1–6},
 publisher = {IEEE},
 title = {An empirical analysis of feature engineering for predictive modeling},
 year = {2016}
}

@inproceedings{Hendricks2016Deep,
 author = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.8},
 file = {:PDF/Deep Compositional Captioning Describing Novel Object Categories without Paired Training Data.pdf:PDF},
 groups = {Describing novel objects},
 pages = {1-10},
 title = {Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data},
 year = {2016}
}

@misc{Herdade2019Image,
      title={Image Captioning: Transforming Objects into Words},
      author={Simao Herdade and Armin Kappeler and Kofi Boakye and Joao Soares},
      year={2020},
      eprint={1906.05963},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Hessel2021CLIPScoreAR,
 author = {Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Joseph Le Bras and Yejin Choi},
 booktitle = {EMNLP},
 file = {:PDF/CLIPScore- A Reference-free Evaluation Metric for Image Captioning.pdf:PDF},
 title = {CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
 year = {2021}
}

@Misc{Hirst2017neural,
  author    = {Hirst, G and Goldberg, Y},
  title     = {Neural Network Methods for Natural Language Processing},
  year      = {2017},
  file      = {:PDF/Neural_Network_Methods_for_Natural_Language_Proces.pdf:PDF},
  groups    = {lm},
  publisher = {San Rafael: Morgan \& Claypool Publishers},
}

@article{Ho2021EncDec,
 author = {Seoung-Ho Choi and Seoung Yeon Jo and Sung Hoon Jung},
 doi = {10.1016/j.icte.2020.08.004},
 file = {:PDF/1-s2.0-S2405959520301429-main.pdf:PDF},
 groups = {attention},
 issn = {2405-9595},
 journal = {ICT Express},
 keywords = {Image captioning, Comparative analysis},
 number = {1},
 pages = {121-125},
 title = {Component based comparative analysis of each module in image captioning},
 url = {https://www.sciencedirect.com/science/article/pii/S2405959520301429},
 volume = {7},
 year = {2021}
}

@article{Hochreiter1997LSTM,
 address = {Cambridge, MA, USA},
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 doi = {10.1162/neco.1997.9.8.1735},
 file = {:PDF/2604.pdf:PDF},
 issn = {0899-7667},
 issue_date = {November 15, 1997},
 journal = {Neural Comput.},
 number = {8},
 numpages = {46},
 pages = {1735–1780},
 publisher = {MIT Press},
 title = {Long Short-Term Memory},
 volume = {9},
 year = {1997}
}

@article{HochReiter1998Vanishing,
 author = {Hochreiter, Sepp},
 doi = {10.1142/S0218488598000094},
 file = {:PDF/The_Vanishing_Gradient_Problem_During_Learning_Rec.pdf:PDF},
 journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
 pages = {107-116},
 title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
 volume = {6},
 year = {1998}
}

@incollection{Hochreiter2001,
 added-at = {2008-02-26T12:05:08.000+0100},
 author = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
 biburl = {https://www.bibsonomy.org/bibtex/279df6721c014a00bfac62abd7d5a9968/schaul},
 booktitle = {A Field Guide to Dynamical Recurrent Neural Networks},
 citeulike-article-id = {2374777},
 description = {idsia},
 editor = {Kremer, S. C. and Kolen, J. F.},
 file = {:PDF/Gradient_Flow_in_Recurrent_Nets_the_Difficulty_of_.pdf:PDF},
 interhash = {485c1bd6a99186c9414c6b9ddaed42c9},
 intrahash = {79df6721c014a00bfac62abd7d5a9968},
 keywords = {daanbib},
 priority = {2},
 publisher = {IEEE Press},
 timestamp = {2008-02-26T12:07:01.000+0100},
 title = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
 year = {2001}
}

@inproceedings{Hodosh2013Senetence,
 author = {Hodosh, Micah and Hockenmaier, Julia},
 booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
 doi = {10.1109/CVPRW.2013.51},
 file = {:PDF/Hodosh_Sentence-Based_Image_Description_2013_CVPR_paper.pdf:PDF},
 groups = {retrieval},
 issn = {2160-7516},
 keywords = {Training;Kernel;Joints;Computational modeling;Measurement;Visualization;Training data},
 pages = {294-300},
 title = {Sentence-Based Image Description with Scalable, Explicit Models},
 year = {2013}
}

@inproceedings{Hoiem2012Diagnosing,
 address = {Berlin, Heidelberg},
 author = {Hoiem, Derek and Chodpathumwan, Yodsawalai and Dai, Qieyun},
 booktitle = {Computer Vision -- ECCV 2012},
 doi = {10.1007/978-3-642-33712-3_25},
 editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
 file = {:pdf/eccv2012_detanalysis_derek.pdf:PDF},
 isbn = {978-3-642-33712-3},
 owner = {Marcin},
 pages = {340--353},
 publisher = {Springer Berlin Heidelberg},
 timestamp = {2021-05-21},
 title = {Diagnosing Error in Object Detectors},
 url = {https://link.springer.com/chapter/10.1007/978-3-642-33712-3_25},
 year = {2012}
}

@article{Holtzman2019,
 author = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1904-09751.bib},
 eprint = {1904.09751},
 eprinttype = {arXiv},
 file = {:PDF/1904.09751.pdf:PDF},
 journal = {CoRR},
 timestamp = {Sat, 29 Apr 2023 10:09:28 +0200},
 title = {The Curious Case of Neural Text Degeneration},
 url = {http://arxiv.org/abs/1904.09751},
 volume = {abs/1904.09751},
 year = {2019}
}

@inproceedings{honda-etal-2021-removing,
 address = {Online},
 author = {Honda, Ukyo and Ushiku, Yoshitaka and Hashimoto, Atsushi and Watanabe, Taro and Matsumoto, Yuji},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 comment = {https://github.com/ukyh/RemovingSpuriousAlignment},
 doi = {10.18653/v1/2021.eacl-main.323},
 file = {:PDF/Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning.pdf:PDF},
 pages = {3692--3702},
 publisher = {Association for Computational Linguistics},
 title = {Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning},
 url = {https://aclanthology.org/2021.eacl-main.323},
 year = {2021}
}

@book{Horn2012Matrix,
 author = {Horn, Roger A and Johnson, Charles R},
 file = {:PDF/Roger_A.Horn. _Matrix_Analysis_2nd_edition(BookSee.org).pdf:PDF},
 publisher = {Cambridge university press},
 title = {Matrix analysis},
 year = {2012}
}

@article{Hossain2019Comprehensive,
 address = {New York, NY, USA},
 articleno = {118},
 author = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
 doi = {10.1145/3295748},
 file = {:PDF/A Comprehensive Survey of Deep Learning for Image Captioning.pdf:PDF},
 groups = {review},
 issn = {0360-0300},
 issue_date = {February 2019},
 journal = {ACM Comput. Surv.},
 keywords = {natural language processing, computer vision, CNN, LSTM, deep learning, Image captioning},
 number = {6},
 numpages = {36},
 printed = {yes},
 publisher = {Association for Computing Machinery},
 title = {A Comprehensive Survey of Deep Learning for Image Captioning},
 url = {https://doi.org/10.1145/3295748},
 volume = {51},
 year = {2019}
}

@misc{Howard2017Mobilenets,
      title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
      author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
      year={2017},
      eprint={1704.04861},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{https://doi.org/10.48550/arxiv.2210.14472,
 author = {Weeraprameshwara, Gihan and Jayawickrama, Vihanga and de Silva, Nisansa and Wijeratne, Yudhanjaya},
 copyright = {Creative Commons Attribution 4.0 International},
 doi = {10.48550/ARXIV.2210.14472},
 file = {:PDF/Sinhala Sentence Embedding- A Two-Tiered Structure for Low-Resource Languages .pdf:PDF},
 keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource Languages},
 url = {https://arxiv.org/abs/2210.14472},
 year = {2022}
}

@article{Hu2018RelationNF,
 author = {Han Hu and Jiayuan Gu and Zheng Zhang and Jifeng Dai and Yichen Wei},
 comment = {https://github.com/msracver/ Relation-Networks-for-Object-Detection},
 file = {:PDF/Relation Networks for Object Detection.pdf:PDF},
 journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {3588-3597},
 printed = {yes},
 title = {Relation Networks for Object Detection},
 year = {2018}
}

@article{Hu2020Vivo,
 author = {Xiaowei Hu and Xi Yin and Kevin Lin and Lijuan Wang and Lei Zhang and Jianfeng Gao and Zicheng Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2009-13682.bib},
 eprint = {2009.13682},
 eprinttype = {arXiv},
 file = {:PDF/VIVO- Visual Vocabulary Pre-Training for Novel Object Captioning.pdf:PDF},
 journal = {CoRR},
 timestamp = {Fri, 21 May 2021 15:45:48 +0200},
 title = {{VIVO:} Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training},
 url = {https://arxiv.org/abs/2009.13682},
 volume = {abs/2009.13682},
 year = {2020}
}

@inproceedings{huang2017densely,
 address = {Los Alamitos, CA, USA},
 author = {G. Huang and Z. Liu and L. Van Der Maaten and K. Q. Weinberger},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2017.243},
 file = {:PDF/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf:PDF},
 issn = {1063-6919},
 keywords = {training;convolution;network architecture;convolutional codes;neural networks;road transportation},
 pages = {2261-2269},
 publisher = {IEEE Computer Society},
 title = {Densely Connected Convolutional Networks},
 year = {2017}
}

@inproceedings{Huang2019adaptively,
 author = {Huang, Lun and Wang, Wenmin and Xia, Yaxian and Chen, Jie},
 booktitle = {Advances in Neural Information Processing Systems},
 comment = {https://github.com/husthuaan/AAT},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 file = {:PDF/NeurIPS-2019-adaptively-aligned-image-captioning-via-adaptive-attention-time-Paper.pdf:PDF},
 groups = {Attention Over Visual Regions, Two-layer LSTM},
 publisher = {Curran Associates, Inc.},
 title = {Adaptively Aligned Image Captioning via Adaptive Attention Time},
 volume = {32},
 year = {2019}
}

@InProceedings{Huang2019attention,
  author    = {Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle = {International Conference on Computer Vision},
  title     = {Attention on Attention for Image Captioning},
  year      = {2019},
  file      = {:PDF/Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
  groups    = {Self-Attention Encoding, Boosting LSTM with Self-Attention, attention taxonomy},
  printed   = {yes},
}

@article{Huang2020Image,
 author = {Huang, Yiqing and Chen, Jiansheng and Ouyang, Wanli and Wan, Weitao and Xue, Youze},
 comment = {https://github.com/RubickH/Image-Captioning-with-MAD-and-SAP},
 doi = {10.1109/TIP.2020.2969330},
 file = {:PDF/Image captioning with end-to-end attribute detection and subsequent attributes prediction.pdf:PDF},
 journal = {IEEE Transactions on Image Processing},
 pages = {4013-4026},
 title = {Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction},
 volume = {29},
 year = {2020}
}

@article{Islam2021ExploringVC,
 author = {Saiful Islam and Aurpan Dash and Ashek Seum and Amir Hossain Raj and Tonmoy Hossain and F. Shah},
 file = {:PDF/Exploring Video Captioning Techniques A Comprehensive Survey on Deep Learning Methods.pdf:PDF},
 journal = {SN Comput. Sci.},
 pages = {120},
 title = {Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods},
 url = {https://link.springer.com/content/pdf/10.1007/s42979-021-00487-x.pdf},
 volume = {2},
 year = {2021}
}

@inproceedings{Iwanowski2021Fuzzy,
 author = {Iwanowski, Marcin and Bartosiewicz, Mateusz},
 booktitle = {2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 doi = {10.1109/FUZZ45933.2021.9494549},
 file = {:PDF/Describing_images_using_fuzzy_mutual_position_matrix_and_saliency-based_ordering_of_predicates.pdf:PDF},
 groups = {moje},
 issn = {1558-4739},
 keywords = {Deep learning;Image retrieval;Object detection;Manuals;Real-time systems;Table lookup;Labeling;image description;object detection;fuzzy reasoning},
 pages = {1-8},
 title = {Describing images using fuzzy mutual position matrix and saliency-based ordering of predicates},
 year = {2021}
}

@article{Jaiswal2021Image,
 author = {Jaiswal, Tarun and others},
 file = {:PDF/Image Captioning through IOT .pdf:PDF},
 journal = {Turkish Journal of Computer and Mathematics Education (TURCOMAT)},
 number = {9},
 pages = {333--351},
 title = {Image Captioning through Cognitive IOT and Machine-Learning Approaches},
 url = {https://www.turcomat.org/index.php/turkbilmat/article/download/3077/2642},
 volume = {12},
 year = {2021}
}

@book{janusz_s_bien_2009_3944137,
 author = {Janusz S. Bień},
 doi = {10.5281/zenodo.3944137},
 file = {:PDF/JSB_PROFOSP_CMYK (1).pdf:PDF},
 groups = {opis polskiego},
 publisher = {Zenodo},
 title = {Problemy formalnego opisu składni polskiej},
 url = {https://doi.org/10.5281/zenodo.3944137},
 year = {2009}
}

@article{Jaworski2010Use,
 affiliation = {Computer Engineering Department, Technical University of Lodz, Poland},
 author = {Jaworski, T. and Kucharski, J.},
 file = {:PDF/Jaworski.pdf:PDF},
 journal = {Automatyka / Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie},
 keywords = {logika rozmyta; relacje przestrzenne; kierunkowe relacje przestrzenne},
 language = {Polish},
 pages = {563-580},
 title = {The use of fuzzy logic for description of spatial relations between objects},
 url = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-article-AGH1-0025-0088},
 volume = {T. 14, z. 3/1},
 year = {2010}
}

@misc{Jegham2025yoloevolutioncomprehensivebenchmark,
 archiveprefix = {arXiv},
 author = {Nidhal Jegham and Chan Young Koh and Marwan Abdelatti and Abdeltawab Hendawi},
 eprint = {2411.00201},
 file = {:PDF/2411.00201v2.pdf:PDF},
 primaryclass = {cs.CV},
 title = {YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions},
 url = {https://arxiv.org/abs/2411.00201},
 year = {2025}
}

@article{Jegou2011Product,
 author = {H. {Jégou} and M. {Douze} and C. {Schmid}},
 doi = {10.1109/TPAMI.2010.57},
 file = {:PDF/Product Quantization for Nearest Neighbor Search.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {1},
 pages = {117-128},
 title = {Product Quantization for Nearest Neighbor Search},
 volume = {33},
 year = {2011}
}

@article{Ji2020ImprovingIC,
author = {Jiayi, Ji and Luo, Yunpeng and Sun, Xiaoshuai and Chen, Fuhai and Luo, Gen and Yongjian, Wu and Gao, Yue and Ji, Rongrong},
year = {2021},
month = {05},
pages = {1655-1663},
title = {Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network},
volume = {35},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v35i2.16258}
}

@inproceedings{Jia2015Guiding,
 address = {USA},
 author = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
 booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2015.277},
 file = {:PDF/1509.04942v1.pdf:PDF},
 groups = {encoder-decoder-lit, global CNN features},
 isbn = {9781467383912},
 numpages = {9},
 pages = {2407–2415},
 publisher = {IEEE Computer Society},
 series = {ICCV '15},
 title = {Guiding the Long-Short Term Memory Model for Image Caption Generation},
 year = {2015}
}

@article{Jia2021Scaling,
 author = {Chao Jia and Yinfei Yang and Ye Xia and Yi{-}Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yun{-}Hsuan Sung and Zhen Li and Tom Duerig},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2102-05918.bib},
 eprint = {2102.05918},
 eprinttype = {arXiv},
 file = {:PDF/Scaling up visual and vision-language representation learning with noisy text supervision.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 05 May 2021 14:06:23 +0200},
 title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
 url = {https://arxiv.org/abs/2102.05918},
 volume = {abs/2102.05918},
 year = {2021}
}

@inproceedings{Jiang2013Salient,
 author = {Jiang, Huaizu and Wang, Jingdong and Yuan, Zejian and Wu, Yang and Zheng, Nanning and Li, Shipeng},
 booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2013.271},
 file = {:PDF/2312.05349v2.pdf:PDF},
 issn = {1063-6919},
 keywords = {Image segmentation;Vectors;Image color analysis;Feature extraction;Object detection;Histograms;Training},
 pages = {2083-2090},
 title = {Salient Object Detection: A Discriminative Regional Feature Integration Approach},
 year = {2013}
}

@inproceedings{Jiang2018LearningToGuide,
 author = {Wenhao Jiang and Lin Ma and Xinpeng Chen and Hanwang Zhang and Wei Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/JiangMCZL18.bib},
 booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018},
 doi = {10.1609/AAAI.V32I1.12283},
 editor = {Sheila A. McIlraith and Kilian Q. Weinberger},
 file = {:PDF/12283-13-15811-1-2-20201228.pdf:PDF},
 pages = {6959--6966},
 publisher = {{AAAI} Press},
 timestamp = {Mon, 04 Sep 2023 16:50:25 +0200},
 title = {Learning to Guide Decoding for Image Captioning},
 url = {https://doi.org/10.1609/aaai.v32i1.12283},
 year = {2018}
}

@inproceedings{Jiang2018RecurrentFusion,
 address = {Berlin, Heidelberg},
 author = {Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong},
 booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II},
 doi = {10.1007/978-3-030-01216-8_31},
 file = {:PDF/Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper.pdf:PDF},
 groups = {Multi-level features},
 isbn = {978-3-030-01215-1},
 keywords = {Image captioning, Encoder-decoder framework, Recurrent fusion network (RFNet)},
 location = {Munich, Germany},
 numpages = {17},
 pages = {510–526},
 publisher = {Springer-Verlag},
 title = {Recurrent Fusion Network for Image Captioning},
 year = {2018}
}

@inproceedings{Jiang2019TIGErTG,
 author = {Ming Jiang and Qiuyuan Huang and Lei Zhang and Xin Wang and Pengchuan Zhang and Zhe Gan and Jana Diesner and Jianfeng Gao},
 booktitle = {EMNLP},
 file = {:PDF/TIGEr- Text-to-Image Grounding for Image Caption Evaluation.pdf:PDF},
 title = {TIGEr: Text-to-Image Grounding for Image Caption Evaluation},
 year = {2019}
}

@article{Jiang2021Multigate,
 author = {Jiang, Weitao and Li, Xiying and Hu, Haifeng and Lu, Qiang and Liu, Bohong},
 doi = {10.1109/ACCESS.2021.3067607},
 file = {:PDF/Multi-Gate_Attention_Network_for_Image_Captioning.pdf:PDF},
 issn = {2169-3536},
 journal = {IEEE Access},
 keywords = {Decoding;Logic gates;Task analysis;Semantics;Visualization;Feature extraction;Proposals;Image captioning;self-attention;transformer;multi-gate attention},
 pages = {69700-69709},
 title = {Multi-Gate Attention Network for Image Captioning},
 volume = {9},
 year = {2021}
}

@article{Jiao2019Survey,
 author = {L. {Jiao} and F. {Zhang} and F. {Liu} and S. {Yang} and L. {Li} and Z. {Feng} and R. {Qu}},
 doi = {10.1109/ACCESS.2019.2939201},
 file = {:PDF/Jiao2019.pdf:PDF},
 journal = {IEEE Access},
 pages = {128837-128868},
 title = {A Survey of Deep Learning-Based Object Detection},
 volume = {7},
 year = {2019}
}

@article{jimaging9080162,
 article-number = {162},
 author = {Hu, Wenjin and Qiao, Lang and Kang, Wendong and Shi, Xinyue},
 doi = {10.3390/jimaging9080162},
 file = {:PDF/jimaging-09-00162.pdf:PDF},
 issn = {2313-433X},
 journal = {Journal of Imaging},
 number = {8},
 pubmedid = {37623694},
 title = {Thangka Image Captioning Based on Semantic Concept Prompt and Multimodal Feature Optimization},
 url = {https://www.mdpi.com/2313-433X/9/8/162},
 volume = {9},
 year = {2023}
}

@article{Jin2015Aligning,
 author = {Junqi Jin and Kun Fu and Runpeng Cui and Fei Sha and Changshui Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/JinFCSZ15.bib},
 eprint = {1506.06272},
 eprinttype = {arXiv},
 file = {:PDF/1506.06272v1.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 21 Feb 2022 19:31:03 +0100},
 title = {Aligning where to see and what to tell: image caption with region-based attention and scene factorization},
 url = {http://arxiv.org/abs/1506.06272},
 volume = {abs/1506.06272},
 year = {2015}
}

@inproceedings{Joachim2002Optimizing,
 address = {New York, NY, USA},
 author = {Joachims, Thorsten},
 booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 doi = {10.1145/775047.775067},
 file = {:PDF/775047.775067.pdf:PDF},
 isbn = {158113567X},
 location = {Edmonton, Alberta, Canada},
 numpages = {10},
 pages = {133–142},
 publisher = {Association for Computing Machinery},
 series = {KDD '02},
 title = {Optimizing search engines using clickthrough data},
 year = {2002}
}

@inproceedings{Johnson2015Image,
 author = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7298990},
 file = {:PDF/Image Retrieval using Scene Graphs.pdf:PDF},
 pages = {3668-3678},
 title = {Image retrieval using scene graphs},
 year = {2015}
}

@inproceedings{Johnson2016Densecap,
 author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
 booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
 file = {:PDF/DenseCap- Fully Convolutional Localization Networks for Dense Captioning.pdf:PDF},
 pages = {4565--4574},
 title = {Densecap: Fully convolutional localization networks for dense captioning},
 year = {2016}
}

@inproceedings{Johnson2018Image,
 author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
 booktitle = {CVPR},
 file = {:PDF/Image generation from scene graph.pdf:PDF},
 title = {Image Generation from Scene Graphs},
 year = {2018}
}

@book{Jurasky2023,
 address = {USA},
 author = {Jurafsky, Daniel and Martin, James H.},
 edition = {1st},
 file = {:PDF/ed3book.pdf:PDF},
 groups = {review},
 isbn = {0130950696},
 publisher = {Prentice Hall PTR},
 title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
 year = {2000}
}

@article{Jzefowicz2016ExploringTL,
 author = {Rafal J{\'o}zefowicz and Oriol Vinyals and Mike Schuster and Noam M. Shazeer and Yonghui Wu},
 file = {:PDF/1602.02410v2.pdf:PDF},
 groups = {lm},
 journal = {ArXiv},
 title = {Exploring the Limits of Language Modeling},
 url = {https://api.semanticscholar.org/CorpusID:260422},
 volume = {abs/1602.02410},
 year = {2016}
}

@article{Kam2025BenchmarkingAttention,
 archiveprefix = {arXiv},
 author = {Hemanth Teja Yanambakkam and Rahul Chinthala},
 eprint = {2502.18734},
 file = {:PDF/2502.18734v1.pdf:PDF},
 groups = {attention},
 primaryclass = {cs.CV},
 title = {Beyond RNNs: Benchmarking Attention-Based Image Captioning Models},
 url = {https://arxiv.org/abs/2502.18734},
 year = {2025}
}

@inproceedings{Karpathy2014,
 author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li F},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 file = {:PDF/NIPS-2014-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping-Paper.pdf:PDF},
 groups = {Earlier Deep Models},
 publisher = {Curran Associates, Inc.},
 title = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
 url = {https://proceedings.neurips.cc/paper/2014/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
 volume = {27},
 year = {2014}
}

@inproceedings{Karpathy2015Deep,
 address = {Los Alamitos, CA, USA},
 author = {A. Karpathy and L. Fei-Fei},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7298932},
 file = {:PDF/Deep visual-semantic alignments for generating image descriptions.pdf:PDF},
 groups = {multimodal, global CNN features},
 issn = {1063-6919},
 pages = {3128-3137},
 printed = {yes},
 publisher = {IEEE Computer Society},
 title = {Deep visual-semantic alignments for generating image descriptions},
 year = {2015}
}

@InProceedings{Ke2019Reflective,
  author    = {Ke, Lei and Pei, Wenjie and Li, Ruiyu and Shen, Xiaoyong and Tai, Yu-Wing},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Reflective Decoding Network for Image Captioning},
  year      = {2019},
  pages     = {8887-8896},
  doi       = {10.1109/ICCV.2019.00898},
  file      = {:PDF/Reflective_Decoding_Network_for_Image_Captioning.pdf:PDF},
  groups    = {Attention Over Visual Regions, Reflective attention},
  keywords  = {Decoding;Visualization;Feature extraction;Syntactics;Task analysis;Rivers;Random access memory},
}

@inproceedings{Keller1999Aspects,
 author = {J. M. {Keller} and P. {Matsakis}},
 booktitle = {FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315)},
 doi = {10.1109/FUZZY.1999.793059},
 file = {:PDF/Axpects_of_high_level_Keller_1999.pdf:PDF},
 pages = {847-852 vol.2},
 title = {Aspects of high level computer vision using fuzzy sets},
 volume = {2},
 year = {1999}
}

@article{Keller2000Fuzzy,
 author = {James M. Keller and Xiaomei Wang},
 doi = {10.1006/cviu.2000.0872},
 file = {:PDF/Fuzzy_rule_Keller_2000.pdf:PDF},
 issn = {1077-3142},
 journal = {Computer Vision and Image Understanding},
 number = {1},
 pages = {21-41},
 title = {A Fuzzy Rule-Based Approach to Scene Description Involving Spatial Relationships},
 url = {https://www.sciencedirect.com/science/article/pii/S1077314200908725},
 volume = {80},
 year = {2000}
}

@article{Khan2020Survey,
 author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
 doi = {10.1007/s10462-020-09825-6},
 file = {:PDF/Survey_CNN_Khan2019.pdf:PDF},
 issn = {1573-7462},
 journal = {Artificial Intelligence Review},
 number = {8},
 pages = {5455–5516},
 publisher = {Springer Science and Business Media LLC},
 title = {A survey of the recent architectures of deep convolutional neural networks},
 volume = {53},
 year = {2020}
}

@article{Khan2022,
 address = {New York, NY, USA},
 articleno = {200},
 author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
 comment = {https://github.com/lucidrains/x-transformers},
 doi = {10.1145/3505244},
 file = {:PDF/2101.01169.pdf:PDF},
 issn = {0360-0300},
 issue_date = {January 2022},
 journal = {ACM Comput. Surv.},
 keywords = {deep neural networks, bidirectional encoders, transformers, self-supervision, convolutional networks, literature survey, Self-attention},
 number = {10s},
 numpages = {41},
 publisher = {Association for Computing Machinery},
 title = {Transformers in Vision: A Survey},
 url = {https://doi.org/10.1145/3505244},
 volume = {54},
 year = {2022}
}

@article{Khan2022SI,
 author = {Khan, Rashid and Islam, M Shujah and Kanwal, Khadija and Iqbal, Mansoor and Hossain, Md Imran and Ye, Zhongfu},
 file = {:PDF/2203.01594v1.pdf:PDF},
 journal = {arXiv [cs.CL]},
 primaryclass = {cs.CL},
 publisher = {arXiv},
 title = {A deep neural framework for image caption generation using {GRU-based} attention mechanism},
 year = {2022}
}

@article{KhangComparative,
 author = {Khaing, Phyu and Yu, May},
 doi = {10.5815/ijigsp.2019.06.01},
 file = {:PDF/IJIGSP-V11-N6-1.pdf:PDF},
 groups = {review},
 journal = {International Journal of Image, Graphics and Signal Processing},
 pages = {1-8},
 title = {Attention-Based Deep Learning Model for Image Captioning: A Comparative Study},
 volume = {11},
 year = {2019}
}

@InProceedings{Khurana2018FE,
  author    = {Khurana, Udayan and Samulowitz, Horst and Turaga, Deepak},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  title     = {Feature engineering for predictive modeling using reinforcement learning},
  year      = {2018},
  publisher = {AAAI Press},
  series    = {AAAI'18/IAAI'18/EAAI'18},
  articleno = {417},
  file      = {:PDF/3504035.3504452.pdf:PDF},
  groups    = {representation learning},
  isbn      = {978-1-57735-800-8},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {8},
}

@article{Khurana2021Video,
 author = {Khurana, Khushboo and Deshpande, Umesh},
 doi = {10.1109/ACCESS.2021.3058248},
 file = {:PDF/Video Question-Answering Techniques,Benchmark Datasets and EvaluationMetrics Leveraging Video Captioning A Comprehensive Survey.pdf:PDF},
 journal = {IEEE Access},
 pages = {43799-43823},
 title = {Video Question-Answering Techniques, Benchmark Datasets and Evaluation Metrics Leveraging Video Captioning: A Comprehensive Survey},
 volume = {9},
 year = {2021}
}

@article{Kieraś_Woliński_2017,
 abstractnote = {Morfeusz jest aplikacją znaną polskim badaczom z kręgu językoznawstwa komputerowego od ponad 10 lat. W artykule przedstawiamy jego nową wersję, skupiając się na zmianach, które pojawiły się od poprzedniej wersji programu.},
 author = {Kieraś, Witold and Woliński, Marcin},
 doi = {10.31286/JP.97.1.7},
 file = {:PDF/[1_2017]77.pdf:PDF},
 groups = {opis polskiego},
 journal = {Język Polski},
 number = {1},
 pages = {75–83},
 title = {Morfeusz 2 – analizator i generator fleksyjny dla języka polskiego},
 url = {https://jezyk-polski.pl/index.php/jp/article/view/453},
 volume = {97},
 year = {2017}
}

@article{Kilimci2020Sentiment,
 abstractnote = {Sentiment analysis is a considerable research field to analyze huge amount of information and specify user opinions on many things and is summarized as the extraction of users’ opinions from the text. Like sentiment analysis, Bitcoin which is a digital cryptocurrency also attracts the researchers considerably in the fields of economics, cryptography, and computer science. The purpose of this study is to forecast the direction of Bitcoin price by analysing user opinions in social media such as Twitter. To our knowledge, this is the very first attempt which estimates the direction of Bitcoin price fluctuations by using deep learning and word embedding models in the state-of-the-art studies. For the purpose of estimating the direction of Bitcoin, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long-short term memory networks (LSTMs) are used as deep learning architectures and Word2Vec, GloVe, and FastText are employed as word embedding models in the experiments. In order to demonstrate the contibution of our work, experiments are carried out on English Twitter dataset. Experiment results show that the usage of FastText model as a word embedding model outperforms other models with 89.13% accuracy value to estimate the direction of Bitcoin price.},
 author = {Kilimci, Zeynep Hilal},
 doi = {10.18201/ijisae.2020261585},
 file = {:Fuzzy.bib (conflicted copy 2021-09-15 142929).sav:sav;:PDF/Sentiment Analysis Based DirectionPrediction in Bitcoin using Deep Learning Algorithms and Word Embedding Models.pdf:PDF},
 journal = {International Journal of Intelligent Systems and Applications in Engineering},
 number = {2},
 pages = {60–65},
 title = {Sentiment Analysis Based Direction Prediction in Bitcoin using Deep Learning Algorithms and Word Embedding Models},
 url = {https://ijisae.org/index.php/IJISAE/article/view/1062},
 volume = {8},
 year = {2020}
}

@article{Kingma2014AdamOptimizer,
 address = {San Diega, CA, USA},
 author = {Kingma, Diederik and Ba, Jimmy},
 booktitle = {International Conference on Learning Representations (ICLR)},
 file = {:PDF/1412.6980v9.pdf:PDF},
 optmonth = {12},
 title = {Adam: A Method for Stochastic Optimization},
 year = {2015}
}

@inproceedings{Kirillov2019Panoptic,
 author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2019.00963},
 file = {:PDF/Panoptic_Segmentation.pdf:PDF},
 issn = {2575-7075},
 keywords = {Recognition: Detection;Categorization;Retrieval;Segmentation;Grouping and Shape},
 pages = {9396-9405},
 title = {Panoptic Segmentation},
 year = {2019}
}

@inproceedings{Kiros2014Multimodal,
 address = {Bejing, China},
 author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Rich},
 booktitle = {Proceedings of the 31st International Conference on Machine Learning},
 editor = {Xing, Eric P. and Jebara, Tony},
 file = {:PDF/kiros14.pdf:PDF},
 groups = {encoder-decoder-lit},
 number = {2},
 pages = {595--603},
 pdf = {http://proceedings.mlr.press/v32/kiros14.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Multimodal Neural Language Models},
 url = {https://proceedings.mlr.press/v32/kiros14.html},
 year = {2014}
}

@misc{KirosSZ2014Unifying,
      title={Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models},
      author={Ryan Kiros and Ruslan Salakhutdinov and Richard S. Zemel},
      year={2014},
      eprint={1411.2539},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{Kleczek2020Polbert,
 author = {Dariusz Kłeczek},
 booktitle = {Proceedings of the PolEval 2020 Workshop},
 editor = {Maciej Ogrodniczuk and Łukasz Kobyliński},
 publisher = {Institute of Computer Science, Polish Academy of Sciences},
 title = {Polbert: Attacking Polish NLP Tasks with Transformers},
 year = {2020}
}

@inproceedings{Klein2003Accurate,
 address = {Sapporo, Japan},
 author = {Klein, Dan and Manning, Christopher D.},
 booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
 doi = {10.3115/1075096.1075150},
 file = {:PDF/P03-1054.pdf:PDF},
 pages = {423--430},
 publisher = {Association for Computational Linguistics},
 title = {Accurate Unlexicalized Parsing},
 year = {2003}
}

@inproceedings{Koehn2005Europarl,
 address = {Phuket, Thailand},
 author = {Koehn, Philipp},
 booktitle = {Proceedings of Machine Translation Summit X: Papers},
 file = {:PDF/2005.mtsummit-papers.11.pdf:PDF},
 pages = {79--86},
 title = {{E}uroparl: A Parallel Corpus for Statistical Machine Translation},
 url = {https://aclanthology.org/2005.mtsummit-papers.11},
 year = {2005}
}

@article{kossen2021self,
 author = {Kossen, Jannik and Band, Neil and Gomez, Aidan N. and Lyle, Clare and Rainforth, Tom and Gal, Yarin},
 comment = {https://github.com/OATML/non-parametric-transformers},
 file = {:PDF/2106.02584.pdf:PDF},
 journal = {arXiv:2106.02584},
 title = {Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning},
 year = {2021}
}

@article{Kowsari2019,
 article-number = {150},
 author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
 doi = {10.3390/info10040150},
 issn = {2078-2489},
 journal = {Information},
 number = {4},
 title = {Text Classification Algorithms: A Survey},
 url = {https://www.mdpi.com/2078-2489/10/4/150},
 volume = {10},
 year = {2019}
}

@inproceedings{Krause2016Paragraphs,
 author = {Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},
 booktitle = {Computer Vision and Patterm Recognition (CVPR)},
 file = {:PDF/A Hierarchical Approach for Generating Descriptive Image Paragraphs.pdf:PDF},
 title = {A Hierarchical Approach for Generating Descriptive Image Paragraphs},
 url = {https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html},
 year = {2017}
}

@Article{Krishna2016Visualgenome,
  author     = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  journal    = {Int. J. Comput. Vision},
  title      = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  year       = {2017},
  issn       = {0920-5691},
  month      = may,
  number     = {1},
  pages      = {32–73},
  volume     = {123},
  abstract   = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that "the person is riding a horse-drawn carriage." In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of $$35$$35 objects, $$26$$26 attributes, and $$21$$21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
  address    = {USA},
  doi        = {10.1007/s11263-016-0981-7},
  file       = {:PDF/Visual_Genome.pdf:PDF},
  issue_date = {May 2017},
  keywords   = {Scene graph, Relationships, Question answering, Objects, Language, Knowledge, Image, Dataset, Crowdsourcing, Computer vision, Attributes},
  numpages   = {42},
  publisher  = {Kluwer Academic Publishers},
}

@article{Krishnapuram1993Qualititative,
 author = {R. {Krishnapuram} and J. M. {Keller} and Y. {Ma}},
 doi = {10.1109/91.236554},
 file = {:PDF/Quantitative_Analysis_Krishnapuram_1993.pdf:PDF},
 journal = {IEEE Transactions on Fuzzy Systems},
 number = {3},
 pages = {222-233},
 title = {Quantitative analysis of properties and spatial relations of fuzzy image regions},
 volume = {1},
 year = {1993}
}

@inproceedings{Krizhevsky2012,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 file = {:PDF/Krizhevsky2012.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{KuipersLevitt1988Navigation,
 abstractnote = {In a large-scale space, structure is at a significantly larger scale than the observations available at an instant. To learn the structure of a large-scale space from observations, the observer must build a cognitive map of the environment by integrating observations over an extended period of time, inferring spatial structure from perceptions and the effects of actions. The cognitive map representation of large-scale space must account for a mapping, or learning structure from observations, and navigation, or creating and executing a plan to travel from one place to another. Approaches to date tend to be fragile either because they don’t build maps; or because they assume nonlocal observations, such as those available in preexisting maps or global coordinate systems, including active landmark beacons and geo-locating satellites. We propose that robust navigation and mapping systems for large-scale space can be developed by adhering to a natural, four-level semantic hierarchy of descriptions for representation, planning, and execution of plans in large-scale space. The four levels are sensorimotor interaction, procedural behaviors, topological mapping, and metric mapping. Effective systems represent the environment, relative to sensors, at all four levels and formulate robust system behavior by moving flexibly between representational levels at run time. We demonstrate our claims in three implemented models: Tour, the Qualnav system simulator, and the NX robot.},
 author = {Kuipers, Benjamin J. and Levitt, Todd S.},
 doi = {10.1609/aimag.v9i2.674},
 file = {:PDF/674-Article Text-671-1-10-20080129.pdf:PDF},
 journal = {AI Magazine},
 number = {2},
 pages = {25},
 title = {Navigation and Mapping in Large Scale Space},
 url = {https://ojs.aaai.org/index.php/aimagazine/article/view/674},
 volume = {9},
 year = {1988}
}

@inproceedings{Kulkarni2011SimpleImgDesc,
 author = {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li, Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara L},
 booktitle = {CVPR 2011},
 doi = {10.1109/CVPR.2011.5995466},
 file = {:PDF/Baby_talk_Understanding_and_generating_simple_image_descriptions.pdf:PDF},
 groups = {template},
 issn = {1063-6919},
 keywords = {Labeling;Detectors;Computer vision;Natural languages;Visualization;Object detection;Image recognition},
 pages = {1601-1608},
 title = {Baby talk: Understanding and generating simple image descriptions},
 year = {2011}
}

@article{Kumar2017Survey,
 author = {Kumar, Akshi and Goel, Shivali},
 doi = {10.3233/HIS-170246},
 file = {:PDF/A survey of evolution of image captioning techniques.pdf:PDF},
 groups = {review},
 journal = {International Journal of Hybrid Intelligent Systems},
 pages = {1-19},
 title = {A survey of evolution of image captioning techniques},
 volume = {14},
 year = {2017}
}

@article{Kumar2022Dual,
 article-number = {6733},
 author = {Kumar, Deepika and Srivastava, Varun and Popescu, Daniela Elena and Hemanth, Jude D.},
 doi = {10.3390/app12136733},
 file = {:PDF/applsci-12-06733.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {13},
 title = {Dual-Modal Transformer with Enhanced Inter- and Intra-Modality Interactions for Image Captioning},
 url = {https://www.mdpi.com/2076-3417/12/13/6733},
 volume = {12},
 year = {2022}
}

@inproceedings{Kusner2015FromWE,
 author = {Matt J. Kusner and Yu Sun and Nicholas I. Kolkin and Kilian Q. Weinberger},
 booktitle = {ICML},
 file = {:PDF/From Word Embeddings To Document Distances.pdf:PDF},
 groups = {metryki},
 title = {From Word Embeddings To Document Distances},
 year = {2015}
}

@inproceedings{Kuznetsova2012Collective,
 address = {Jeju Island, Korea},
 author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Alexander and Berg, Tamara and Choi, Yejin},
 booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 editor = {Li, Haizhou and Lin, Chin-Yew and Osborne, Miles and Lee, Gary Geunbae and Park, Jong C.},
 file = {:PDF/P12-1038.pdf:PDF},
 groups = {retrieval},
 pages = {359--368},
 publisher = {Association for Computational Linguistics},
 title = {Collective Generation of Natural Image Descriptions},
 year = {2012}
}

@article{Kuznetsova2014Treetalk,
 address = {Cambridge, MA},
 author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Tamara L. and Choi, Yejin},
 doi = {10.1162/tacl_a_00188},
 editor = {Lin, Dekang and Collins, Michael and Lee, Lillian},
 file = {:PDF/tacl_a_00188.pdf:PDF},
 groups = {retrieval},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {351--362},
 publisher = {MIT Press},
 title = {{T}ree{T}alk: Composition and Compression of Trees for Image Descriptions},
 url = {https://aclanthology.org/Q14-1028},
 volume = {2},
 year = {2014}
}

@inproceedings{Lan2017Fluency,
 address = {New York, NY, USA},
 author = {Lan, Weiyu and Li, Xirong and Dong, Jianfeng},
 booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
 doi = {10.1145/3123266.3123366},
 file = {:PDF/Fluency-Guided Cross-Lingual Image Captioning.pdf:PDF},
 isbn = {9781450349062},
 keywords = {english-chinese, cross-lingual image captioning, sentence fluency},
 location = {Mountain View, California, USA},
 numpages = {9},
 pages = {1549–1557},
 publisher = {Association for Computing Machinery},
 series = {MM '17},
 title = {Fluency-Guided Cross-Lingual Image Captioning},
 url = {https://doi.org/10.1145/3123266.3123366},
 year = {2017}
}

@article{Landola2016SqueezeNet,
 author = {Forrest N. Iandola and Matthew W. Moskewicz and Khalid Ashraf and Song Han and William J. Dally and Kurt Keutzer},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/IandolaMAHDK16.bib},
 eprint = {1602.07360},
 eprinttype = {arXiv},
 file = {:PDF/Iandola2016.pdf:PDF},
 journal = {CoRR},
 primaryclass = {cs.CV},
 timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
 title = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB model size},
 url = {http://arxiv.org/abs/1602.07360},
 volume = {abs/1602.07360},
 year = {2016}
}

@book{Lappin1996LAPTHO,
 address = {Cambridge, Mass., USA},
 editor = {Shalom Lappin},
 file = {:PDF/The Handbook of Contemporary Semantic Theory - 2015 - Lappin.pdf:PDF},
 publisher = {Blackwell Reference},
 title = {The Handbook of Contemporary Semantic Theory},
 year = {1996}
}

@inproceedings{Le2014Distributed,
 author = {Le, Quoc and Mikolov, Tomas},
 booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
 file = {:PDF/paragraph_vector.pdf:PDF},
 groups = {Probabilistic Neural Network Language Model},
 location = {Beijing, China},
 pages = {II–1188–II–1196},
 publisher = {JMLR.org},
 series = {ICML'14},
 title = {Distributed representations of sentences and documents},
 year = {2014}
}

@inproceedings{Lebret2015,
 author = {R{\'{e}}mi Lebret and Pedro H. O. Pinheiro and Ronan Collobert},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/LebretPC14.bib},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
 editor = {Yoshua Bengio and Yann LeCun},
 file = {:PDF/1412.8419v3.pdf:PDF},
 groups = {retrieval},
 timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
 title = {Simple Image Description Generator via a Linear Phrase-Based Approach},
 url = {http://arxiv.org/abs/1412.8419},
 year = {2015}
}

@inproceedings{Lebret2015Phrase,
 author = {Lebret, R\'{e}mi and Pinheiro, Pedro O. and Collobert, Ronan},
 booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
 file = {:PDF/lebret15.pdf:PDF},
 groups = {Earlier Deep Models},
 location = {Lille, France},
 numpages = {10},
 pages = {2085–2094},
 publisher = {JMLR.org},
 series = {ICML'15},
 title = {Phrase-Based Image Captioning},
 year = {2015}
}

@article{LeCun2015Deep,
 author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
 day = {01},
 doi = {10.1038/nature14539},
 file = {:PDF/nature14539.pdf:PDF},
 issn = {1476-4687},
 journal = {Nature},
 number = {7553},
 pages = {436-444},
 title = {Deep learning},
 volume = {521},
 year = {2015}
}

@inproceedings{Lee2020Vilbert,
 address = {Online},
 author = {Lee, Hwanhee and Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Jung, Kyomin},
 booktitle = {Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems},
 doi = {10.18653/v1/2020.eval4nlp-1.4},
 file = {:PDF/ViLBERTScore_Evaluating_Image_Caption_Using_Vision.pdf:PDF},
 pages = {34--39},
 publisher = {Association for Computational Linguistics},
 title = {{V}i{LBERTS}core: Evaluating Image Caption Using Vision-and-Language {BERT}},
 url = {https://aclanthology.org/2020.eval4nlp-1.4},
 year = {2020}
}

@inproceedings{Lee2021Umic,
 address = {Online},
 author = {Lee, Hwanhee and Yoon, Seunghyun and Dernoncourt, Franck and Bui, Trung and Jung, Kyomin},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 doi = {10.18653/v1/2021.acl-short.29},
 file = {:PDF/lee2021umic.pdf:PDF},
 pages = {220--226},
 publisher = {Association for Computational Linguistics},
 title = {{UMIC}: An Unreferenced Metric for Image Captioning via Contrastive Learning},
 url = {https://aclanthology.org/2021.acl-short.29},
 year = {2021}
}

@inproceedings{Levinboim2021Quality,
 address = {Online},
 author = {Levinboim, Tomer and Thapliyal, Ashish V. and Sharma, Piyush and Soricut, Radu},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 comment = {-rodzaj esymatora jakosci(QE) z szczegolnym uwzglednieniem osob niewidomych i niedowidzacych. Jak ich najlepiej poinformowac o jakosci podpisu
- badanie jakosci podpisow z perspektywy czlowieka
- badanie jakosci podpisow jezeli nie znamy podpisu referencyjnego
- metoda bez uzycia obrazow referencyjnych poprawnych
-podpisy uzyskane z modeli opartych na metodzie Transformers i zbiorze ConceptualCaptions (3.3m trening 15kvalidacja) Zbiór ten wg @Piyush2018 daje lepsze wyniki dla obrazow bez konkretnej domeny dziedzinowej(czyli dowolnych)
- zauwazono ze lepsze wyniki maja podpisy z wieksza iloscia informacji albo mniejsza iloscia bledow
- wzor oceny: dla kazdego obrazu jest 10 ocen (0 lub 1). Wyciagamy srednia dla ok 9 obrazowz ocen i zaokraglamy
pˆ = round(mean(ri) ∗ 8)/8,
gdzie ri to
Produkty:

- zbiór Caption-Quality 65k par obraz-podpis zawierajacy oceny ludzi do wygenerowaych podpisow na przestrzeni wieloletnich badan modeli img-capt
- moze byc uzyty do badania finalnej jakosci modelu a nie w czasie rzeczywistym (klasyfikacja binarna, dobry lub nie podpis)
-  T2 Test Set oceny ludzi do podpisow dla 5 najelpszych modeli w wyzwaniu  Conceptual Captions Challenge Workshop at CVPR 2019.
-  zbudwanie rankingu na podstawie zbudowanego QE ktory decyduje czy podpis jest warty pokazania ludziom niepelnosprawnym},
 doi = {10.18653/v1/2021.naacl-main.253},
 file = {:PDF/Quality Estimation for Image Captions Based on Large-scale Human Evaluations.pdf:PDF},
 pages = {3157--3166},
 publisher = {Association for Computational Linguistics},
 title = {Quality Estimation for Image Captions Based on Large-scale Human Evaluations},
 url = {https://aclanthology.org/2021.naacl-main.253},
 year = {2021}
}

@article{Levitt1990Qualititative,
 author = {Tod S. Levitt and Daryl T. Lawton},
 doi = {10.1016/0004-3702(90)90027-W},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 number = {3},
 pages = {305-360},
 title = {Qualitative navigation for mobile robots},
 url = {https://www.sciencedirect.com/science/article/pii/000437029090027W},
 volume = {44},
 year = {1990}
}

@inproceedings{Levy2014WordEmbAsMatrixFactorization,
 address = {Cambridge, MA, USA},
 author = {Levy, Omer and Goldberg, Yoav},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 location = {Montreal, Canada},
 numpages = {9},
 pages = {2177–2185},
 publisher = {MIT Press},
 series = {NIPS'14},
 title = {Neural word embedding as implicit matrix factorization},
 year = {2014}
}

@inproceedings{Li1994MarkovMRFinCV,
  title={Markov random field models in computer vision},
  author={Li, Stan Z},
  booktitle={European conference on computer vision},
  pages={361--370},
  year={1994},
  organization={Springer International Publishing}
}

@inproceedings{li2007Clasifying,
 author = {Li, Li-Jia and Li Fei-Fei},
 booktitle = {2007 IEEE 11th International Conference on Computer Vision},
 doi = {10.1109/ICCV.2007.4408872},
 file = {:PDF/What_where_and_who_Classifying_events_by_scene_and-1.pdf:PDF},
 issn = {2380-7504},
 keywords = {Layout;Object recognition;Image recognition;Humans;Snow;Lakes;Boats;Graphical models;Assembly;Image databases},
 pages = {1-8},
 title = {What, where and who? Classifying events by scene and object recognition},
 year = {2007}
}

@inproceedings{Li2011,
 address = {Portland, Oregon, USA},
 author = {Li, Siming and Kulkarni, Girish and Berg, Tamara L and Berg, Alexander C and Choi, Yejin},
 booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
 file = {:PDF/W11-0326.pdf:PDF},
 groups = {template},
 pages = {220--228},
 publisher = {Association for Computational Linguistics},
 title = {Composing Simple Image Descriptions using Web-scale N-grams},
 url = {https://aclanthology.org/W11-0326},
 year = {2011}
}

@article{Li2012Reasoning,
 author = {Li, Sanjiang and Cohn, Anthony G.},
 doi = {10.1111/j.1467-8640.2012.00431.x},
 eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00431.x},
 file = {:PDF/Reasoning_Li_2012.pdf:PDF},
 journal = {Computational Intelligence},
 keywords = {qualitative spatial reasoning, topology, region connection calculus, rectangle algebra, joint satisfaction problem, approximation},
 number = {4},
 pages = {579-616},
 title = {REASONING WITH TOPOLOGICAL AND DIRECTIONAL SPATIAL INFORMATION},
 url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00431.x},
 volume = {28},
 year = {2012}
}

@article{Li2018GLA,
 author = {Li, Linghui and Tang, Sheng and Zhang, Yongdong and Deng, Lixi and Tian, Qi},
 doi = {10.1109/TMM.2017.2751140},
 file = {:PDF/GLA_GlobalLocal_Attention_for_Image_Description.pdf:PDF},
 groups = {attention taxonomy},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Recurrent neural networks;Decoding;Image recognition;Feature extraction;Natural language processing;Computational modeling;Convolutional neural network;recurrent neural network;image description;natural language processing},
 number = {3},
 pages = {726-737},
 title = {GLA: Global–Local Attention for Image Description},
 volume = {20},
 year = {2018}
}

@inproceedings{Li2018SemanticLSTM,
 author = {Li, Nannan and Chen, Zhenzhong},
 booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
 file = {:PDF/0110.pdf:PDF},
 groups = {Other deep learning methods},
 isbn = {9780999241127},
 location = {Stockholm, Sweden},
 numpages = {7},
 pages = {793–799},
 publisher = {AAAI Press},
 series = {IJCAI'18},
 title = {Image captioning with visual-semantic LSTM},
 year = {2018}
}

@article{Li2019Boosted,
 article-number = {3260},
 author = {Li, Jiangyun and Yao, Peng and Guo, Longteng and Zhang, Weicun},
 doi = {10.3390/app9163260},
 file = {:PDF/applsci-09-03260.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {16},
 title = {Boosted Transformer for Image Captioning},
 url = {https://www.mdpi.com/2076-3417/9/16/3260},
 volume = {9},
 year = {2019}
}

@inproceedings{Li2019Entangled,
 author = {Li, Guang and Zhu, Linchao and Liu, Ping and Yang, Yi},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2019.00902},
 file = {:PDF/Entangled_Transformer_for_Image_Captioning.pdf:PDF},
 groups = {Early self-attention approaches, Gating mechanisms.},
 issn = {2380-7504},
 keywords = {Semantics;Visualization;Decoding;Encoding;Logic gates;Snow;Proposals},
 pages = {8927-8936},
 title = {Entangled Transformer for Image Captioning},
 year = {2019}
}

@misc{Li2019generatingdiverseaccuratevisual,
 archiveprefix = {arXiv},
 author = {Dianqi Li and Qiuyuan Huang and Xiaodong He and Lei Zhang and Ming-Ting Sun},
 eprint = {1804.00861},
 file = {:PDF/27.pdf:PDF},
 groups = {Other deep learning methods},
 primaryclass = {cs.CV},
 title = {Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning},
 url = {https://arxiv.org/abs/1804.00861},
 year = {2019}
}

@inproceedings{Li2019MetaLF,
 author = {Nannan Li and Zhenzhong Chen and Shan Liu},
 booktitle = {AAAI},
 file = {:PDF/Meta Learning for Image Captioning .pdf:PDF},
 title = {Meta Learning for Image Captioning},
 year = {2019}
}

@inproceedings{Li2020Context,
 author = {Li, Zhuowan and Tran, Quan and Mai, Long and Lin, Zhe and Yuille, Alan L.},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR42600.2020.00350},
 file = {:PDF/Context-aware group captioning via self-attention and contrastive features.pdf:PDF},
 pages = {3437-3447},
 title = {Context-Aware Group Captioning via Self-Attention and Contrastive Features},
 year = {2020}
}

@inproceedings{Li2020Oscar,
 author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
 booktitle = {European Conference on Computer Vision},
 file = {:PDF/Oscar- Object-semantics aligned pre-training for vision-language tasks.pdf:PDF},
 groups = {Early fusion and vision-and-language pre-training., BERT},
 organization = {Springer International Publishing},
 pages = {121--137},
 title = {Oscar: Object-semantics aligned pre-training for vision-language tasks},
 url = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750120.pdf},
 year = {2020}
}

@inproceedings{Li2020Unicoder,
 author = {Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 file = {:PDF/Unicoder-vl- A universal encoder for vision and language by cross-modal pre-training.pdf:PDF},
 number = {07},
 pages = {11336--11344},
 title = {Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training},
 url = {https://arxiv.org/pdf/1908.06066.pdf},
 volume = {34},
 year = {2020}
}

@article{Li2024-zs,
 author = {Li, Bing and Yang, Peng and Sun, Yuankang and Hu, Zhongjian and Yi, Meng},
 file = {:PDF/download.pdf:PDF},
 groups = {bledy w SI},
 journal = {Frontiers of Information Technology \& Electronic Engineering},
 number = {1},
 pages = {64--83},
 title = {Advances and challenges in artificial intelligence text generation},
 volume = {25},
 year = {2024}
}

@article{Lim2014Scene,
 author = {Lim, Chern Hong and Risnumawan, Anhar and Chan, Chee Seng},
 doi = {10.1109/TFUZZ.2014.2298233},
 file = {:PDF/Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding.pdf:PDF},
 journal = {IEEE Transactions on Fuzzy Systems},
 title = {Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding},
 volume = {22},
 year = {2014}
}

@InProceedings{Lin1998AnID,
author = {Lin, Dekang},
title = {An Information-Theoretic Definition of Similarity},
year = {1998},
isbn = {1558605568},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
pages = {296–304},
numpages = {9},
series = {ICML '98}
}

@InProceedings{Lin2004Rouge,
  author    = {Lin, Chin},
  booktitle = {Text Summarization Branches Out},
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  year      = {2004},
  address   = {Barcelona, Spain},
  pages     = {74--81},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/ROUGE- A Package for Automatic Evaluation of Summaries.pdf:PDF},
  groups    = {metryki},
  url       = {https://aclanthology.org/W04-1013},
}

@inproceedings{Lin2014Microsoft,
 address = {Cham},
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
 booktitle = {Computer Vision -- ECCV 2014},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 file = {:PDF/Lin2014_Chapter_MicrosoftCOCOCommonObjectsInCo.pdf:PDF},
 isbn = {978-3-319-10602-1},
 pages = {740--755},
 publisher = {Springer International Publishing},
 title = {Microsoft COCO: Common Objects in Context},
 year = {2014}
}

@misc{Lin2018Focal,
 author = {Tsung{-}Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll{\'{a}}r},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1708-02002.bib},
 eprint = {1708.02002},
 eprinttype = {arXiv},
 file = {:PDF/RetinaNet_Lin2018.pdf:PDF},
 journal = {CoRR},
 primaryclass = {cs.CV},
 timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
 title = {Focal Loss for Dense Object Detection},
 url = {http://arxiv.org/abs/1708.02002},
 volume = {abs/1708.02002},
 year = {2017}
}

@inproceedings{Lindh2018Diverse,
 address = {Cham},
 author = {Lindh, Annika and Ross, Robert J. and Mahalunkar, Abhijit and Salton, Giancarlo and Kelleher, John D.},
 booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2018},
 editor = {K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
 file = {:PDF/aics_38.pdf:PDF},
 groups = {Other deep learning methods},
 isbn = {978-3-030-01418-6},
 pages = {176--187},
 publisher = {Springer International Publishing},
 title = {Generating Diverse and Meaningful Captions},
 year = {2018}
}

@article{Liu2011Gabor,
 author = {Liu, Xiaobai and Lin, Liang and Yan, Shuicheng and Jin, Hai and Tao, Wenbing},
 doi = {10.1109/TCSVT.2010.2087570},
 file = {:PDF/Integrating_Spatio-Temporal_Context_With_Multiview_Representation_for_Object_Recognition_in_Visual_Surveillance.pdf:PDF},
 issn = {1558-2205},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 keywords = {Context;Context modeling;Surveillance;Bismuth;Videos;Object recognition;Computational modeling;Active feature;deformable template;object recognition;spatio-temporal context},
 number = {4},
 pages = {393-407},
 title = {Integrating Spatio-Temporal Context With Multiview Representation for Object Recognition in Visual Surveillance},
 volume = {21},
 year = {2011}
}

@inproceedings{Liu2016SSD,
 address = {Cham},
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 booktitle = {Computer Vision -- ECCV 2016},
 editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
 file = {:PDF/Liu2016_Chapter_SSDSingleShotMultiBoxDetector.pdf:PDF},
 isbn = {978-3-319-46448-0},
 pages = {21--37},
 publisher = {Springer International Publishing},
 title = {SSD: Single Shot MultiBox Detector},
 url = {https://arxiv.org/pdf/1512.02325.pdf},
 year = {2016}
}

@article{Liu2018,
 author = {Liu, Shuang and Bai, Liang and Hu, Yanli and Wang, Haoran},
 doi = {10.1051/matecconf/201823201052},
 file = {:PDF/Image_Captioning_Based_on_Deep_Neural_Networks.pdf:PDF},
 groups = {vgg16, resnet},
 journal = {MATEC Web of Conferences},
 pages = {01052},
 title = {Image Captioning Based on Deep Neural Networks},
 volume = {232},
 year = {2018}
}

@inproceedings{Liu2019,
 author = {Liu, Fenglin and Gao, Meng and Zhang, Tianhao and Zou, Yuexian},
 booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
 doi = {10.1109/ICDM.2019.00054},
 file = {:PDF/Exploring_Semantic_Relationships_for_Image_Captioning_without_Parallel_Data.pdf:PDF},
 groups = {relations},
 pages = {439-448},
 title = {Exploring Semantic Relationships for Image Captioning without Parallel Data},
 year = {2019}
}

@article{Liu2019Deep,
 author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
 day = {01},
 doi = {10.1007/s11263-019-01247-4},
 file = {:PDF/Liu2018.pdf:PDF},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 number = {2},
 pages = {261-318},
 title = {Deep Learning for Generic Object Detection: A Survey},
 volume = {128},
 year = {2020}
}

@inproceedings{Liu2019ExploringAD,
 author = {Fenglin Liu and Xuancheng Ren and Yuanxin Liu and Kai Lei and Xu Sun},
 booktitle = {International Joint Conference on Artificial Intelligence},
 file = {:PDF/Exploring and Distilling Cross-Modal Information for Image Captioning.pdf:PDF},
 groups = {attention},
 title = {Exploring and Distilling Cross-Modal Information for Image Captioning},
 year = {2019}
}

@article{Liu2019Survey,
 address = {Berlin, Heidelberg},
 author = {Liu, Xiaoxiao and Xu, Qingyang and Wang, Ning},
 doi = {10.1007/s00371-018-1566-y},
 file = {:PDF/s00371-018-1566-y.pdf:PDF},
 groups = {review},
 issn = {0178-2789},
 issue_date = {March 2019},
 journal = {Vis. Comput.},
 keywords = {Attention mechanism, Dense captioning, Image captioning, Image understanding, Language model, Object detection},
 number = {3},
 numpages = {26},
 pages = {445–470},
 publisher = {Springer-Verlag},
 title = {A survey on deep neural network-based image captioning},
 volume = {35},
 year = {2019}
}

@article{LIU2020102178,
 author = {Maofu Liu and Lingjun Li and Huijun Hu and Weili Guan and Jing Tian},
 doi = {10.1016/j.ipm.2019.102178},
 file = {:PDF/1-s2.0-S0306457319307885-main.pdf:PDF},
 groups = {attention taxonomy},
 issn = {0306-4573},
 journal = {Information Processing \& Management},
 keywords = {Image caption generation, Textual attention, Visual attention, Dual attention, Fully convolutional network},
 number = {2},
 pages = {102178},
 title = {Image caption generation with dual attention mechanism},
 url = {https://www.sciencedirect.com/science/article/pii/S0306457319307885},
 volume = {57},
 year = {2020}
}

@article{Liu2020ChineseIC,
 author = {Maofu Liu and Huijun Hu and Lingjun Li and Yan Yu and Weili Guan},
 file = {:PDF/Chinese_Image_Caption_Generation_via_Visual_Attention_and_Topic_Modeling.pdf:PDF},
 journal = {IEEE Transactions on Cybernetics},
 pages = {1247-1257},
 title = {Chinese Image Caption Generation via Visual Attention and Topic Modeling},
 url = {https://api.semanticscholar.org/CorpusID:219986264},
 volume = {52},
 year = {2020}
}

@inproceedings{Liu2020InteractiveDG,
 author = {Junhao Liu and Kai Wang and Chunpu Xu and Zhou Zhao and Ruifeng Xu and Ying Shen and Min Yang},
 booktitle = {AAAI},
 file = {:PDF/Interactive_Dual_Generative_Adversarial_Networks_f.pdf:PDF},
 title = {Interactive Dual Generative Adversarial Networks for Image Captioning},
 year = {2020}
}

@misc{Liu2021cptr,
 archiveprefix = {arXiv},
 author = {Wei Liu and Sihan Chen and Longteng Guo and Xinxin Zhu1 and Jing Liu1},
 eprint = {2101.10804},
 file = {:PDF/cptr-upload.pdf:PDF},
 groups = {Vision Transformer.},
 primaryclass = {cs.CV},
 title = {CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING},
 year = {2021}
}

@inproceedings{liu2021Swin,
 author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
 file = {:PDF/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf:PDF},
 title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
 year = {2021}
}

@inproceedings{Liu2022Deconfound,
 author = {Liu, Bing and Wang, Dong and Yang, Xu and Zhou, Yong and Yao, Rui and Shao, Zhiwen and Zhao, Jiaqi},
 booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR52688.2022.01751},
 file = {:PDF/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf:PDF},
 pages = {18020-18029},
 title = {Show, Deconfound and Tell: Image Captioning with Causal Inference},
 year = {2022}
}

@misc{Liu2022Prophet,
 author = {Liu, Fenglin and Ma, Xuewei and Ren, Xuancheng and Wu, Xian and Fan, Wei and Zou, Yuexian and Sun, Xu},
 doi = {10.48550/arXiv.2210.10914},
 file = {:PDF/NeurIPS-2020-prophet-attention-predicting-attention-with-future-attention-Paper.pdf:PDF},
 groups = {Boosting LSTM with Self-Attention},
 title = {Prophet Attention: Predicting Attention with Future Attention for Improved Image Captioning},
 year = {2022}
}

@article{Lo2023ImageTextEmbedding,
 author = {Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
 doi = {10.1109/TPAMI.2022.3148470},
 file = {:PDF/Image-Text_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {1},
 pages = {641-656},
 title = {Image-Text Embedding Learning via Visual and Textual Semantic Reasoning},
 volume = {45},
 year = {2023}
}

@inproceedings{Lochter2020DeOutOfVocab,
 address = {Berlin, Heidelberg},
 author = {Lochter, Johannes V. and Silva, Renato M. and Almeida, Tiago A.},
 booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
 doi = {10.1007/978-3-030-61377-8_29},
 file = {:PDF/2007.07318v2.pdf:PDF},
 isbn = {978-3-030-61376-1},
 keywords = {Out-of-vocabulary words, Machine learning, Natural language processing},
 location = {Rio Grande, Brazil},
 numpages = {17},
 pages = {418–434},
 publisher = {Springer-Verlag},
 title = {Deep Learning Models for Representing Out-of-Vocabulary Words},
 year = {2020}
}

@article{Lowe2004SIFT,
 author = {Lowe, David G.},
 day = {01},
 doi = {10.1023/B:VISI.0000029664.99615.94},
 file = {:PDF/B_VISI.0000029664.99615.94-1.pdf:PDF},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 number = {2},
 pages = {91-110},
 title = {Distinctive Image Features from Scale-Invariant Keypoints},
 volume = {60},
 year = {2004}
}

@inproceedings{Lu2016Visual,
 author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
 booktitle = {European Conference on Computer Vision},
 comment = {https://cs.stanford.edu/people/ranjaykrishna/vrd/},
 title = {Visual Relationship Detection with Language Priors},
 year = {2016}
}

@inproceedings{Lu2018Context,
 address = {New York, NY, USA},
 author = {Liu, Daqing and Zha, Zheng-Jun and Zhang, Hanwang and Zhang, Yongdong and Wu, Feng},
 booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
 comment = {https://github.com/daqingliu/CAVP},
 doi = {10.1145/3240508.3240632},
 file = {:PDF/Context-Aware Visual Policy Network for Sequence-Level Image Captioning.pdf:PDF},
 groups = {Visual Policy},
 isbn = {9781450356657},
 keywords = {image captioning, policy network, reinforcement learning, visual context},
 location = {Seoul, Republic of Korea},
 numpages = {9},
 pages = {1416–1424},
 printed = {yes},
 publisher = {Association for Computing Machinery},
 series = {MM '18},
 title = {Context-Aware Visual Policy Network for Sequence-Level Image Captioning},
 year = {2018}
}

@article{Lu2018NeuralBT,
 author = {Jiasen Lu and Jianwei Yang and Dhruv Batra and Devi Parikh},
 file = {:PDF/neural baby talk.pdf:PDF},
 groups = {Visual sentinel, Two-layer LSTM},
 journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {7219-7228},
 title = {Neural Baby Talk},
 year = {2018}
}

@inbook{Lu2019VILBERT,
 address = {Red Hook, NY, USA},
 articleno = {2},
 author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
 booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
 file = {:PDF/NeurIPS-2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks-Paper.pdf:PDF},
 numpages = {11},
 publisher = {Curran Associates Inc.},
 title = {ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
 year = {2019}
}

@article{Lu2021Chinese,
 address = {New York, NY, USA},
 articleno = {14},
 author = {Lu, Huimin and Yang, Rui and Deng, Zhenrong and Zhang, Yonglin and Gao, Guangwei and Lan, Rushi},
 doi = {10.1145/3422668},
 file = {:PDF/Chinese Image Captioning via Fuzzy Attention-based DenseNet-BiLSTM.pdf:PDF},
 issn = {1551-6857},
 issue_date = {April 2021},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 keywords = {Image captioning, fuzzy attention, DenseNet, BiLSTM},
 number = {1s},
 numpages = {18},
 publisher = {Association for Computing Machinery},
 title = {Chinese Image Captioning via Fuzzy Attention-Based DenseNet-BiLSTM},
 url = {https://doi.org/10.1145/3422668},
 volume = {17},
 year = {2021}
}

@article{Luo2018Discriminability,
 author = {Ruotian Luo and Brian L. Price and Scott Cohen and Gregory Shakhnarovich},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1803-04376.bib},
 comment = {nacisk na rónorodność
-standardowe modele tworza podobne podpisy  (MLE-maximum likelihood estimation )
-transformers
architektura koder-dekoder (podobnie do RNN), ale moga rownolegle otrzymywac na wejsciu całe zdanie( rownolegle otrzymywanie kazdego slowa w zdaniu) np. całe zdanie rownolegle slowo po slowie jest wstrzykiwane do pierwszej warstwy embeddings
transformers nie uzywa  konwolucji(wiedzy o poprzdnich slowach)  - uzwane sa zafiksownae wagi slow w zdaniach
cechy obrazu tecetron features
blok dekodera ma 2 warstwy z mechanizmem self atencji,  a druga to prosta, pozycyjna, w pełni połączona sieć typu feed-forward.  Dla kazdego slowa otrzymujemy wektor atencji, ktory przechowuje kontekst słów w zdaniu. Atencja wieloglowicowa w dekoderze eykonuje self-attention.. Pozwala to laczyc slowa na wejsciu  z innymi slowami.
Dekoder uzywa 3 warstwy ktora wykonuje atencje wieloglowicowa na wyjsciu encodera. Podobnie do dekodera, podwarstwy sa laczone przez normalizacje. Wekotry atencji wejscia i wyjscia sa przekazywane do mechanizmu atencji wieloglowicowej. Standardowo dekoder zachwouje sie jak klasyfikator (softmax)

podpisy:beam_search
własna miara roznorodnosci

zbiór danych  COCO
113,287 tren
5,000 validacja  5,000  test.
5 podpisow do obrazu

model obraz-jezyk to Vilibert wytrenowany na conceptual captions. Wytrenowany na wielu zadanich jezyk obraz(odp. na pytani, rozumienie obrazu, wyszukiwanie obrazu, w kazdym zadaniu osiagnieto state of the art poziom). Skupiono sie na  tym  by stworzyc  model wiązacy obraz z tekstem, ktory jest latwy do przeniesienia. Porzucono podejscie gdzie znajdowanie powiazan obraz tekst to czesc treningu/},
 eprint = {1803.04376},
 eprinttype = {arXiv},
 file = {:PDF/Discriminability objective for training descriptive captions.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
 title = {Discriminability objective for training descriptive captions},
 volume = {abs/1803.04376},
 year = {2018}
}

@article{Luo2022ATR,
 author = {Gaifang Luo and Lijun Cheng and Chao Jing and Can Zhao and Guozhu Song},
 file = {:PDF/A thorough review of models  evaluation metrics  and datasets on image captioning.pdf:PDF},
 groups = {review},
 journal = {IET Image Process.},
 pages = {311-332},
 title = {A thorough review of models, evaluation metrics, and datasets on image captioning},
 volume = {16},
 year = {2022}
}

@inproceedings{Luo2023SemanticDiffusionNetwork,
 address = {Los Alamitos, CA, USA},
 author = {Luo, Jianjie and Li, Yehao and Pan, Yingwei and Yao, Ting and Feng, Jianlin and Chao, Hongyang and Mei, Tao},
 booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR52729.2023.02237},
 file = {:PDF/Semantic-Conditional_Diffusion_Networks_for_Image_Captioning.pdf:PDF},
 groups = {transformer},
 keywords = {Training;Shape;Computational modeling;Source coding;Semantics;Diffusion processes;Coherence},
 pages = {23359-23368},
 publisher = {IEEE Computer Society},
 title = {{ Semantic-Conditional Diffusion Networks for Image Captioning* }},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.02237},
 year = {2023}
}

@inproceedings{Luo2024unleashing,
 author = {Luo, Jianjie and Chen, Jingwen and Li, Yehao and Pan, Yingwei and Feng, Jianlin and Chao, Hongyang and Yao, Ting},
 booktitle = {European Conference on Computer Vision (ECCV)},
 file = {:PDF/2501.00437v1.pdf:PDF},
 title = {Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning},
 year = {2024}
}

@misc{Luo2025EmpiricalEtudyCatastrophicForgetting,
 archiveprefix = {arXiv},
 author = {Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
 eprint = {2308.08747},
 file = {:PDF/2308.08747v5.pdf:PDF},
 primaryclass = {cs.CL},
 title = {An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
 url = {https://arxiv.org/abs/2308.08747},
 year = {2025}
}

@inproceedings{Luong2015Effective,
 address = {Lisbon, Portugal},
 author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D15-1166},
 editor = {M{\`a}rquez, Llu{\'\i}s and Callison-Burch, Chris and Su, Jian},
 file = {:PDF/D15-1166.pdf:PDF},
 groups = {attention taxonomy},
 pages = {1412--1421},
 publisher = {Association for Computational Linguistics},
 title = {Effective Approaches to Attention-based Neural Machine Translation},
 url = {https://aclanthology.org/D15-1166},
 year = {2015}
}

@article{Lv2019Local,
 article-number = {3006},
 author = {Lv, Yafei and Zhang, Xiaohan and Xiong, Wei and Cui, Yaqi and Cai, Mi},
 doi = {10.3390/rs11243006},
 file = {:PDF/remotesensing-11-03006.pdf:PDF},
 issn = {2072-4292},
 journal = {Remote Sensing},
 number = {24},
 title = {An End-to-End Local-Global-Fusion Feature Extraction Network for Remote Sensing Image Scene Classification},
 volume = {11},
 year = {2019}
}

@inproceedings{Ma2015,
 author = {Ma, Lin and Lu, Zhengdong and Shang, Lifeng and Li, Hang},
 booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2015.301},
 file = {:PDF/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf:PDF},
 groups = {Earlier Deep Models},
 issn = {2380-7504},
 keywords = {Convolution;Image representation;Semantics;Neural networks;Computer architecture;Natural languages;Grounding},
 pages = {2623-2631},
 title = {Multimodal Convolutional Neural Networks for Matching Image and Sentence},
 year = {2015}
}

@inproceedings{Ma2016,
 author = {Ma, Shubo and Han, Yahong},
 booktitle = {2016 IEEE International Conference on Multimedia and Expo (ICME)},
 doi = {10.1109/ICME.2016.7552883},
 file = {:PDF/07552883.pdf:PDF},
 groups = {Compositional architectures},
 issn = {1945-788X},
 keywords = {Logic gates;Feature extraction;Measurement;Recurrent neural networks;Training;Hidden Markov models;Visualization;Image description;structural words;LSTM;machine translation},
 pages = {1-6},
 title = {Describing images by feeding LSTM with structural words},
 year = {2016}
}

@article{Ma2023Grid,
 author = {Yiwei Ma and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Rongrong Ji},
 doi = {10.1016/j.patcog.2023.109420},
 file = {:PDF/1-s2.0-S0031320323001218-main.pdf:PDF},
 issn = {0031-3203},
 journal = {Pattern Recognition},
 keywords = {Image captioning, Attention mechanism, Local visual modeling},
 pages = {109420},
 title = {Towards local visual modeling for image captioning},
 volume = {138},
 year = {2023}
}

@article{Maciag2022NLPRekonstrukcja,
 author = {Maciag, Rafal},
 doi = {10.4467/20843976ZK.22.003.15869},
 file = {:PDF/3_Maciag_Zarzadzanie-w-Kulturze_23(1)_2022-2.pdf:PDF},
 journal = {Zarządzanie w Kulturze},
 pages = {37-53},
 title = {Zaawansowane procedury NLP jako przesłanka rekonstrukcji idei wiedzy},
 volume = {23},
 year = {2022}
}

@inproceedings{Macleod2017Understanding,
 author = {MacLeod, Haley and Bennett, Cynthia L and Morris, Meredith Ringel and Cutrell, Edward},
 booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
 pages = {5988--5999},
 title = {Understanding blind people's experiences with computer-generated captions of social media images},
 year = {2017}
}

@article{Mamdani1974Applications,
 author = {E. Mamdani},
 doi = {10.1049/piee.1974.0328},
 journal = {Proceedings of the IEEE},
 title = {Applications of fuzzy algorithms for control of a simple dynamic plant},
 url = {https://digital-library.theiet.org/content/journals/10.1049/piee.1974.0328},
 year = {1974}
}

@article{Mamdani1999eltitExperiment,
 author = {E.H. MAMDANI and S. ASSILIAN},
 doi = {10.1006/ijhc.1973.0303},
 file = {:PDF/Mamdani1999.pdf:PDF},
 issn = {1071-5819},
 journal = {International Journal of Human-Computer Studies},
 number = {2},
 pages = {135-147},
 title = {An Experiment in Linguistic Synthesis with a Fuzzy Logic Controller},
 url = {https://www.sciencedirect.com/science/article/pii/S1071581973603035},
 volume = {51},
 year = {1999}
}

@book{Manning1999Foundations,
 author = {Manning, C. and Schutze, H.},
 file = {:PDF/Christopher_D._Manning_Hinrich_Schütze_Foundations_Of_Statistical_Natural_Language_Processing.pdf:PDF},
 isbn = {9780262133609},
 lccn = {99021137},
 publisher = {MIT Press},
 series = {Foundations of Statistical Natural Language Processing},
 title = {Foundations of Statistical Natural Language Processing},
 url = {https://books.google.pl/books?id=YiFDxbEX3SUC},
 year = {1999}
}

@book{Manning2008IntroductionToInformationRetrieval,
 address = {USA},
 author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
 file = {:PDF/irbookprint.pdf:PDF},
 groups = {lm},
 isbn = {0521865719},
 publisher = {Cambridge University Press},
 title = {Introduction to Information Retrieval},
 year = {2008}
}

@article{Mao2014,
 author = {Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Alan L. Yuille},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/MaoXYWY14.bib},
 eprint = {1410.1090},
 eprinttype = {arXiv},
 file = {:PDF/1410.1090.pdf:PDF},
 groups = {resnet},
 journal = {CoRR},
 timestamp = {Tue, 15 Sep 2020 18:57:32 +0200},
 title = {Explain Images with Multimodal Recurrent Neural Networks},
 url = {http://arxiv.org/abs/1410.1090},
 volume = {abs/1410.1090},
 year = {2014}
}

@misc{Mao2014DeepCW,
      title={Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},
      author={Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Zhiheng Huang and Alan Yuille},
      year={2015},
      eprint={1412.6632},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{Marcinczuk2021Text,
 address = {University of South Africa (UNISA)},
 author = {Marci{\'n}czuk, Micha{\l} and Gniewkowski, Mateusz and Walkowiak, Tomasz and B{\k{e}}dkowski, Marcin},
 booktitle = {Proceedings of the 11th Global Wordnet Conference},
 editor = {Vossen, Piek and Fellbaum, Christiane},
 file = {:PDF/2021.gwc-1.24.pdf:PDF},
 pages = {207--214},
 publisher = {Global Wordnet Association},
 title = {Text Document Clustering: {W}ordnet vs. {TF}-{IDF} vs. Word Embeddings},
 url = {https://aclanthology.org/2021.gwc-1.24/},
 year = {2021}
}

@article{Mars2022FromWordEmbeddings,
 article-number = {8805},
 author = {Mars, Mourad},
 doi = {10.3390/app12178805},
 file = {:PDF/applsci-12-08805-v3.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {17},
 title = {From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough},
 url = {https://www.mdpi.com/2076-3417/12/17/8805},
 volume = {12},
 year = {2022}
}

@inproceedings{Maru2021ComparisonofEncoderArchforImgCapt,
 author = {Maru, Harsh and Chandana, TSS and Naik, Dinesh},
 booktitle = {2021 5th International Conference on Computing Methodologies and Communication (ICCMC)},
 doi = {10.1109/ICCMC51019.2021.9418234},
 file = {:PDF/Comparison_of_Image_Encoder_Architectures_for_Image_Captioning.pdf:PDF},
 pages = {740-744},
 title = {Comparison of Image Encoder Architectures for Image Captioning},
 year = {2021}
}

@inproceedings{Mason2014nonparametric,
 author = {Mason, Rebecca and Charniak, Eugene},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 file = {:PDF/P14-2097.pdf:PDF},
 groups = {retrieval},
 pages = {592--598},
 title = {Nonparametric method for data-driven image captioning},
 year = {2014}
}

@inproceedings{Masotti2017DeepLF,
 author = {Caterina Masotti and Danilo Croce and Roberto Basili},
 booktitle = {CLiC-it},
 file = {:PDF/Deep learning for automatic image captioning in poor training conditions..pdf:PDF},
 title = {Deep Learning for Automatic Image Captioning in Poor Training Conditions},
 year = {2017}
}

@book{Matlab2021,
 address = {Natick, Massachusetts},
 author = {MATLAB},
 publisher = {The MathWorks Inc.},
 title = {version 7.10.0 (R2010a)},
 year = {2021}
}

@article{Matsakis1999New,
 author = {P. {Matsakis} and L. {Wendling}},
 doi = {10.1109/34.777374},
 file = {:PDF/A_new_way_Matsakis_1999.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {7},
 pages = {634-643},
 title = {A new way to represent the relative position between areal objects},
 volume = {21},
 year = {1999}
}

@inbook{Matsakis2010General,
 address = {Berlin, Heidelberg},
 author = {Matsakis, Pascal and Wendling, Laurent and Ni, JingBo},
 booktitle = {Methods for Handling Imperfect Spatial Information},
 doi = {10.1007/978-3-642-14755-5_3},
 editor = {Jeansoulin, Robert and Papini, Odile and Prade, Henri and Schockaert, Steven},
 file = {:PDF/Matsakis2010_Chapter_AGeneralApproachToTheFuzzyMode.pdf:PDF},
 isbn = {978-3-642-14755-5},
 pages = {49--74},
 publisher = {Springer Berlin Heidelberg},
 title = {A General Approach to the Fuzzy Modeling of Spatial Relationships},
 url = {https://doi.org/10.1007/978-3-642-14755-5_3},
 year = {2010}
}

@article{Matsakis2010Relative,
 author = {P. Matsakis and L. Wawrzyniak and J. Ni},
 doi = {10.1080/13658810802270587},
 eprint = {https://doi.org/10.1080/13658810802270587},
 file = {:PDF/Relative positions in words a system that builds descriptions around Allen relations.pdf:PDF},
 journal = {International Journal of Geographical Information Science},
 number = {1},
 pages = {1-23},
 publisher = {Taylor \& Francis},
 title = {Relative positions in words: a system that builds descriptions around Allen relations},
 url = {https://doi.org/10.1080/13658810802270587},
 volume = {24},
 year = {2010}
}

@inproceedings{Matsakis2016Fuzzy,
 author = {P. {Matsakis} and M. {Naeem}},
 booktitle = {2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 doi = {10.1109/FUZZ-IEEE.2016.7737810},
 file = {:PDF/Fuzzy_models_Matsakis2016..pdf:PDF},
 pages = {1096-1104},
 title = {Fuzzy models of topological relationships based on the PHI-descriptor},
 year = {2016}
}

@misc{Meng2021Objectcentric,
 archiveprefix = {arXiv},
 author = {Zihang Meng and David Yang and Xuefei Cao and Ashish Shah and Ser-Nam Lim},
 eprint = {2112.00969},
 file = {:PDF/Object-Centric Unsupervised Image Captioning.pdf:PDF},
 primaryclass = {cs.CV},
 title = {Object-Centric Unsupervised Image Captioning},
 year = {2021}
}

@article{Mert2021Cosmic,
 author = {Mert Inan and Piyush Sharma and Baber Khalid and Radu Soricut and Matthew Stone and Malihe Alikhani},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2109-05281.bib},
 eprint = {2109.05281},
 eprinttype = {arXiv},
 file = {:PDF/COSMic- A Coherence-Aware Generation Metric for Image.pdf:PDF},
 journal = {CoRR},
 timestamp = {Tue, 21 Sep 2021 17:46:04 +0200},
 title = {COSMic: {A} Coherence-Aware Generation Metric for Image Descriptions},
 url = {https://arxiv.org/abs/2109.05281},
 volume = {abs/2109.05281},
 year = {2021}
}

@misc{merullo2023linearly,
 archiveprefix = {arXiv},
 author = {Jack Merullo and Louis Castricato and Carsten Eickhoff and Ellie Pavlick},
 comment = {https://github.com/jmerullo/limber},
 eprint = {2209.15162},
 file = {:PDF/5799_linearly_mapping_from_image_to.pdf:PDF},
 primaryclass = {cs.CL},
 title = {Linearly Mapping from Image to Text Space},
 year = {2023}
}

@inproceedings{Meteor2008Agarwal,
author = {Agarwal, Abhaya and Lavie, Alon},
title = {METEOR, M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine translation output},
year = {2008},
isbn = {9781932432091},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments. We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor.},
booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
pages = {115–118},
numpages = {4},
location = {Columbus, Ohio},
series = {StatMT '08}
}


@article{Mienye2024GRU,
 article-number = {517},
 author = {Mienye, Ibomoiye Domor and Swart, Theo G. and Obaido, George},
 doi = {10.3390/info15090517},
 file = {:PDF/information-15-00517.pdf:PDF},
 groups = {GRU},
 issn = {2078-2489},
 journal = {Information},
 number = {9},
 title = {Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications},
 volume = {15},
 year = {2024}
}

@article{Migalska2018Density,
 author = {Migalska, Agata},
 doi = {10.1002/asmb.2321},
 eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.2321},
 file = {:pdf/Migalska2018.pdf:PDF},
 journal = {Applied Stochastic Models in Business and Industry},
 keywords = {density-free test, information theory, negentropy, statistical inference, symmetry verification},
 number = {5},
 owner = {Marcin},
 pages = {618-632},
 timestamp = {2021-05-10},
 title = {Density-free test for symmetry verification in images},
 url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2321},
 volume = {34},
 year = {2018}
}

@inproceedings{mikolov10_interspeech,
 author = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
 booktitle = {Interspeech 2010},
 doi = {10.21437/Interspeech.2010-343},
 file = {:PDF/mikolov10_interspeech.pdf:PDF},
 groups = {word2vec},
 issn = {2958-1796},
 pages = {1045--1048},
 title = {Recurrent neural network based language model},
 year = {2010}
}

@inproceedings{Mikolov2013DistributedRO,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 file = {:PDF/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf:PDF},
 groups = {word2vec},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{Mikolov2013EfficientEO,
 author = {Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
 booktitle = {International Conference on Learning Representations},
 file = {:PDF/1301.3781.pdf:PDF},
 groups = {word2vec},
 title = {Efficient Estimation of Word Representations in Vector Space},
 year = {2013}
}

@article{Milewski2020Scene,
 author = {Victor Milewski and Marie{-}Francine Moens and Iacer Calixto},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2009-12313.bib},
 eprint = {2009.12313},
 eprinttype = {arXiv},
 file = {:PDF/Are scene graphs good enough to improve Image Captioning?.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 30 Sep 2020 16:16:22 +0200},
 title = {Are scene graphs good enough to improve Image Captioning?},
 url = {https://arxiv.org/abs/2009.12313},
 volume = {abs/2009.12313},
 year = {2020}
}

@article{Miller1995Wordnet,
 address = {New York, NY, USA},
 author = {Miller, George A.},
 doi = {10.1145/219717.219748},
 file = {:PDF/wordnet a lexical.pdf:PDF},
 issn = {0001-0782},
 issue_date = {Nov. 1995},
 journal = {Commun. ACM},
 number = {11},
 numpages = {3},
 pages = {39–41},
 publisher = {Association for Computing Machinery},
 title = {WordNet: A Lexical Database for English},
 volume = {38},
 year = {1995}
}

@inproceedings{Miltenburg2016Negation,
 author = {Miltenburg, Emiel and Morante, Roser and Elliott, Desmond},
 doi = {10.18653/v1/W16-3207},
 file = {:PDF/W16-3207.pdf:PDF},
 groups = {negacja},
 pages = {54-59},
 title = {Pragmatic Factors in Image Description: The Case of Negations},
 year = {2016}
}

@phdthesis{Miltenburg2019Pragmatic,
 author = {{van Miltenburg}, Emiel},
 day = {14},
 file = {:PDF/phdthesis.pdf:PDF},
 language = {English},
 school = {Vrije Universiteit Amsterdam},
 title = {Pragmatic factors in (automatic) image description},
 year = {2019}
}

@article{Ming2022VisualsToText,
 author = {Ming, Yue and Hu, Nannan and Fan, Chunxiao and Feng, Fan and Zhou, Jiangwan and Yu, Hui},
 doi = {10.1109/JAS.2022.105734},
 file = {:PDF/A_Comprehensive_Review_on_Automatic_Image_Captioning.pdf:PDF},
 groups = {review, wazne_wazne},
 issn = {2329-9274},
 journal = {IEEE/CAA Journal of Automatica Sinica},
 keywords = {Measurement;Training;Deep learning;Visualization;Computer vision;Information processing;Natural language processing;Artificial intelligence;attention mechanism;encoder-decoder framework;image captioning;multi-modal understanding;training strategies},
 number = {8},
 pages = {1339-1365},
 priority = {prio1},
 title = {Visuals to Text: A Comprehensive Review on Automatic Image Captioning},
 volume = {9},
 year = {2022}
}

@inproceedings{Mitchel2012,
 address = {USA},
 author = {Mitchell, Margaret and Han, Xufeng and Dodge, Jesse and Mensch, Alyssa and Goyal, Amit and Berg, Alex and Yamaguchi, Kota and Berg, Tamara and Stratos, Karl and Daum\'{e}, Hal},
 booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
 file = {:PDF/E12-1076.pdf:PDF},
 groups = {template},
 isbn = {9781937284190},
 location = {Avignon, France},
 numpages = {10},
 pages = {747–756},
 publisher = {Association for Computational Linguistics},
 series = {EACL '12},
 title = {Midge: Generating Image Descriptions from Computer Vision Detections},
 year = {2012}
}

@inproceedings{Miyajima1994Spatial,
 author = {K. {Miyajima} and A. {Ralescu}},
 booktitle = {Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference},
 doi = {10.1109/FUZZY.1994.343710},
 file = {:PDF/Spatial_organizatoin_Miyaima1994.pdf:PDF},
 pages = {100-105 vol.1},
 title = {Spatial organization in 2D images},
 year = {1994}
}

@inproceedings{Mnih2007Three,
 address = {New York, NY, USA},
 author = {Mnih, Andriy and Hinton, Geoffrey},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 doi = {10.1145/1273496.1273577},
 file = {:PDF/threenew.pdf:PDF},
 isbn = {9781595937933},
 location = {Corvalis, Oregon, USA},
 numpages = {8},
 pages = {641–648},
 publisher = {Association for Computing Machinery},
 series = {ICML '07},
 title = {Three new graphical models for statistical language modelling},
 url = {https://doi.org/10.1145/1273496.1273577},
 year = {2007}
}

@article{Mocanu2010From,
 author = {Mocanu, Irina},
 doi = {10.4316/aece.2010.04008},
 file = {:PDF/From_Content-Based_Image_Retrieval_by_Shape_to_Ima.pdf:PDF},
 journal = {Advances in Electrical and Computer Engineering},
 title = {From Content-Based Image Retrieval by Shape to Image Annotation},
 volume = {10},
 year = {2010}
}

@article{mokady2021clipcap,
 author = {Mokady, Ron and Hertz, Amir and Bermano, Amit H},
 comment = {https://github.com/rmokady/CLIP_prefix_caption},
 file = {:PDF/ClipCap- CLIP Prefix for Image Captioning.pdf:PDF},
 journal = {arXiv preprint arXiv:2111.09734},
 title = {ClipCap: CLIP Prefix for Image Captioning},
 year = {2021}
}

@inproceedings{Morin2005Hierarchical,
 author = {Morin, Frederic and Bengio, Yoshua},
 booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
 editor = {Cowell, Robert G. and Ghahramani, Zoubin},
 file = {:PDF/hierarchical-nnlm-aistats05.pdf:PDF},
 groups = {Probabilistic Neural Network Language Model},
 note = {Reissued by PMLR on 30 March 2021.},
 pages = {246--252},
 pdf = {http://proceedings.mlr.press/r5/morin05a/morin05a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Hierarchical Probabilistic Neural Network Language Model},
 url = {https://proceedings.mlr.press/r5/morin05a.html},
 volume = {R5},
 year = {2005}
}

@inproceedings{Naacl21ZhangInductive,
 author = {Zhang, Tianyi and Hashimoto, Tatsunori},
 booktitle = {North American Association for Computational Linguistics (NAACL)},
 file = {:PDF/On the Inductive Bias of Masked Language Modeling- From Statistical to Syntactic Dependencies.pdf:PDF;:PDF/IRJET-V8I12123.pdf:PDF},
 title = {On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies},
 year = {2021}
}

@conference{Naeem2015Relative,
 author = {Naeem, M. and Matsakis, P.},
 booktitle = {ICPRAM},
 document_type = {Conference Paper},
 doi = {10.5220/0005211002860295},
 file = {:PDF/ICPRAM_2015_78.pdf:PDF},
 journal = {ICPRAM 2015 - 4th International Conference on Pattern Recognition Applications and Methods, Proceedings},
 note = {cited By 7},
 pages = {286-295},
 source = {Scopus},
 title = {Relative position descriptors a review},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938834316&doi=10.5220%2f0005211002860295&partnerID=40&md5=863f0a96c17ef2046ee402d9fac6b2a1},
 volume = {1},
 year = {2015}
}

@inproceedings{NagaDurga2022AtttBasedComparisonOfImgCapt,
 address = {Singapore},
 author = {NagaDurga, Cheboyina Sindhu and Anuradha, T.},
 booktitle = {Advances in Micro-Electronics, Embedded Systems and IoT},
 editor = {Chakravarthy, V. V. S. S. S. and Flores-Fuentes, Wendy and Bhateja, Vikrant and Biswal, B.N.},
 isbn = {978-981-16-8550-7},
 pages = {157--167},
 publisher = {Springer Nature Singapore},
 title = {Attention-Based Comparison of Automatic Image Caption Generation Encoders},
 year = {2022}
}

@inproceedings{Nagchowdhuryetal2021Exploiting,
 address = {Kyiv, Ukraine},
 author = {Nag Chowdhury, Sreyasi and Bhowmik, Rajarshi and Ravi, Hareesh and de Melo, Gerard and Razniewski, Simon and Weikum, Gerhard},
 booktitle = {Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)},
 file = {:PDF/Exploiting Image Text Synergy for Contextual Image Captioning.pdf:PDF},
 pages = {30--37},
 publisher = {Association for Computational Linguistics},
 title = {Exploiting Image Text Synergy for Contextual Image Captioning},
 url = {https://www.aclweb.org/anthology/2021.lantern-1.3},
 year = {2021}
}

@inproceedings{Nair2010,
 address = {Madison, WI, USA},
 author = {Nair, Vinod and Hinton, Geoffrey E.},
 booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 file = {:PDF/reluICML.pdf:PDF},
 isbn = {9781605589077},
 journal = {Proceedings of ICML},
 location = {Haifa, Israel},
 numpages = {8},
 pages = {807–814},
 publisher = {Omnipress},
 series = {ICML'10},
 title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
 year = {2010}
}

@article{Ng2020Understanding,
 author = {Edwin G. Ng and Bo Pang and Piyush Sharma and Radu Soricut},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2012-02339.bib},
 eprint = {2012.02339},
 eprinttype = {arXiv},
 file = {:PDF/Understanding Guided Image Captioning Performance across Domains.pdf:PDF},
 journal = {CoRR},
 timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
 title = {Understanding Guided Image Captioning Performance across Domains},
 url = {https://arxiv.org/abs/2012.02339},
 volume = {abs/2012.02339},
 year = {2020}
}

@inproceedings{Nguyen2022GRIT,
 address = {Cham},
 author = {Nguyen, Van-Quang and Suganuma, Masanori and Okatani, Takayuki},
 booktitle = {Computer Vision -- ECCV 2022},
 editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
 file = {:PDF/136960165.pdf:PDF},
 isbn = {978-3-031-20059-5},
 pages = {167--184},
 publisher = {Springer Nature Switzerland},
 title = {GRIT: Faster and Better Image Captioning Transformer Using Dual Visual Features},
 year = {2022}
}

@inproceedings{NIPS2013_7cce53cf,
 author = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc\textquotesingle Aurelio and Mikolov, Tomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 file = {:PDF/NIPS-2013-devise-a-deep-visual-semantic-embedding-model-Paper.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 title = {DeViSE: A Deep Visual-Semantic Embedding Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{Ogrodniczuk2017Lingwistyka,
 abstractnote = {Tekst jest publicystyczną próbą nakreślenia dalszych kierunków prac nad komputerowym przetwarzaniem polszczyzny w obliczu intensywnego rozwoju cyfrowych narzędzi i zasobów dla języka polskiego oraz zacieśniającej się współpracy między polskimi ośrodkami badawczymi zajmującymi się lingwistyką komputerową. Za najważniejszy temat autor uważa wznowienie prac nad korpusem narodowym, który jako zasób podstawowy dla językoznawstwa polskiego wymaga stałego poszerzania bazy materiałowej i opisu lingwistycznego, włączenia podkorpusów diachronicznych, gwarowych i równoległych. W sferze technologii językowej autor postuluje wzbogacenie formalnego opisu polszczyzny o głęboki poziom składniowy, semantykę i dyskurs oraz zwraca uwagę na konieczność stałego poprawiania jakości dostępnych narzędzi i zasobów metodą współpracy środowiska językoznawczego z informatycznym.},
 author = {Ogrodniczuk, Maciej},
 doi = {10.31286/JP.97.1.3},
 file = {:PDF/Lingwistyka_komputerowa_dla_jezyka_polskiego_dzis_.pdf:PDF},
 groups = {opis polskiego},
 journal = {Język Polski},
 number = {1},
 pages = {18–28},
 title = {Lingwistyka komputerowa dla języka polskiego: dziś i jutro},
 url = {https://jezyk-polski.pl/index.php/jp/article/view/449},
 volume = {97},
 year = {2017}
}

@article{Oliva2001ModelingTS,
 author = {Aude Oliva and Antonio Torralba},
 file = {:PDF/A_1011139631724.pdf:PDF},
 journal = {International Journal of Computer Vision},
 pages = {145-175},
 title = {Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope},
 url = {https://api.semanticscholar.org/CorpusID:11664336},
 volume = {42},
 year = {2001}
}

@article{Oluwasammi2021Survey,
 author = {Oluwasammi, Ariyo and Aftab, Muhammad Umar and Qin, Zhiguang and Ngo, Son Tung and Doan, Thang Van and Nguyen, Son Ba and Nguyen, Son Hoang and Nguyen, Giang Hoang},
 day = {23},
 doi = {10.1155/2021/5538927},
 file = {:PDF/Features to Text- A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning.pdf:PDF},
 issn = {1076-2787},
 journal = {Complexity},
 pages = {5538927},
 publisher = {Hindawi},
 title = {Features to Text: A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning},
 url = {https://doi.org/10.1155/2021/5538927},
 volume = {2021},
 year = {2021}
}

@article{Ondeng,
 article-number = {11103},
 author = {Ondeng, Oscar and Ouma, Heywood and Akuon, Peter},
 doi = {10.3390/app131911103},
 file = {:PDF/applsci-13-11103-v2.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {19},
 title = {A Review of Transformer-Based Approaches for Image Captioning},
 url = {https://www.mdpi.com/2076-3417/13/19/11103},
 volume = {13},
 year = {2023}
}

@Misc{Dalle2023dalle3,
  author = {{OpenAI}},
  note   = {Accessed: 2024-07-12},
  title  = {DALL·E 3 System Card},
  year   = {2023},
  url    = {https://openai.com/research/dall-e-3-system-card},
}

@misc{openai2024gpt4o,
 author = {{OpenAI}},
 note = {Accessed: 2024-07-12},
 title = {Introducing GPT-4o and More Tools to ChatGPT Free Users},
 url = {https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/},
 year = {2024}
}

@inproceedings{Ordonez2011Im2Text,
 author = {Vicente Ordonez and Girish Kulkarni and Tamara L. Berg},
 booktitle = {NIPS},
 file = {:PDF/NIPS-2011-im2text-describing-images-using-1-million-captioned-photographs-Paper.pdf:PDF},
 groups = {retrieval},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 year = {2011}
}

@inproceedings{Oruganti2016,
 author = {Oruganti, Ram Manohar and Sah, Shagan and Pillai, Suhas and Ptucha, Raymond},
 booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
 doi = {10.1109/ICIP.2016.7533033},
 file = {:PDF/Image_description_through_fusion_based_recurrent_multi-modal_learning.pdf:PDF},
 groups = {Compositional architectures},
 issn = {2381-8549},
 keywords = {Decision support systems;Computer vision;Handheld computers;Pattern recognition;Conferences;Indexes;Computational linguistics;Image Captioning;CNN;LSTM;Recurrent Neural Networks},
 pages = {3613-3617},
 title = {Image description through fusion based recurrent multi-modal learning},
 year = {2016}
}

@inproceedings{Osaid2022,
 author = {Osaid, Muhammad and Memon, Zulfiqar Ali},
 booktitle = {2022 International Conference on Emerging Trends in Smart Technologies (ICETST)},
 doi = {10.1109/ICETST55735.2022.9922935},
 file = {:PDF/A_Survey_On_Image_Captioning.pdf:PDF},
 groups = {review},
 pages = {1-6},
 title = {A Survey On Image Captioning},
 year = {2022}
}

@article{Osman2023Survey,
 author = {Osman, Asmaa and Wahby Shalaby, Mohamed and Soliman, Mona and Elsayed, Khaled},
 doi = {10.14569/IJACSA.2023.0140249},
 file = {:PDF/Paper_49-A_Survey_on_Attention_Based_Models_for_Image_Captioning-withNUaff.pdf:PDF},
 groups = {review},
 journal = {International Journal of Advanced Computer Science and Applications},
 title = {A Survey on Attention-Based Models for Image Captioning},
 volume = {14},
 year = {2023}
}

@Article{Osowski2018SieciNeuronowe,
  author  = {Osowski, Stanisław},
  journal = {Przegląd Telekomunikacyjny - Wiadomości Telekomunikacyjne},
  title   = {Głębokie sieci neuronowe i ich zastosowania w eksploracji danych},
  year    = {2018},
  pages   = {1-10},
  doi     = {10.15199/59.2018.5.2},
  file    = {:PDF/04 - Stanisław Osowski - Głębokie sieci neuronowe i ich zastosowania w eksploracji danych.pdf:PDF},
  groups  = {cnn},
}

@book{Osowski2020SN,
 author = {Osowski, Stanis{\l}aw},
 language = {pl},
 title = {Sieci neuronowe do przetwarzania informacji},
 year = {2020}
}

@book{Osowski2023ML,
 author = {Osowski, Stanisław and Szmurło, Robert},
 isbn = {978-83-8156-598-1},
 publisher = {Oficyna Wydawnicza Politechniki Warszawskiej},
 title = {Matematyczne modele uczenia maszynowego w językach MATLAB i PYTHON},
 year = {2024}
}

@misc{Over2003Duc,
 added-at = {2011-03-14T01:05:57.000+0100},
 author = {Over, P. and Yen, J.},
 biburl = {https://www.bibsonomy.org/bibtex/2f9cfa29d7fd8761529266b686ea74c31/lantiq},
 file = {:PDF/duc2003intro.pdf:PDF},
 groups = {public},
 howpublished = {\url{http://www-nlpir.nist.gov/projects/duc/pubs/2003slides/duc2003intro.pdf}},
 interhash = {d76270e046732aa34bff1beec17bad23},
 intrahash = {f9cfa29d7fd8761529266b686ea74c31},
 keywords = {language summarization NLP},
 timestamp = {2011-08-28T03:10:47.000+0200},
 title = {An Introduction to {DUC} 2003: Intrinsic Evaluation of Generic News Text Summarization Systems},
 username = {lantiq},
 year = {2003}
}

@article{Pan2010TransferLearning,
 author = {Pan, Sinno Jialin and Yang, Qiang},
 doi = {10.1109/TKDE.2009.191},
 file = {:PDF/A_Survey_on_Transfer_Learning.pdf:PDF},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 number = {10},
 pages = {1345-1359},
 title = {A Survey on Transfer Learning},
 volume = {22},
 year = {2010}
}

@InProceedings{Pan2020XLinear,
  author     = {Y and Pan and T and Yao and Y. Li and T. Mei},
  booktitle  = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {X-Linear Attention Networks for Image Captioning},
  year       = {2020},
  address    = {Los Alamitos, CA, USA},
  pages      = {10968-10977},
  publisher  = {IEEE Computer Society},
  doi        = {10.1109/CVPR42600.2020.01098},
  file       = {:PDF/X-linear attention networks for image captioning.pdf:PDF},
  groups     = {Self-Attention Encoding, Boosting LSTM with Self-Attention, attention taxonomy},
  keywords   = {visualization;decoding;cognition;knowledge discovery;task analysis;aggregates;weight measurement},
  printed    = {yes},
  shorttitle = {X-Linear Attention...}
}

@article{Pan2021Mesa,
 author = {Zizheng Pan and Peng Chen and Haoyu He and Jing Liu and Jianfei Cai and Bohan Zhuang},
 comment = {https://github.com/zhuang-group/Mesa},
 file = {:PDF/Mesa- A Memory-saving Training Framework for Transformers.pdf:PDF},
 journal = {arXiv preprint arXiv:2111.11124},
 title = {Mesa: A Memory-saving Training Framework for Transformers},
 year = {2021}
}

@inproceedings{Papineni2002Bleu,
 address = {USA},
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
 booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
 doi = {10.3115/1073083.1073135},
 file = {:PDF/BLEU- A Method for Automatic Evaluation of Machine Translation.pdf:PDF},
 groups = {metryki},
 location = {Philadelphia, Pennsylvania},
 numpages = {8},
 pages = {311–318},
 publisher = {Association for Computational Linguistics},
 series = {ACL '02},
 title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
 year = {2002}
}

@inproceedings{Park2020MHSAN,
 author = {Park, Geondo and Han, Chihye and Yoon, Wonjun and Kim, Daeshik},
 booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
 file = {:PDF/MHSAN- Multi-Head Self-Attention Network for Visual Semantic Embedding.pdf:PDF},
 title = {MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding},
 year = {2020}
}

@inproceedings{Pascanau2013DifficultyRNN,
 author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 file = {:PDF/pascanu13.pdf:PDF},
 groups = {rnn},
 location = {Atlanta, GA, USA},
 pages = {III–1310–III–1318},
 publisher = {JMLR.org},
 series = {ICML'13},
 title = {On the difficulty of training recurrent neural networks},
 year = {2013}
}

@inproceedings{pascanu2013difficulty,
 author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
 booktitle = {International conference on machine learning},
 file = {:PDF/On the difficulty of training recurrent neural networks.pdf:PDF},
 organization = {PMLR},
 pages = {1310--1318},
 title = {On the difficulty of training recurrent neural networks},
 year = {2013}
}

@article{Pathak2021Conent,
 author = {Debanjan Pathak and U.S.N. Raju},
 doi = {10.1016/j.ijleo.2021.167754},
 file = {:PDF/Content-based image retrieval using feature-fusion of GroupNormalized-Inception-Darknet-53 features and handcraft features.pdf:PDF},
 issn = {0030-4026},
 journal = {Optik},
 keywords = {CBIR, CNN, Feature-fusion, Inception layer, Group normalization, DDBTC},
 pages = {167754},
 title = {Content-based image retrieval using feature-fusion of GroupNormalized-Inception-Darknet-53 features and handcraft features},
 url = {https://www.sciencedirect.com/science/article/pii/S0030402621013486},
 volume = {246},
 year = {2021}
}

@article{Patterson2014Sun,
 address = {USA},
 author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
 doi = {10.1007/s11263-013-0695-z},
 file = {:PDF/Common_Subspace_for_Model_and_Similarity_Phrase_Learning_for_Caption_Generation_from_Images.pdf:PDF},
 groups = {retrieval},
 issn = {0920-5691},
 issue_date = {May 2014},
 journal = {Int. J. Comput. Vision},
 keywords = {Scene understanding, Scene parsing, Image captioning, Crowdsourcing, Attributes},
 number = {1–2},
 numpages = {23},
 pages = {59–81},
 publisher = {Kluwer Academic Publishers},
 title = {The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding},
 url = {https://doi.org/10.1007/s11263-013-0695-z},
 volume = {108},
 year = {2014}
}

@inproceedings{Pavlopoulos2019Survey,
 address = {Minneapolis, Minnesota},
 author = {Pavlopoulos, John and Kougia, Vasiliki and Androutsopoulos, Ion},
 booktitle = {Proceedings of the Second Workshop on Shortcomings in Vision and Language},
 doi = {10.18653/v1/W19-1803},
 file = {:PDF/A Survey on Biomedical Image Captioning.pdf:PDF},
 pages = {26--36},
 publisher = {Association for Computational Linguistics},
 title = {A Survey on Biomedical Image Captioning},
 url = {https://www.aclweb.org/anthology/W19-1803},
 year = {2019}
}

@article{Payandeh2023DeepRepresentationLearning,
 author = {Payandeh, Amirreza and Baghaei, Kourosh T. and Fayyazsanavi, Pooya and Ramezani, Somayeh Bakhtiari and Chen, Zhiqian and Rahimi, Shahram},
 doi = {10.1109/ACCESS.2023.3335196},
 file = {:PDF/Deep_Representation_Learning_Fundamentals_Technologies_Applications_and_Open_Challenges.pdf:PDF},
 groups = {representation learning},
 issn = {2169-3536},
 journal = {IEEE Access},
 keywords = {Representation learning;Natural language processing;Task analysis;Generative adversarial networks;Feature extraction;Deep learning;Training;Transfer learning;Computer vision;Representation learning;deep learning;feature extraction;transfer learning;natural language processing;computer vision},
 pages = {137621-137659},
 title = {Deep Representation Learning: Fundamentals, Technologies, Applications, and Open Challenges},
 volume = {11},
 year = {2023}
}

@inproceedings{Pedersoli2017Areas,
 author = {Pedersoli, Marco and Lucas, Thomas and Schmid, Cordelia and Verbeek, Jakob},
 booktitle = {Proceedings of the IEEE international conference on computer vision},
 file = {:PDF/Pedersoli_Areas_of_Attention_ICCV_2017_paper.pdf:PDF},
 groups = {single layer LSTM, Geometric Transforms, attention taxonomy},
 pages = {1242--1250},
 title = {Areas of attention for image captioning},
 year = {2017}
}

@article{Pengpeng2022S2,
 author = {Pengpeng Zeng and Haonan Zhang and Jingkuan Song and Lianli Gao},
 booktitle = {IJCAI},
 file = {:PDF/0224.pdf:PDF;:PDF/0224.pdf:PDF},
 pages = {1608--1614},
 title = {S2 Transformer for Image Captioning},
 year = {2022}
}

@inproceedings{Pennington2014GLOVE,
 address = {Doha, Qatar},
 author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
 doi = {10.3115/v1/D14-1162},
 editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
 file = {:PDF/GloVe Global Vectors for Word Representation.pdf:PDF},
 pages = {1532--1543},
 publisher = {Association for Computational Linguistics},
 title = {{G}lo{V}e: Global Vectors for Word Representation},
 year = {2014}
}

@article{Perone2018,
 author = {Christian S. Perone and Roberto Pereira Silveira and Thomas S. Paula},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1806-06259.bib},
 eprint = {1806.06259},
 eprinttype = {arXiv},
 groups = {embeddings},
 journal = {CoRR},
 timestamp = {Tue, 20 Jul 2021 13:58:53 +0200},
 title = {Evaluation of sentence embeddings in downstream and linguistic probing tasks},
 url = {http://arxiv.org/abs/1806.06259},
 volume = {abs/1806.06259},
 year = {2018}
}

@inproceedings{Peters2018Deep,
 address = {New Orleans, Louisiana},
 author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
 doi = {10.18653/v1/N18-1202},
 editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
 file = {:PDF/N18-1202.pdf:PDF},
 pages = {2227--2237},
 publisher = {Association for Computational Linguistics},
 title = {Deep Contextualized Word Representations},
 url = {https://aclanthology.org/N18-1202/},
 year = {2018}
}

@inproceedings{Peyre2017ICCV,
 author = {Peyre, Julia and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
 booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
 file = {:PDF/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf:PDF},
 title = {Weakly-Supervised Learning of Visual Relations},
 url = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Peyre_Weakly-Supervised_Learning_of_ICCV_2017_paper.pdf},
 year = {2017}
}

@inproceedings{Peyre2019ICCV,
 author = {Peyre, Julia and Laptev, Ivan and Schmid, Cordelia and Sivic, Josef},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
 file = {:PDF/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf:PDF},
 title = {Detecting Unseen Visual Relations Using Analogies},
 url = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf},
 year = {2019}
}

@conference{Pierrard2018Learning,
 art_number = {8491559},
 author = {Pierrard, R. and Poli, J.-P. and Hudelot, C.},
 document_type = {Conference Paper},
 doi = {10.1109/Fuzz-Ieee.2018.8491538},
 file = {:PDF/Learning_fuzzy_relations_Pierrard2018.pdf:PDF},
 journal = {IEEE International Conference on Fuzzy Systems},
 note = {cited By 3},
 source = {Scopus},
 title = {Learning fuzzy relations and properties for explainable artificial intelligence},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060479646&doi=10.1109%2fFuzz-Ieee.2018.8491538&partnerID=40&md5=71faa5d1ed83794a9cabf7208afbf638},
 volume = {2018-July},
 year = {2018}
}

@Article{Pierrard2021Spatial,
  author        = {Pierrard, R. and Poli, J.-P. and Hudelot, C.},
  journal       = {Artificial Intelligence},
  title         = {Spatial relation learning for explainable image classification and annotation in critical applications},
  year          = {2021},
  note          = {cited By 0},
  volume        = {292},
  art_number    = {103434},
  document_type = {Article},
  doi           = {10.1016/j.artint.2020.103434},
  file          = {:PDF/Spatial-relation-learning-for-explainable-image-classification-and-annotation-in-critical-applications2021Artificial-Intelligence.pdf:PDF},
  groups        = {datasets},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097535218&doi=10.1016%2fj.artint.2020.103434&partnerID=40&md5=aaa35b76d6ea225b37454c1e99e3aa2d},
}

@article{Pierre2020,
 author = {Pierre L. Dognin and Igor Melnyk and Youssef Mroueh and Inkit Padhi and Mattia Rigotti and Jarret Ross and Yair Schiff and Richard A. Young and Brian Belgodere},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2012-11696.bib},
 eprint = {2012.11696},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 06 Jan 2021 15:26:52 +0100},
 title = {Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge},
 url = {https://arxiv.org/abs/2012.11696},
 volume = {abs/2012.11696},
 year = {2020}
}

@inproceedings{Piyush2018Conceptual,
 address = {Melbourne, Australia},
 author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
 booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/P18-1238},
 editor = {Gurevych, Iryna and Miyao, Yusuke},
 file = {:PDF/Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:PDF},
 pages = {2556--2565},
 publisher = {Association for Computational Linguistics},
 title = {Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
 year = {2018}
}

@misc{polish-nlp-resources,
 author = {S{\l}awomir Dadas},
 howpublished = {Github},
 title = {A repository of Polish {NLP} resources},
 url = {https://github.com/sdadas/polish-nlp-resources/},
 year = {2019}
}

@inproceedings{Ponte1998LMInformationRetrieval,
 address = {New York, NY, USA},
 author = {Ponte, Jay M. and Croft, W. Bruce},
 booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 doi = {10.1145/290941.291008},
 file = {:PDF/290941.291008.pdf:PDF},
 groups = {lm},
 isbn = {1581130155},
 location = {Melbourne, Australia},
 numpages = {7},
 pages = {275–281},
 publisher = {Association for Computing Machinery},
 series = {SIGIR '98},
 title = {A language modeling approach to information retrieval},
 url = {https://doi.org/10.1145/290941.291008},
 year = {1998}
}

@article{Porter1980Stemmer,
 author = {Porter, M. F.},
 day = {01},
 doi = {10.1108/eb046814},
 file = {:PDF/Porter-1980.pdf:PDF},
 issn = {0033-0337},
 journal = {Program},
 number = {3},
 pages = {130-137},
 publisher = {MCB UP Ltd},
 title = {An algorithm for suffix stripping},
 volume = {14},
 year = {1980}
}

@article{Priya2019Hungarian,
 author = {Priya, D and Ramesh, G},
 doi = {10.1088/1742-6596/1377/1/012046},
 file = {:PDF/The_Hungarian_Method_for_the_Assignment_Problem_Wi.pdf:PDF},
 journal = {Journal of Physics: Conference Series},
 pages = {012046},
 title = {The Hungarian Method for the Assignment Problem, With Generalized Interval Arithmetic and Its Applications},
 volume = {1377},
 year = {2019}
}

@book{Przepiorkowski2002FormalnyOpis,
 address = {Warszawa},
 author = {Adam Przepiórkowski and Anna Kupść and Małgorzata Marciniak and Agnieszka Mykowiecka},
 file = {:PDF/hpsg.pdf:PDF},
 groups = {opis polskiego},
 publisher = {aowe},
 title = {Formalny opis języka polskiego: Teoria i implementacja},
 year = {2002}
}

@inproceedings{Pu2016,
 address = {Cadiz, Spain},
 author = {Pu, Yunchen and Yuan, Win and Stevens, Andrew and Li, Chunyuan and Carin, Lawrence},
 booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
 editor = {Gretton, Arthur and Robert, Christian C.},
 file = {:PDF/pu16.pdf:PDF},
 groups = {encoder-decoder-lit},
 pages = {741--750},
 pdf = {http://proceedings.mlr.press/v51/pu16.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {A Deep Generative Deconvolutional Image Model},
 url = {https://proceedings.mlr.press/v51/pu16.html},
 volume = {51},
 year = {2016}
}

@article{Qi2020ImageBERTCP,
 author = {Di Qi and Lin Su and Jianwei Song and Edward Cui and Taroon Bharti and Arun Sacheti},
 file = {:PDF/2001.07966v2.pdf:PDF},
 groups = {datasets},
 journal = {ArXiv},
 title = {ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data},
 url = {https://api.semanticscholar.org/CorpusID:210859480},
 volume = {abs/2001.07966},
 year = {2020}
}

@article{Qi2025Markov,
 author = {Qi, Zhenhan},
 file = {:PDF/An_Analysis_of_Markov_Models_Applications.pdf:PDF},
 groups = {lm},
 journal = {Theoretical and Natural Science},
 number = {1},
 pages = {82--87},
 publisher = {EWA Publishing},
 title = {An analysis of Markov models applications},
 volume = {92},
 year = {2025}
}

@InProceedings{Quin2019LookBackRegions,
  author    = {Qin, Yu and Du, Jiajun and Zhang, Yonghua and Lu, Hongtao},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Look Back and Predict Forward in Image Captioning},
  year      = {2019},
  pages     = {8361},
  abstract  = {Most existing attention-based methods on image captioning focus on the current word and visual information in one time step and generate the next word, without considering the visual and linguistic coherence. We propose Look Back (LB) method to embed visual information from the past and Predict Forward (PF) approach to look into future. LB method introduces attention value from the previous time step into the current attention generation to suit visual coherence of human. PF model predicts the next two words in one time step and jointly employs their probabilities for inference. Then the two approaches are combined together as LBPF to further integrate visual information from the past and linguistic information in the future to improve image captioning performance. All the three methods are applied on a classic base decoder, and show remarkable improvements on MSCOCO dataset with small increments on parameter counts. Our LBPF model achieves BLEU-4 / CIDEr / SPICE scores of 37.4 / 116.4 / 21.2 with cross-entropy loss and 38.3 / 127.6 / 22.0 with CIDEr optimization. Our three proposed methods can be easily applied on most attention-based encoder-decoder models for image captioning.},
  doi       = {10.1109/CVPR.2019.00856},
  file      = {:PDF/Look_Back_and_Predict_Forward_in_Image_Captioning.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Vision + Language;Deep Learning ; Image and Video Synthesis},
}

@inproceedings{Radford2018ImprovingLU,
 author = {Alec Radford and Karthik Narasimhan},
 file = {:PDF/GPT-1.pdf:PDF},
 title = {Improving Language Understanding by Generative Pre-Training},
 url = {https://api.semanticscholar.org/CorpusID:49313245},
 year = {2018}
}

@inproceedings{Radford2021LearningTV,
 author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
 booktitle = {Proceedings of the 38th International Conference on Machine Learning},
 editor = {Meila, Marina and Zhang, Tong},
 file = {:PDF/2103.00020v1:00020v1},
 pages = {8748--8763},
 pdf = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Learning Transferable Visual Models From Natural Language Supervision},
 url = {https://proceedings.mlr.press/v139/radford21a.html},
 volume = {139},
 year = {2021}
}

@inproceedings{Raina2007,
 address = {New York, NY, USA},
 author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 doi = {10.1145/1273496.1273592},
 file = {:PDF/1273496.1273592.pdf:PDF},
 isbn = {9781595937933},
 location = {Corvalis, Oregon, USA},
 numpages = {8},
 pages = {759–766},
 publisher = {Association for Computing Machinery},
 series = {ICML '07},
 title = {Self-Taught Learning: Transfer Learning from Unlabeled Data},
 url = {https://doi.org/10.1145/1273496.1273592},
 year = {2007}
}

@misc{Ralf2019LSTM,
 archiveprefix = {arXiv},
 author = {Ralf C. Staudemeyer and Eric Rothstein Morris},
 eprint = {1909.09586},
 file = {:PDF/1909.09586v1.pdf:PDF},
 groups = {LSTM},
 primaryclass = {cs.NE},
 title = {Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks},
 url = {https://arxiv.org/abs/1909.09586},
 year = {2019}
}

@inproceedings{Ramanishka2017TopDown,
 address = {Los Alamitos, CA, USA},
 author = {Ramanishka, Vasili and Das, Abir and Zhang, Jianming and Saenko, Kate},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2017.334},
 file = {:PDF/Top-Down_Visual_Saliency_Guided_by_Captions.pdf:PDF},
 groups = {Exploiting human attention},
 issn = {1063-6919},
 keywords = {Visualization;Decoding;Computational modeling;Heating systems;Analytical models;Predictive models},
 pages = {3135-3144},
 publisher = {IEEE Computer Society},
 title = {{ Top-Down Visual Saliency Guided by Captions }},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.334},
 year = {2017}
}

@article{Ramisa2018BreakingNewsAA,
 author = {Arnau Ramisa and Fei Yan and Francesc Moreno-Noguer and Krystian Mikolajczyk},
 file = {:/Users/mateuszb/Nextcloud/bib/Mateusz doktorat/PDF/Breaking News- Article Annotatdion by Image and Text processing.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 pages = {1072-1085},
 title = {BreakingNews: Article Annotation by Image and Text Processing},
 volume = {40},
 year = {2018}
}

@inproceedings{Randell1992Spatial,
 address = {San Francisco, CA, USA},
 author = {Randell, David A. and Cui, Zhan and Cohn, Anthony G.},
 booktitle = {Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning},
 file = {:PDF/A_Spatial_Logic_based_on_Regions_and_Connection.pdf:PDF},
 isbn = {1558602623},
 location = {Cambridge, MA},
 numpages = {12},
 pages = {165–176},
 publisher = {Morgan Kaufmann Publishers Inc.},
 series = {KR'92},
 title = {A Spatial Logic Based on Regions and Connection},
 url = {https://www.researchgate.net/publication/221393453_A_Spatial_Logic_based_on_Regions_and_Connection},
 year = {1992}
}

@article{Ranzato2016SequenceLT,
 author = {Marc'Aurelio Ranzato and Sumit Chopra and Michael Auli and Wojciech Zaremba},
 file = {:PDF/Sequence level training with recurrent neural networks.pdf:PDF},
 journal = {CoRR},
 title = {Sequence Level Training with Recurrent Neural Networks},
 volume = {abs/1511.06732},
 year = {2016}
}

@proceedings{Rashtchian2010Collecting,
 address = {Los Angeles},
 editor = {Callison-Burch, Chris and Dredze, Mark},
 file = {:PDF/rashtchian2010Collecting.pdf:PDF},
 publisher = {Association for Computational Linguistics},
 title = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk},
 url = {https://www.aclweb.org/anthology/W10-0700},
 year = {2010}
}

@inproceedings{RashtchianEtal2010Collecting,
 address = {Los Angeles},
 author = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
 booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk},
 file = {:PDF/Collecting Image Annotations Using Amazon’s Mechanical Turk.pdf:PDF;:PDF/The Role of Syntactic Planning in Compositional Image Captioning.pdf:PDF},
 pages = {139--147},
 publisher = {Association for Computational Linguistics},
 title = {Collecting Image Annotations Using {A}mazon{'}s {M}echanical {T}urk},
 url = {https://www.aclweb.org/anthology/W10-0721},
 year = {2010}
}

@inproceedings{Rebuffi2020There,
 author = {Rebuffi, Sylvestre-Alvise and Fong, Ruth and Ji, Xu and Vedaldi, Andrea},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 file = {:PDF/There and back again- Revisiting backpropagation saliency methods.pdf:PDF},
 pages = {8839--8848},
 title = {There and back again: Revisiting backpropagation saliency methods},
 url = {https://arxiv.org/pdf/2004.02866.pdf},
 year = {2020}
}

@inproceedings{Redmon2016You,
 author = {J. {Redmon} and S. {Divvala} and R. {Girshick} and A. {Farhadi}},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.91},
 file = {:PDF/Redmon2016.pdf:PDF},
 pages = {779-788},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 year = {2016}
}

@inproceedings{Redmon2017YOLO9000,
 author = {J. {Redmon} and A. {Farhadi}},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2017.690},
 file = {:PDF/YOLO9000Redmon.pdf:PDF},
 pages = {6517-6525},
 title = {YOLO9000: Better, Faster, Stronger},
 year = {2017}
}

@misc{Redmon2018YOLOv3,
 author = {Joseph Redmon and Ali Farhadi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
 eprint = {1804.02767},
 eprinttype = {arXiv},
 file = {:PDF/Redmon2018.pdf:PDF},
 journal = {CoRR},
 primaryclass = {cs.CV},
 timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},
 title = {YOLOv3: An Incremental Improvement},
 url = {http://arxiv.org/abs/1804.02767},
 volume = {abs/1804.02767},
 year = {2018}
}

@inproceedings{Rehurek2010Gensim,
 address = {Valletta, Malta},
 author = {Řehůřek, Radim and Sojka, Petr},
 booktitle = {Proceedings of LREC 2010 workshop New Challenges for NLP Frameworks},
 file = {:PDF/lrec2010_final.pdf:PDF},
 howpublished = {paměťový nosič},
 isbn = {2-9517408-6-7},
 keywords = {document similarity; NLP; software; vector space model; topical modelling; software framework; topical document similarity; Python; IR; LSA; LDA; gensim; DML-CZ},
 language = {eng},
 location = {Valletta, Malta},
 pages = {46--50},
 publisher = {University of Malta},
 title = {Software Framework for Topic Modelling with Large Corpora},
 url = {http://nlp.fi.muni.cz/projekty/gensim/},
 year = {2010}
}

@Article{Reiter2009Investigation,
  author     = {Reiter, Ehud and Belz, Anja},
  journal    = {Comput. Linguist.},
  title      = {An investigation into the validity of some metrics for automatically evaluating natural language generation systems},
  year       = {2009},
  issn       = {0891-2017},
  month      = dec,
  number     = {4},
  pages      = {529–558},
  volume     = {35},
  abstract   = {There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/coli.2009.35.4.35405},
  file       = {:PDF/J09-4008.pdf:PDF},
  issue_date = {December 2009},
  numpages   = {30},
  publisher  = {MIT Press},
}

@inproceedings{Ren2015Faster,
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 file = {:PDF/Faster_R-CNN_Ren2017.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 volume = {28},
 year = {2015}
}

@article{Ren2017DeepRL,
 author = {Zhou Ren and Xiaoyu Wang and Ning Zhang and Xutao Lv and Li-Jia Li},
 file = {:PDF/Deep Reinforcement Learning-Based Image Captioning with Embedding Reward.pdf:PDF},
 groups = {Other deep learning methods},
 journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {1151-1159},
 title = {Deep Reinforcement Learning-Based Image Captioning with Embedding Reward},
 year = {2017}
}

@article{Ren2017Faster,
 author = {S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}},
 doi = {10.1109/TPAMI.2016.2577031},
 file = {:PDF/Faster_R-CNN_Ren2017.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {6},
 pages = {1137-1149},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 volume = {39},
 year = {2017}
}

@InProceedings{Rennie2017SelfCriticalST,
  author    = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Self-Critical Sequence Training for Image Captioning},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {1179-1195},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.131},
  file      = {:PDF/Self- critical sequence training for image captioning.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Training;Inference algorithms;Measurement;Logic gates;Predictive models;Learning (artificial intelligence)},
}

@article{Rensink2000Dynamic,
 author = {Rensink, Ronald},
 doi = {10.1080/135062800394667},
 file = {:PDF/The Dynamic Representation of Scenes .pdf:PDF},
 journal = {Visual Cognition},
 pages = {17-42},
 title = {The Dynamic Representation of Scenes},
 volume = {7},
 year = {2000}
}

@inbook{Robinson2014,
 address = {Dordrecht},
 author = {Robinson, John},
 booktitle = {Encyclopedia of Quality of Life and Well-Being Research},
 doi = {10.1007/978-94-007-0753-5_1654},
 isbn = {978-94-007-0753-5},
 pages = {3620--3621},
 publisher = {Springer Netherlands},
 title = {Likert Scale},
 year = {2014}
}

@inproceedings{Rohrbach2018ObjectHI,
 address = {Brussels, Belgium},
 author = {Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/D18-1437},
 file = {:PDF/Object Hallucination in Image Captioning.pdf:PDF},
 pages = {4035--4045},
 publisher = {Association for Computational Linguistics},
 title = {Object Hallucination in Image Captioning},
 url = {https://aclanthology.org/D18-1437},
 year = {2018}
}

@article{Rong2014Word2VecExplained,
 author = {Xin Rong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/Rong14.bib},
 eprint = {1411.2738},
 eprinttype = {arXiv},
 file = {:PDF/1411.2738v4.pdf:PDF},
 groups = {word2vec},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
 title = {word2vec Parameter Learning Explained},
 url = {http://arxiv.org/abs/1411.2738},
 volume = {abs/1411.2738},
 year = {2014}
}

@misc{Roy2020efficient,
 author = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},
 comment = {https://github.com/lucidrains/routing-transformer},
 file = {:PDF/2003.05997.pdf:PDF},
 title = {Efficient Content-Based Sparse Attention with Routing Transformers},
 url = {https://arxiv.org/pdf/2003.05997.pdf},
 year = {2020}
}

@article{Russakovsky2015Imagenet,
 author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
 day = {01},
 doi = {10.1007/s11263-015-0816-y},
 file = {:PDF/Russakovsky2015_Article_ImageNetLargeScaleVisualRecogn.pdf:PDF},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 number = {3},
 pages = {211-252},
 title = {ImageNet Large Scale Visual Recognition Challenge},
 url = {https://doi.org/10.1007/s11263-015-0816-y},
 volume = {115},
 year = {2015}
}

@article{Ryoma2021WMDRe,
 author = {Ryoma Sato and Makoto Yamada and Hisashi Kashima},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2105-14403.bib},
 eprint = {2105.14403},
 eprinttype = {arXiv},
 file = {:PDF/sato22b.pdf:PDF},
 group = {metryki},
 journal = {CoRR},
 timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
 title = {Re-evaluating Word Mover's Distance},
 url = {https://arxiv.org/abs/2105.14403},
 volume = {abs/2105.14403},
 year = {2021}
}

@article{Sabbeh2023WordEmbedding,
 article-number = {1425},
 author = {Sabbeh, Sahar F. and Fasihuddin, Heba A.},
 doi = {10.3390/electronics12061425},
 file = {:PDF/electronics-12-01425.pdf:PDF},
 issn = {2079-9292},
 journal = {Electronics},
 number = {6},
 title = {A Comparative Analysis of Word Embedding and Deep Learning for Arabic Sentiment Classification},
 url = {https://www.mdpi.com/2079-9292/12/6/1425},
 volume = {12},
 year = {2023}
}

@article{Salem2018Fuzzy,
 author = {Salem, Omar and Wang, Liwei},
 doi = {10.4018/IJSI.2018010105},
 file = {:PDF/FuzzyMutualInformationFeatureSelectionBasedonRepresentativeSamples.pdf:PDF},
 journal = {International Journal of Software Innovation},
 pages = {58-72},
 title = {Fuzzy Mutual Information Feature Selection Based on Representative Samples},
 volume = {6},
 year = {2018}
}

@article{Samar2023Enhanced,
 author = {Samar Elbedwehy and T. Medhat and Taher Hamza and Mohammed F. Alrahmawy},
 doi = {10.32604/csse.2023.038376},
 file = {:PDF/1-s2.0-S0031320323001218-main.pdf:PDF},
 journal = {Computer Systems Science and Engineering},
 number = {3},
 pages = {3637--3652},
 title = {Enhanced Image Captioning Using Features Concatenation and Efficient Pre-Trained Word Embedding},
 volume = {46},
 year = {2023}
}

@inproceedings{Sanchez2005Fuzzy,
 author = {Sanchez, Luciano and Su{\'a}rez, M Rosario and Couso, In{\'e}s},
 booktitle = {Proc. Internat. Conf. on Machine Intelligence (ACIDCA-ICMI05), Tozeur, Tunisia},
 file = {:PDF/A fuzzy definition of Mutual Information with application to the design of Genetic Fuzzy Classifiers.pdf:PDF},
 pages = {602--609},
 title = {A fuzzy definition of Mutual Information with application to the design of Genetic Fuzzy Classifiers},
 url = {https://sci2s.ugr.es/keel/pdf/keel/congreso/SPS123.pdf},
 year = {2005}
}

@inproceedings{Sanchez2007Some,
 author = {Sanchez, Luciano and Suarez, M. Rosario and Villar, J. R. and Couso, Ines},
 booktitle = {2007 IEEE International Fuzzy Systems Conference},
 doi = {10.1109/FUZZY.2007.4295665},
 file = {:PDF/Some_Results_about_Mutual_Information-based_Featur.pdf:PDF},
 pages = {1-6},
 title = {Some Results about Mutual Information-based Feature Selection and Fuzzy Discretization of Vague Data},
 year = {2007}
}

@inproceedings{Sandler2018MobilenetV2,
 author = {M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2018.00474},
 file = {:PDF/MobleNetv2_Sandler2018.pdf:PDF},
 pages = {4510-4520},
 title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
 year = {2018}
}

@article{Santoro2017Simple,
 author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 file = {:PDF/a-simple-neural-network-module-for-relational-reasoning-Paper.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 timestamp = {Wed, 24 Jul 2019 11:32:12 +0200},
 title = {A simple neural network module for relational reasoning},
 url = {https://proceedings.neurips.cc/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Santos2022PortugeseDataset,
 article-number = {13},
 author = {dos Santos, Gabriel Oliveira and Colombini, Esther Luna and Avila, Sandra},
 doi = {10.3390/data7020013},
 file = {:PDF/#PraCegoVer- A Large Dataset for Image Captioning in Portuguese.pdf:PDF},
 issn = {2306-5729},
 journal = {Data},
 number = {2},
 title = {#PraCegoVer: A Large Dataset for Image Captioning in Portuguese},
 url = {https://www.mdpi.com/2306-5729/7/2/13},
 volume = {7},
 year = {2022}
}

@book{Santosh2018Document,
 author = {Santosh, K.C.},
 document_type = {Book},
 doi = {10.1007/978-981-13-2339-3},
 file = {:PDF/Document-image-analysis-Current-trends-and-challenges-in-graphics-recognition2018.pdf:PDF},
 journal = {Document Image Analysis: Current Trends and Challenges in Graphics Recognition},
 note = {cited By 7},
 pages = {1-174},
 source = {Scopus},
 title = {Document image analysis: Current trends and challenges in graphics recognition},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063493512&doi=10.1007%2f978-981-13-2339-3&partnerID=40&md5=d990830b1b7cd29105a5fb13aa520401},
 year = {2018}
}

@article{Sarica2021StopWords,
 author = {Sarica, Serhad AND Luo, Jianxi},
 doi = {10.1371/journal.pone.0254937},
 journal = {PLOS ONE},
 number = {8},
 pages = {1-13},
 publisher = {Public Library of Science},
 title = {Stopwords in technical language processing},
 volume = {16},
 year = {2021}
}

@inproceedings{Sariyildiz2020LearnVisRepr,
 address = {Berlin, Heidelberg},
 author = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
 booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII},
 doi = {10.1007/978-3-030-58598-3_10},
 file = {:PDF/2008.01392v1.pdf:PDF},
 isbn = {978-3-030-58597-6},
 location = {Glasgow, United Kingdom},
 numpages = {18},
 pages = {153–170},
 publisher = {Springer-Verlag},
 title = {Learning Visual Representations with&nbsp;Caption Annotations},
 url = {https://doi.org/10.1007/978-3-030-58598-3_10},
 year = {2020}
}

@article{Sarto2024Retrieval,
 address = {New York, NY, USA},
 articleno = {242},
 author = {Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Nicolosi, Alessandro and Cucchiara, Rita},
 doi = {10.1145/3663667},
 issn = {1551-6857},
 issue_date = {August 2024},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 keywords = {Image captioning, image retrieval, vision-and-language},
 number = {8},
 numpages = {22},
 publisher = {Association for Computing Machinery},
 title = {Towards Retrieval-Augmented Architectures for Image Captioning},
 volume = {20},
 year = {2024}
}

@inproceedings{Sarto22Retrieval,
 address = {New York, NY, USA},
 author = {Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
 booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
 doi = {10.1145/3549555.3549585},
 file = {:PDF/3549555.3549585.pdf:PDF},
 isbn = {9781450397209},
 keywords = {vision-and-language., image retrieval, image captioning},
 location = {Graz, Austria},
 numpages = {7},
 pages = {1–7},
 publisher = {Association for Computing Machinery},
 series = {CBMI '22},
 title = {Retrieval-Augmented Transformer for Image Captioning},
 year = {2022}
}

@article{Scaiella2019Large,
 author = {Scaiella, Antonio and Croce, Danilo and Basili, Roberto},
 editor = {Roberto Basili and Simonetta Montemagni},
 file = {:PDF/Large scale datasets for Image and Video Captioning in Italian.pdf:PDF},
 journal = {Italian Journal of Computational Linguistics},
 number = {5},
 pages = {49-60},
 publisher = {Accademia University Press},
 title = {Large scale datasets for Image and Video Captioning in Italian},
 url = {http://www.ai-lc.it/IJCoL/v5n2/IJCOL_5_2_3___scaiella_et_al.pdf},
 volume = {2},
 year = {2019}
}


@inproceedings{Schneider2017Regnet,
 author = {Schneider, Nick and Piewak, Florian and Stiller, Christoph and Franke, Uwe},
 booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
 doi = {10.1109/IVS.2017.7995968},
 file = {:PDF/2101.00590v1.pdf:PDF},
 keywords = {Calibration;Feature extraction;Laser radar;Cameras;Sensor systems;Neural networks},
 pages = {1803-1810},
 title = {RegNet: Multimodal sensor registration using deep neural networks},
 year = {2017}
}

@inproceedings{Schumann2021Step,
 author = {Candice Schumann and Susanna Ricco and Utsav Prabhu and Vittorio Ferrari and Caroline Rebecca Pantofaru},
 booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
 file = {:PDF/A Step Toward More Inclusive People Annotations for Fairness.pdf:PDF},
 title = {A Step Toward More Inclusive People Annotations for Fairness},
 year = {2021}
}

@article{schuster1997,
 author = {Schuster, Mike and Paliwal, Kuldip},
 doi = {10.1109/78.650093},
 file = {:PDF/Bidirectional recurrent neural networks .pdf:PDF},
 journal = {Signal Processing, IEEE Transactions on},
 pages = {2673 - 2681},
 title = {Bidirectional recurrent neural networks},
 volume = {45},
 year = {1997}
}

@inproceedings{Schuster2015Generating,
 address = {Lisbon, Portugal},
 author = {Schuster, Sebastian and Krishna, Ranjay and Chang, Angel and Fei-Fei, Li and Manning, Christopher D.},
 booktitle = {Proceedings of the Fourth Workshop on Vision and Language},
 doi = {10.18653/v1/W15-2812},
 editor = {Belz, Anja and Coheur, Luisa and Ferrari, Vittorio and Moens, Marie-Francine and Pastra, Katerina and Vuli{\'c}, Ivan},
 file = {:PDF/W15-2812.pdf:PDF;:PDF/W15-2812.pdf:PDF},
 pages = {70--80},
 publisher = {Association for Computational Linguistics},
 title = {Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval},
 year = {2015}
}

@inproceedings{Shabani2012Efficient,
 author = {A. {Shabani} and P. {Matsakis}},
 booktitle = {2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)},
 doi = {10.1109/CCECE.2012.6335023},
 file = {:PDF/Efficient_computation_Shabani2012.pdf:PDF},
 pages = {1-4},
 title = {Efficient computation of objects' spatial relations in digital images},
 year = {2012}
}

@article{Shakarami2020Efficient,
 author = {Shakarami, Ashkan and Tarrah, Hadis},
 file = {:PDF/An efficient image descriptor for image classification and CBIR.pdf:PDF},
 journal = {Optik},
 pages = {164833},
 publisher = {Elsevier},
 title = {An efficient image descriptor for image classification and CBIR},
 url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7198219/pdf/main.pdf},
 volume = {214},
 year = {2020}
}

@inproceedings{Shao2019Objects365,
 author = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
 file = {:PDF/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf:PDF},
 title = {Objects365: A Large-Scale, High-Quality Dataset for Object Detection},
 year = {2019}
}

@article{Shao2021Sequence,
 author = {Shao, Chenze and Feng, Yang and Zhang, Jinchao and Meng, Fandong and Zhou, Jie},
 doi = {10.1162/coli_a_00421},
 file = {:PDF/coli_a_00421.pdf:PDF},
 issn = {0891-2017},
 journal = {Computational Linguistics},
 number = {4},
 pages = {891-925},
 title = {Sequence-Level Training for Non-Autoregressive Neural Machine Translation},
 volume = {47},
 year = {2021}
}

@inproceedings{Sharma1995Inferences,
 address = {Berlin, Heidelberg},
 author = {Sharma, Jayant and Flewelling, Douglas M.},
 booktitle = {Advances in Spatial Databases},
 editor = {Egenhofer, Max J. and Herring, John R.},
 file = {:PDF/Sharma-Flewelling1995_Chapter_InferencesFromCombinedKnowledg.pdf:PDF},
 isbn = {978-3-540-49536-9},
 pages = {279--291},
 publisher = {Springer Berlin Heidelberg},
 title = {Inferences from combined knowledge about topology and directions},
 year = {1995}
}

@inproceedings{sharma2020Survey,
 author = {Sharma, Himanshu and Agrahari, Manmohan and Singh, Sujeet Kumar and Firoj, Mohd and Mishra, Ravi Kumar},
 booktitle = {2020 International Conference on Power Electronics IoT Applications in Renewable Energy and its Control (PARC)},
 doi = {10.1109/PARC49193.2020.236619},
 file = {:PDF/Image_Captioning_A_Comprehensive_Survey.pdf:PDF},
 groups = {review},
 pages = {325-328},
 title = {Image Captioning: A Comprehensive Survey},
 year = {2020}
}

@article{Sharma2024Survey,
 author = {Sharma, Himanshu and Padha, Devanand},
 doi = {10.1007/s10462-023-10488-2},
 file = {:PDF/s10462-023-10488-2.pdf:PDF},
 groups = {review},
 journal = {Artificial Intelligence Review},
 pages = {1-43},
 title = {A comprehensive survey on image captioning: from handcrafted to deep learning-based techniques, a taxonomy and open research issues},
 volume = {56},
 year = {2023}
}

@article{Shelhamer2017Fully,
 author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
 doi = {10.1109/TPAMI.2016.2572683},
 file = {:PDF/Fully_Convolutional_Networks_for_Semantic_Segmentation.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {4},
 pages = {640-651},
 title = {Fully Convolutional Networks for Semantic Segmentation},
 volume = {39},
 year = {2017}
}

@inproceedings{Shetty2017Speaking,
 author = {Shetty, Rakshith and Rohrbach, Marcus and Hendricks, Lisa Anne and Fritz, Mario and Schiele, Bernt},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2017.445},
 file = {:PDF/Speaking the Same Language- Matching Machine to Human Captions by Adversarial Training.pdf:PDF},
 groups = {Other deep learning methods},
 pages = {4155-4164},
 title = {Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training},
 year = {2017}
}

@inproceedings{Shietal2020Improving,
 address = {Online},
 author = {Shi, Zhan and Zhou, Xu and Qiu, Xipeng and Zhu, Xiaodan},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 comment = {https://github.com/Gitsamshi/WeakVRD-Captioning},
 doi = {10.18653/v1/2020.acl-main.664},
 file = {:PDF/Improving Image Captioning with Better Use of Captions.pdf:PDF},
 groups = {Scene graphs, Two-layer LSTM},
 pages = {7454--7464},
 printed = {yes},
 publisher = {Association for Computational Linguistics},
 title = {Improving Image Captioning with Better Use of Caption},
 url = {https://aclanthology.org/2020.acl-main.664},
 year = {2020}
}

@inproceedings{Shuying2015VeryDeep,
 author = {Liu, Shuying and Deng, Weihong},
 booktitle = {2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)},
 doi = {10.1109/ACPR.2015.7486599},
 file = {:PDF/Very_deep_convolutional_neural_network_based_image_classification_using_small_training_sample_size.pdf:PDF},
 pages = {730-734},
 title = {Very deep convolutional neural network based image classification using small training sample size},
 year = {2015}
}

@InProceedings{sidorov2019textcaps,
  author    = {Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II},
  title     = {TextCaps: A Dataset for Image Captioning with Reading Comprehension},
  year      = {2020},
  address   = {Berlin, Heidelberg},
  pages     = {742–758},
  publisher = {Springer-Verlag},
  abstract  = {Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.},
  doi       = {10.1007/978-3-030-58536-5_44},
  isbn      = {978-3-030-58535-8},
  location  = {Glasgow, United Kingdom},
  numpages  = {17},
}

@article{Siino2024Preprocessing,
 author = {Marco Siino and Ilenia Tinnirello and Marco {La Cascia}},
 doi = {10.1016/j.is.2023.102342},
 file = {:PDF/1-s2.0-S0306437923001783-main.pdf:PDF},
 groups = {preprocessing},
 issn = {0306-4379},
 journal = {Information Systems},
 keywords = {Text preprocessing, Natural Language Processing, Fake news, SVM, Bayes, Transformers, Deep learning, LSTM, Convolutional neural networks},
 pages = {102342},
 title = {Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers},
 volume = {121},
 year = {2024}
}

@inproceedings{Simonyan15VeryDeep,
 author = {Karen Simonyan and Andrew Zisserman},
 booktitle = {International Conference on Learning Representations},
 file = {:PDF/simonyan15.pdf:PDF},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 year = {2015}
}

@article{Sinha2021MaskedLM,
 author = {Koustuv Sinha and Robin Jia and Dieuwke Hupkes and Joelle Pineau and Adina Williams and Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2104-06644.bib},
 eprint = {2104.06644},
 eprinttype = {arXiv},
 file = {:PDF/Masked Language Modeling and the Distributional Hypothesis- Order Word Matters Pre-training for Little.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
 title = {Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little},
 url = {https://arxiv.org/abs/2104.06644},
 volume = {abs/2104.06644},
 year = {2021}
}

@article{Smeulders2000Content,
 author = {A. W. M. {Smeulders} and M. {Worring} and S. {Santini} and A. {Gupta} and R. {Jain}},
 doi = {10.1109/34.895972},
 file = {:PDF/COntent_based_retrieval_PAMI_Smeulders2000.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {12},
 pages = {1349-1380},
 title = {Content-based image retrieval at the end of the early years},
 volume = {22},
 year = {2000}
}

@article{Socher2014Grounded,
 address = {Cambridge, MA},
 author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V. and Manning, Christopher D. and Ng, Andrew Y.},
 doi = {10.1162/tacl_a_00177},
 editor = {Lin, Dekang and Collins, Michael and Lee, Lillian},
 file = {:PDF/Q14-1017.pdf:PDF},
 groups = {Earlier Deep Models},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {207--218},
 publisher = {MIT Press},
 title = {Grounded Compositional Semantics for Finding and Describing Images with Sentences},
 volume = {2},
 year = {2014}
}

@inproceedings{Song1999GeneralLM,
 address = {New York, NY, USA},
 author = {Song, Fei and Croft, W. Bruce},
 booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
 doi = {10.1145/319950.320022},
 file = {:PDF/319950.320022.pdf:PDF},
 groups = {lm},
 isbn = {1581131461},
 keywords = {statistical language modeling, model combinations, good-turing estimate, curve-fitting functions},
 location = {Kansas City, Missouri, USA},
 numpages = {6},
 pages = {316–321},
 publisher = {Association for Computing Machinery},
 series = {CIKM '99},
 title = {A general language model for information retrieval},
 year = {1999}
}

@inproceedings{Song2016Multimodal,
 author = {Song, Mingoo and Yoo, Chang D.},
 booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
 doi = {10.1109/ICIP.2016.7532765},
 issn = {2381-8549},
 keywords = {Data models;Smoothing methods;Context;Context modeling;Predictive models;Mathematical model;Numerical models;multimodal representation;neural language model;Kneser-Ney smoothing;skip-gram;data sparsity},
 pages = {2281-2285},
 title = {Multimodal representation: Kneser-ney smoothing/skip-gram based neural language model},
 year = {2016}
}

@inproceedings{Song2019Polysemous,
 author = {Song, Yale and Soleymani, Mohammad},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf:PDF},
 groups = {embeddings},
 title = {Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval},
 year = {2019}
}

@misc{Song2021Exploring,
 archiveprefix = {arXiv},
 author = {Zeliang Song and Xiaofei Zhou},
 eprint = {2105.02391},
 file = {:PDF/EXPLORING EXPLICIT AND IMPLICIT VISUAL RELATIONSHIPS FOR IMAGE CAPTIONING.pdf:PDF},
 groups = {relations},
 primaryclass = {cs.CV},
 title = {Exploring Explicit and Implicit Visual Relationships for Image Captioning},
 year = {2021}
}

@article{Song2023,
 author = {Song, Binyang and Zhou, Rui and Ahmed, Faez},
 doi = {10.1115/1.4063954},
 eprint = {https://asmedigitalcollection.asme.org/computingengineering/article-pdf/24/1/010801/7062966/jcise\_24\_1\_010801.pdf},
 file = {:PDF/2302.10909v2.pdf:PDF},
 group = {review},
 issn = {1530-9827},
 journal = {Journal of Computing and Information Science in Engineering},
 number = {1},
 pages = {010801},
 title = {Multi-Modal Machine Learning in Engineering Design: A Review and Future Directions},
 volume = {24},
 year = {2023}
}

@inproceedings{Sow2019SequentialGuiding,
 author = {Sow, Daouda and Qin, Zengchang and Niasse, Mouhamed and Wan, Tao},
 booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 doi = {10.1109/ICASSP.2019.8682505},
 file = {:PDF/A_Sequential_Guiding_Network_with_Attention_for_Image_Captioning.pdf:PDF},
 keywords = {Semantics;Decoding;Task analysis;Adaptation models;Computational modeling;Visualization;Recurrent neural networks},
 pages = {3802-3806},
 title = {A Sequential Guiding Network with Attention for Image Captioning},
 year = {2019}
}

@inbook{Sparck1988TFIDF,
 address = {GBR},
 author = {Sparck Jones, Karen},
 booktitle = {Document Retrieval Systems},
 file = {:PDF/lecture1-firth.pdf:PDF},
 isbn = {0947568212},
 numpages = {11},
 pages = {132–142},
 publisher = {Taylor Graham Publishing},
 title = {A statistical interpretation of term specificity and its application in retrieval},
 year = {1988}
}

@article{Spratling2004Feedback,
 author = {Spratling, Mike W and Johnson, Mark H},
 file = {:PDF/A feedback model of visual attention.pdf:PDF},
 journal = {Journal of cognitive neuroscience},
 number = {2},
 pages = {219--237},
 publisher = {MIT Press},
 title = {A feedback model of visual attention},
 url = {https://core.ac.uk/reader/29880237?utm_source=linkout},
 volume = {16},
 year = {2004}
}

@inproceedings{Srivastava2018ASO,
 author = {Gargi Srivastava and Rajeev Srivastava},
 booktitle = {International Conference on Mathematics and Computing},
 file = {:PDF/.1273496.1273592.pdf.~771c99d8:~771c99d8},
 groups = {review},
 title = {A Survey on Automatic Image Captioning},
 year = {2018}
}

@article{staniute2019Systematic,
 article-number = {2024},
 author = {Staniūtė, Raimonda and Šešok, Dmitrij},
 doi = {10.3390/app9102024},
 file = {:PDF/A Systematic Literature Review on Image Captioning .pdf:PDF},
 groups = {review},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {10},
 title = {A Systematic Literature Review on Image Captioning},
 url = {https://www.mdpi.com/2076-3417/9/10/2024},
 volume = {9},
 year = {2019}
}

@misc{Stefanini2021Show,
      title={From Show to Tell: A Survey on Deep Learning-based Image Captioning},
      author={Matteo Stefanini and Marcella Cornia and Lorenzo Baraldi and Silvia Cascianelli and Giuseppe Fiameni and Rita Cucchiara},
      year={2021},
      eprint={2107.06912},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{Stewart2015End,
 author = {Stewart, Russell and Andriluka, Mykhaylo and Ng, Andrew Y.},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.255},
 file = {:PDF/End-to-End_People_Detection_in_Crowded_Scenes.pdf:PDF},
 pages = {2325-2333},
 title = {End-to-End People Detection in Crowded Scenes},
 year = {2016}
}

@article{Su202125DVR,
 author = {Yu{-}Chuan Su and Soravit Changpinyo and Xiangning Chen and Sathish Thoppay and Cho{-}Jui Hsieh and Lior Shapira and Radu Soricut and Hartwig Adam and Matthew Brown and Ming{-}Hsuan Yang and Boqing Gong},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2104-12727.bib},
 eprint = {2104.12727},
 eprinttype = {arXiv},
 file = {:PDF/2.5D Visual Relationship Detection.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 03 May 2021 17:38:30 +0200},
 title = {2.5D Visual Relationship Detection},
 url = {https://arxiv.org/abs/2104.12727},
 volume = {abs/2104.12727},
 year = {2021}
}

@article{Subarna2021,
 author = {Subarna Tripathi and Kien Nguyen and Tanaya Guha and Bang Du and Truong Q. Nguyen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2102-04990.bib},
 comment = {https://github.com/Kien085/SG2Caps},
 eprint = {2102.04990},
 eprinttype = {arXiv},
 file = {:PDF/In Defense of Scene Graphs for Image Captioning .pdf:PDF},
 journal = {CoRR},
 timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
 title = {SG2Caps: Revisiting Scene Graphs for Image Captioning},
 url = {https://arxiv.org/abs/2102.04990},
 volume = {abs/2102.04990},
 year = {2021}
}

@article{Subash_2019,
 author = {R. Subash and R. Jebakumar and Yash Kamdar and Nishit Bhatt},
 doi = {10.1088/1742-6596/1362/1/012096},
 file = {:PDF/Subash_2019_J._Phys.__Conf._Ser._1362_012096.pdf:PDF},
 groups = {vgg16},
 journal = {Journal of Physics: Conference Series},
 number = {1},
 pages = {012096},
 publisher = {IOP Publishing},
 title = {Automatic Image Captioning Using Convolution Neural Networks and LSTM},
 url = {https://dx.doi.org/10.1088/1742-6596/1362/1/012096},
 volume = {1362},
 year = {2019}
}

@inproceedings{Sulaimi2021,
 author = {Sulaimi, Mousa Al and Ahmad, Imtiaz and Jeragh, Mohammad},
 booktitle = {2021 29th Conference of Open Innovations Association (FRUCT)},
 doi = {10.23919/FRUCT52173.2021.9435534},
 file = {:PDF/Deep_Image_Captioning_Survey_A_Resource_Availability_Perspective.pdf:PDF},
 pages = {3-13},
 title = {Deep Image Captioning Survey: A Resource Availability Perspective},
 year = {2021}
}

@article{Sundaramoorthy2021End,
 author = {Carola Sundaramoorthy and Lin Ziwen Kelvin and Mahak Sarin and Shubham Gupta},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2104-14721.bib},
 eprint = {2104.14721},
 eprinttype = {arXiv},
 file = {:PDF/End-to-End_Attention-based_Image_Captioning.pdf:PDF},
 journal = {CoRR},
 timestamp = {Tue, 04 May 2021 15:12:43 +0200},
 title = {End-to-End Attention-based Image Captioning},
 url = {https://arxiv.org/abs/2104.14721},
 volume = {abs/2104.14721},
 year = {2021}
}

@inproceedings{sundermeyer12_interspeech,
 author = {Martin Sundermeyer and Ralf Schlüter and Hermann Ney},
 booktitle = {Proc. Interspeech 2012},
 doi = {10.21437/Interspeech.2012-65},
 file = {:PDF/sundermeyer12_interspeech.pdf:PDF},
 pages = {194--197},
 title = {{LSTM neural networks for language modeling}},
 year = {2012}
}

@inproceedings{sung2022vladapter,
 author = {Yi-Lin Sung, Jaemin Cho, Mohit Bansal},
 booktitle = {CVPR},
 comment = {https://github.com/ylsung/VL_adapter},
 file = {:PDF/Welcome to Nextcloud Hub.pdf:PDF},
 title = {VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
 year = {2022}
}

@inproceedings{Sutskever2014Seq2Seq,
 address = {Cambridge, MA, USA},
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
 file = {:PDF/NIPS-2014-sequence-to-sequence-learning-with-neural-networks-Paper.pdf:PDF},
 groups = {lm},
 location = {Montreal, Canada},
 numpages = {9},
 pages = {3104–3112},
 publisher = {MIT Press},
 series = {NIPS'14},
 title = {Sequence to sequence learning with neural networks},
 year = {2014}
}

@article{Sutton2012CRF,
 address = {Hanover, MA, USA},
 author = {Sutton, Charles and McCallum, Andrew},
 doi = {10.1561/2200000013},
 file = {:PDF/1011.4088v1.pdf:PDF},
 issn = {1935-8237},
 issue_date = {April 2012},
 journal = {Found. Trends Mach. Learn.},
 number = {4},
 numpages = {107},
 pages = {267–373},
 publisher = {Now Publishers Inc.},
 title = {An Introduction to Conditional Random Fields},
 volume = {4},
 year = {2012}
}

@article{sym13071184,
 article-number = {1184},
 author = {Tian, Peng and Mo, Hongwei and Jiang, Laihao},
 doi = {10.3390/sym13071184},
 file = {:PDF/symmetry-13-01184-v2.pdf:PDF},
 issn = {2073-8994},
 journal = {Symmetry},
 number = {7},
 title = {Image Caption Generation Using Multi-Level Semantic Context Information},
 url = {https://www.mdpi.com/2073-8994/13/7/1184},
 volume = {13},
 year = {2021}
}

@inproceedings{Szegedy2014Going,
 address = {Los Alamitos, CA, USA},
 author = {C. Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7298594},
 file = {:PDF/Going_deeper_with_convolutions.pdf:PDF},
 issn = {1063-6919},
 pages = {1-9},
 publisher = {IEEE Computer Society},
 title = {Going deeper with convolutions},
 year = {2015}
}

@inproceedings{Szegedy2016RethinkingTI,
 author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.308},
 file = {:PDF/Rethinking the Inception Architecture for Computer Vision.pdf:PDF},
 issn = {1063-6919},
 keywords = {Convolution;Computer architecture;Training;Computational efficiency;Computer vision;Benchmark testing;Computational modeling},
 pages = {2818-2826},
 title = {Rethinking the Inception Architecture for Computer Vision},
 year = {2016}
}

@inproceedings{Szegedy2017InceptionV4,
 author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
 booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
 file = {:PDF/11231-Article Text-14759-1-2-20201228.pdf:PDF},
 location = {San Francisco, California, USA},
 numpages = {7},
 pages = {4278–4284},
 publisher = {AAAI Press},
 series = {AAAI'17},
 title = {Inception-v4, inception-ResNet and the impact of residual connections on learning},
 year = {2017}
}

@misc{Tadeusiewicz2015Leksykon,
 address = {Wrocław},
 author = {Tadeusiewicz, Ryszard and Sokołowski Ośrodek Kultury},
 booktitle = {Leksykon sieci neuronowych},
 file = {:PDF/Leksykon_sieci_neuronowych-1.pdf:PDF},
 isbn = {9788363270100},
 keywords = {Sieci neuronowe (informatyka)},
 language = {pol},
 publisher = {Wydawnictwo Fundacji "Projekt Nauka"},
 title = {Leksykon sieci neuronowych},
 year = {2015}
}

@inproceedings{Tan2014Grounding,
 author = {J. {Tan} and Z. {Ju} and H. {Liu}},
 booktitle = {2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
 doi = {10.1109/FUZZ-IEEE.2014.6891797},
 file = {:PDF/Grounding_spatial_relations_Tan_2014.pdf:PDF},
 pages = {1743-1750},
 title = {Grounding spatial relations in natural language by fuzzy representation for human-robot interaction},
 year = {2014}
}

@inproceedings{Tanbansal2019lxmert,
 address = {Hong Kong, China},
 author = {Tan, Hao and Bansal, Mohit},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1514},
 editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
 file = {:PDF/D19-1514.pdf:PDF},
 pages = {5100--5111},
 publisher = {Association for Computational Linguistics},
 title = {{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers},
 url = {https://aclanthology.org/D19-1514},
 year = {2019}
}

@article{Tang2021AttentionGuided,
 article-number = {7982},
 author = {Tang, Ziwei and Yi, Yaohua and Sheng, Hao},
 doi = {10.3390/s21237982},
 file = {:PDF/sensors-21-07982-1.pdf:PDF},
 issn = {1424-8220},
 journal = {Sensors},
 number = {23},
 pubmedid = {34883986},
 title = {Attention-Guided Image Captioning through Word Information},
 url = {https://www.mdpi.com/1424-8220/21/23/7982},
 volume = {21},
 year = {2021}
}

@inproceedings{Tanti2017Role,
 address = {Santiago de Compostela, Spain},
 author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth},
 booktitle = {Proceedings of the 10th International Conference on Natural Language Generation},
 doi = {10.18653/v1/W17-3506},
 editor = {Alonso, Jose M. and Bugar{\'i}n, Alberto and Reiter, Ehud},
 file = {:PDF/W17-3506.pdf:PDF},
 pages = {51--60},
 publisher = {Association for Computational Linguistics},
 title = {What is the Role of Recurrent Neural Networks ({RNN}s) in an Image Caption Generator?},
 year = {2017}
}

@article{TantiGattCamilleri2018Where,
 author = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth P.},
 doi = {10.1017/S1351324918000098},
 file = {:PDF/Where to put the Image in an Image Caption Generator.pdf:PDF;:PDF/1703.09137v2.pdf:PDF},
 journal = {Natural Language Engineering},
 number = {3},
 pages = {467–489},
 publisher = {Cambridge University Press},
 title = {Where to put the image in an image caption generator},
 volume = {24},
 year = {2018}
}

@Misc{Tavalkoyi2017PayingAttention,
  author        = {Hamed R. Tavakoli and Rakshith Shetty and Ali Borji and Jorma Laaksonen},
  title         = {Paying Attention to Descriptions Generated by Image Captioning Models},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1704.07434},
  file          = {:PDF/1704.07434v3.pdf:PDF},
  pages         = {1-10},
  primaryclass  = {cs.CV},
}

@article{Terven2023Yolo,
 author = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
 doi = {10.3390/make5040083},
 file = {:PDF/make-05-00083-v2.pdf:PDF},
 issn = {2504-4990},
 journal = {Machine Learning and Knowledge Extraction},
 number = {4},
 pages = {1680--1716},
 title = {A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS},
 volume = {5},
 year = {2023}
}

@inproceedings{Tian2019Image,
 author = {Tian, Junjiao and Oh, Jean},
 booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
 doi = {10.24963/ijcai.2019/496},
 file = {:PDF/Image_Captioning_with_Compositional_Neural_Module_.pdf:PDF},
 pages = {3576--3584},
 publisher = {International Joint Conferences on Artificial Intelligence Organization},
 title = {Image Captioning with Compositional Neural Module Networks},
 year = {2019}
}

@InProceedings{Torralba2003Vi,
  author    = {Torralba, Antonio and Murphy, Kevin P. and Freeman, William T. and Rubin, Mark A.},
  booktitle = {Proceedings of the Ninth IEEE International Conference on Computer Vision - Volume 2},
  title     = {Context-based vision system for place and object recognition},
  year      = {2003},
  address   = {USA},
  pages     = {273},
  publisher = {IEEE Computer Society},
  series    = {ICCV '03},
  abstract  = {While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. In this paper we present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, Main Street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides real-time feedback to the user.},
  file      = {:PDF/Context-based_vision_system_for_place_and_object_recognition.pdf:PDF},
  isbn      = {0769519504},
}

@article{Torralba80Milion,
 author = {Torralba, Antonio and Fergus, Rob and Freeman, William T.},
 doi = {10.1109/TPAMI.2008.128},
 file = {:PDF/80_Million_Tiny_Images_A_Large_Data_Set_for_Nonparametric_Object_and_Scene_Recognition.pdf:PDF},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Layout;Image recognition;Internet;Image databases;Image sampling;Psychology;Humans;Visual system;Degradation;Image resolution;Computer vision;Object recognition;large datasets;nearest-neighbor methods;Computer vision;Object recognition;large datasets;nearest-neighbor methods},
 number = {11},
 pages = {1958-1970},
 title = {80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition},
 volume = {30},
 year = {2008}
}

@inproceedings{Touvron2021Training,
 author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
 booktitle = {Proceedings of the 38th International Conference on Machine Learning},
 editor = {Meila, Marina and Zhang, Tong},
 file = {:PDF/touvron21a.pdf:PDF},
 groups = {Vision Transformer.},
 pages = {10347--10357},
 pdf = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Training data-efficient image transformers distillation through attention},
 url = {https://proceedings.mlr.press/v139/touvron21a.html},
 volume = {139},
 year = {2021}
}

@inproceedings{Tran2016,
 author = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 doi = {10.1109/CVPRW.2016.61},
 file = {:PDF/Tran_Rich_Image_Captioning_CVPR_2016_paper.pdf:PDF},
 groups = {Compositional architectures},
 issn = {2160-7516},
 keywords = {Training;Visualization;Face;Knowledge based systems;Semantics;Data models;Neural networks},
 pages = {434-441},
 title = {Rich Image Captioning in the Wild},
 year = {2016}
}

@article{Trisana2024FusionTextRepresentation,
 article-number = {10420},
 author = {Trisna, Komang Wahyu and Huang, Jinjie and Liang, Hengyu and Dharma, Eddy Muntina},
 doi = {10.3390/app142210420},
 file = {:PDF/applsci-14-10420.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {22},
 title = {Fusion Text Representations to Enhance Contextual Meaning in Sentiment Classification},
 url = {https://www.mdpi.com/2076-3417/14/22/10420},
 volume = {14},
 year = {2024}
}

@article{Uddagiri2022,
 author = {Uddagiri Sirisha and Bolem Sai Chandana},
 doi = {10.1080/23311916.2022.2104333},
 eprint = {https://doi.org/10.1080/23311916.2022.2104333},
 file = {:PDF/Semantic interdisciplinary evaluation of image captioning models.pdf:PDF},
 groups = {review},
 journal = {Cogent Engineering},
 number = {1},
 pages = {2104333},
 publisher = {Cogent OA},
 ranking = {rank5},
 title = {Semantic interdisciplinary evaluation of image captioning models},
 url = {https://doi.org/10.1080/23311916.2022.2104333},
 volume = {9},
 year = {2022}
}

@article{Uddin2018Proposing,
 article-number = {646},
 author = {Uddin, Muhammad Fahim and Lee, Jeongkyu and Rizvi, Syed and Hamada, Samir},
 doi = {10.3390/app8040646},
 file = {:PDF/applsci-08-00646.pdf:PDF},
 groups = {representation learning},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {4},
 title = {Proposing Enhanced Feature Engineering and a Selection Model for Machine Learning Processes},
 url = {https://www.mdpi.com/2076-3417/8/4/646},
 volume = {8},
 year = {2018}
}

@article{Urzędowska2024PoprawnoscJezykowa,
 abstractnote = {Tekst podejmuje próbę oceny poprawności językowej algorytmów lingwistycznych czatu GPT-4. Przeprowadzone i opisane eksperymenty odnoszą się do opisu funkcjonalności czatu jako wysoko inteligentnej pod względem języka w perspektywie poprawności, sprawności i kultury językowej. Przez odniesienie do gardnerowskiego rozumienia inteligencji językowej oraz jego krytyków opisano umiejętności korzystania ze znormalizowanego języka przez sztuczną inteligencję, a także oznaczono błędy w danych i procesie kodowania modeli językowych w czacie GPT-4.},
 author = {Urzędowska, Aleksandra},
 doi = {10.24917/20837275.16.2.5},
 file = {:PDF/3442188.3445922.pdf:PDF},
 groups = {bledy w SI},
 journal = {Annales Universitatis Paedagogicae Cracoviensis | Studia de Cultura},
 number = {2},
 pages = {71–84},
 title = {Sztuczna inteligencja a inteligencja językowa. Eksperyment poprawnościowy czatu GPT-4},
 url = {https://studiadecultura.uken.krakow.pl/article/view/11305},
 volume = {16},
 year = {2024}
}

@inproceedings{Ushiku2012Efficient,
 address = {New York, NY, USA},
 author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
 booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
 doi = {10.1145/2393347.2393424},
 file = {:PDF/Efficient Image Annotation for Automatic Sentence Generation.pdf:PDF},
 isbn = {9781450310895},
 keywords = {online learning, passive-aggressive, multi-stack decoding},
 location = {Nara, Japan},
 numpages = {10},
 pages = {549–558},
 publisher = {Association for Computing Machinery},
 series = {MM '12},
 title = {Efficient Image Annotation for Automatic Sentence Generation},
 year = {2012}
}

@inproceedings{Ushiku2015Common,
 author = {Ushiku, Yoshitaka and Yamaguchi, Masataka and Mukuta, Yusuke and Harada, Tatsuya},
 booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2015.306},
 file = {:PDF/Common_Subspace_for_Model_and_Similarity_Phrase_Learning_for_Caption_Generation_from_Images.pdf:PDF},
 groups = {retrieval},
 issn = {2380-7504},
 keywords = {Training;Visualization;Learning systems;Neural networks;Grammar;Scalability;Feature extraction},
 pages = {2668-2676},
 title = {Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images},
 year = {2015}
}

@inproceedings{van-der-lee-etal-2019-best,
 address = {Tokyo, Japan},
 author = {van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel},
 booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
 doi = {10.18653/v1/W19-8643},
 editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
 file = {:PDF/W19-8643.pdf:PDF},
 pages = {355--368},
 publisher = {Association for Computational Linguistics},
 title = {Best practices for the human evaluation of automatically generated text},
 url = {https://aclanthology.org/W19-8643},
 year = {2019}
}

@inproceedings{Vaswani2017AttentionIA,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 comment = {http://nlp.seas.harvard.edu/2018/04/03/attention.html},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 file = {:PDF/attention is all you need.pdf:PDF},
 groups = {Self-Attention Encoding, attention taxonomy},
 printed = {yes},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{Vedantam2015Cider,
 address = {Los Alamitos, CA, USA},
 author = {R. Vedantam and C. Zitnick and D. Parikh},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7299087},
 file = {:PDF/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf:PDF},
 groups = {metryki},
 issn = {1063-6919},
 pages = {4566-4575},
 publisher = {IEEE Computer Society},
 title = {CIDEr: Consensus-based image description evaluation},
 year = {2015}
}

@article{Venugopalan2017CaptioningIW,
 author = {Subhashini Venugopalan and Lisa Anne Hendricks and Marcus Rohrbach and Raymond J. Mooney and Trevor Darrell and Kate Saenko},
 file = {:PDF/Captioning Images with Diverse Objects.pdf:PDF},
 journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {1170-1178},
 title = {Captioning Images with Diverse Objects},
 year = {2017}
}

@inproceedings{Verma2014ImTxtandText2Im,
 author = {Verma, Yashaswi and Jawahar, C. V.},
 booktitle = {Proceedings of the British Machine Vision Conference},
 doi = {10.5244/C.28.97},
 editors = {Valstar, Michel and French, Andrew and Pridmore, Tony},
 file = {:PDF/paper089.pdf:PDF},
 groups = {retrieval},
 publisher = {BMVA Press},
 title = {Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval},
 year = {2014}
}

@article{Villanueva2019mapping,
 author = {Villanueva Jr, Jaime M and Subramanian, Anantharam and Ahir, Vishal and Pollock, Andrew},
 file = {:PDF/Mapping Relationships and Positions of Objects in Images.pdf:PDF},
 journal = {SMU Data Science Review},
 number = {3},
 pages = {11},
 title = {Mapping Relationships and Positions of Objects in Images Using Mask and Bounding Box Data},
 url = {https://scholar.smu.edu/cgi/viewcontent.cgi?article=1122&context=datasciencereview},
 volume = {2},
 year = {2019}
}

@inproceedings{Vinyals2015PointerNetworks,
 author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 file = {:PDF/NIPS-2015-pointer-networks-Paper.pdf:PDF},
 publisher = {Curran Associates, Inc.},
 title = {Pointer Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf},
 volume = {28},
 year = {2015}
}

@InProceedings{Vinyals2015Show,
  author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Show and tell: A neural image caption generator},
  year      = {2015},
  pages     = {3156-3164},
  abstract  = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  doi       = {10.1109/CVPR.2015.7298935},
  file      = {:PDF/Show_and_tell_A_neural_image_caption_generator.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Logic gates;Measurement;Training;Visualization;Recurrent neural networks;Google},
}

@article{Vinyals2017ShowTellLessonsLearned,
 address = {Los Alamitos, CA, USA},
 author = {O. Vinyals and A. Toshev and S. Bengio and D. Erhan},
 doi = {10.1109/TPAMI.2016.2587640},
 file = {:PDF/07505636.pdf:PDF},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis Machine Intelligence},
 keywords = {logic gates;training;recurrent neural networks;visualization;computer vision;computational modeling;natural languages},
 number = {04},
 pages = {652-663},
 publisher = {IEEE Computer Society},
 title = {Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},
 volume = {39},
 year = {2017}
}

@inproceedings{Viola2001Haar,
 author = {Viola, P. and Jones, M.},
 booktitle = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
 doi = {10.1109/ICCV.2001.937709},
 file = {:PDF/Robust_real-time_face_detection.pdf:PDF},
 keywords = {Robustness;Face detection;Pixel;Detectors;Boosting;Object detection;Laboratories;Video sequences;Color;Information resources},
 pages = {747-747},
 title = {Robust real-time face detection},
 volume = {2},
 year = {2001}
}

@inproceedings{Wang2008Study,
 address = {Berlin, Heidelberg},
 author = {Wang, Xin and Matsakis, Pascal and Trick, Lana and Nonnecke, Blair and Veltman, Melanie},
 booktitle = {Headway in Spatial Data Handling},
 editor = {Ruas, Anne and Gold, Christopher},
 file = {:PDF/Wang2008_Chapter_AStudyOnHowHumansDescribeRelat.pdf:PDF},
 isbn = {978-3-540-68566-1},
 pages = {1--18},
 publisher = {Springer Berlin Heidelberg},
 title = {A Study on how Humans Describe Relative Positions of Image Objects},
 url = {https://www.researchgate.net/publication/220885029_A_Study_on_how_Humans_Describe_Relative_Positions_of_Image_Objects},
 year = {2008}
}

@inproceedings{Wang2016,
 author = {Wang, Minsi and Song, Li and Yang, Xiaokang and Luo, Chuanfei},
 booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
 doi = {10.1109/ICIP.2016.7533201},
 file = {:PDF/A_parallel-fusion_RNN-LSTM_architecture_for_image_caption_generation.pdf:PDF},
 groups = {Compositional architectures},
 issn = {2381-8549},
 keywords = {Training;Computational modeling;Feature extraction;Measurement;Data models;Recurrent neural networks;Image captioning;deep neural network;RNN;LSTM},
 pages = {4448-4452},
 title = {A parallel-fusion RNN-LSTM architecture for image caption generation},
 year = {2016}
}

@InProceedings{Wang2017Skeleton,
  author        = {Yufei Wang and Zhe Lin and Xiaohui Shen and Scott Cohen and Garrison W. Cottrell},
  title         = {Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition},
  year          = {2017},
  pages         = {1-10},
  archiveprefix = {arXiv},
  eprint        = {1704.06972},
  file          = {:PDF/1704.06972v1.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1704.06972},
}

@article{Wang2018,
 address = {New York, NY, USA},
 articleno = {40},
 author = {Wang, Cheng and Yang, Haojin and Meinel, Christoph},
 doi = {10.1145/3115432},
 file = {:PDF/3115432.pdf:PDF},
 groups = {bez atencji},
 issn = {1551-6857},
 issue_date = {April 2018},
 journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
 keywords = {Deep learning, LSTM, image captioning, multimodal representations, mutli-task learning},
 number = {2s},
 numpages = {20},
 publisher = {Association for Computing Machinery},
 title = {Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning},
 volume = {14},
 year = {2018}
}

@article{Wang2019Hierarchical,
 abstractnote = {Recently, attention mechanism has been successfully applied in image captioning, but the existing attention methods are only established on low-level spatial features or high-level text features, which limits richness of captions. In this paper, we propose a Hierarchical Attention Network (HAN) that enables attention to be calculated on pyramidal hierarchy of features synchronously. The pyramidal hierarchy consists of features on diverse semantic levels, which allows predicting different words according to different features. On the other hand, due to the different modalities of features, a Multivariate Residual Module (MRM) is proposed to learn the joint representations from features. The MRM is able to model projections and extract relevant relations among different features. Furthermore, we introduce a context gate to balance the contribution of different features. Compared with the existing methods, our approach applies hierarchical features and exploits several multimodal integration strategies, which can significantly improve the performance. The HAN is verified on benchmark MSCOCO dataset, and the experimental results indicate that our model outperforms the state-of-the-art methods, achieving a BLEU1 score of 80.9 and a CIDEr score of 121.7 in the Karpathy’s test split.},
 author = {Wang, Weixuan and Chen, Zhihong and Hu, Haifeng},
 doi = {10.1609/aaai.v33i01.33018957},
 file = {:PDF/4924-Article Text-7990-1-10-20190709-1.pdf:PDF},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {01},
 pages = {8957-8964},
 title = {Hierarchical Attention Network for Image Captioning},
 volume = {33},
 year = {2019}
}

@article{Wang2019Survey,
 author = {Yiyu Wang and Jungang Xu and Yingfei Sun and Ben He},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1905-08110.bib},
 eprint = {1905.08110},
 eprinttype = {arXiv},
 file = {:PDF/1905.08110v1.pdf:PDF},
 groups = {review},
 journal = {CoRR},
 timestamp = {Tue, 28 May 2019 12:48:08 +0200},
 title = {Image Captioning based on Deep Learning Methods: {A} Survey},
 url = {http://arxiv.org/abs/1905.08110},
 volume = {abs/1905.08110},
 year = {2019}
}

@article{WANG2020107075,
 author = {Junbo Wang and Wei Wang and Liang Wang and Zhiyong Wang and David Dagan Feng and Tieniu Tan},
 doi = {10.1016/j.patcog.2019.107075},
 file = {:PDF/1-s2.0-S0031320319303760-main.pdf:PDF},
 issn = {0031-3203},
 journal = {Pattern Recognition},
 keywords = {Image captioning, Relational reasoning, Context-aware attention},
 pages = {107075},
 title = {Learning visual relationship and context-aware attention for image captioning},
 url = {https://www.sciencedirect.com/science/article/pii/S0031320319303760},
 volume = {98},
 year = {2020}
}

@article{Wang2020AnOO,
 author = {Haoran Wang and Yanjing Zhang and Xiaosheng Yu},
 doi = {10.1155/2020/3062706},
 file = {:PDF/An Overview of Image Caption Generation Methods.pdf:PDF},
 groups = {review},
 journal = {Computational Intelligence and Neuroscience},
 title = {An Overview of Image Caption Generation Methods},
 volume = {2020},
 year = {2020}
}

@article{Wang2020ShowRecall,
 abstractnote = {Generating natural and accurate descriptions in image captioning has always been a challenge. In this paper, we propose a novel recall mechanism to imitate the way human conduct captioning. There are three parts in our recall mechanism : recall unit, semantic guide (SG) and recalled-word slot (RWS). Recall unit is a text-retrieval module designed to retrieve recalled words for images. SG and RWS are designed for the best use of recalled words. SG branch can generate a recalled context, which can guide the process of generating caption. RWS branch is responsible for copying recalled words to the caption. Inspired by pointing mechanism in text summarization, we adopt a soft switch to balance the generated-word probabilities between SG and RWS. In the CIDEr optimization step, we also introduce an individual recalled-word reward (WR) to boost training. Our proposed methods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICE scores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 / 129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathy test split, which surpass the results of other state-of-the-art methods.},
 author = {Wang, Li and Bai, Zechen and Zhang, Yonghua and Lu, Hongtao},
 doi = {10.1609/aaai.v34i07.6898},
 file = {:PDF/6898-Article Text-10127-1-10-20200525.pdf:PDF},
 groups = {Attention Over Visual Regions, Two-layer LSTM},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {07},
 pages = {12176-12183},
 title = {Show, Recall, and Tell: Image Captioning with Recall Mechanism},
 volume = {34},
 year = {2020}
}

@inproceedings{Wang2021DynamiAttention,
 author = {Wang, Changzhi and Gu, Xiaodong},
 booktitle = {2021 International Joint Conference on Neural Networks (IJCNN)},
 doi = {10.1109/IJCNN52387.2021.9533994},
 file = {:PDF/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
 keywords = {Visualization;Fuses;Computational modeling;Neural networks;Interference;Task analysis;Computational complexity;Image captioning;Attention mechanism;LSTM;CNN},
 pages = {1-8},
 title = {An Image Captioning Approach Using Dynamical Attention},
 year = {2021}
}

@article{Wang2021Integrative,
 author = {Wang, Chaoyang and Zhou, Ziwei and Xu, Liang},
 doi = {10.1088/1742-6596/1748/4/042060},
 file = {:PDF/An_Integrative_Review_of_Image_Captioning_Research.pdf:PDF},
 journal = {Journal of Physics: Conference Series},
 pages = {042060},
 title = {An Integrative Review of Image Captioning Research},
 volume = {1748},
 year = {2021}
}

@article{Wang2022AdaptiveIncremental,
 address = {USA},
 author = {Wang, Changzhi and Gu, Xiaodong},
 doi = {10.1007/s10489-021-02734-3},
 file = {:PDF/s10489-021-02734-3.pdf:PDF},
 issn = {0924-669X},
 issue_date = {Apr 2022},
 journal = {Applied Intelligence},
 keywords = {Image captioning, Attention mechanism, Incremental global context attention, LSTM},
 number = {6},
 numpages = {23},
 pages = {6575–6597},
 publisher = {Kluwer Academic Publishers},
 title = {Image captioning with adaptive incremental global context attention},
 url = {https://doi.org/10.1007/s10489-021-02734-3},
 volume = {52},
 year = {2022}
}

@article{Wang2022Discriminative,
 author = {Wang, Changshuo and Ning, Xin and Sun, Linjun and Zhang, Liping and Li, Weijun and Bai, Xiao},
 doi = {10.1109/TGRS.2022.3170493},
 file = {:PDF/2022-Learning_Discriminative_Features_by_Covering_Local_Geometric_Space_for_Point_Cloud_Analysis.pdf:PDF},
 journal = {IEEE Transactions on Geoscience and Remote Sensing},
 keywords = {Point cloud compression;Feature extraction;Three-dimensional displays;Convolution;Shape;Geometry;Task analysis;Edge feature;high-order relationship;point cloud analysis;shape classification;space-cover convolutional neural network (SC-CNN)},
 pages = {1-15},
 title = {Learning Discriminative Features by Covering Local Geometric Space for Point Cloud Analysis},
 volume = {60},
 year = {2022}
}

@article{Wang2022Diversity,
 author = {Wang, Qingzhong and Wan, Jia and Chan, Antoni B.},
 doi = {10.1109/TPAMI.2020.3013834},
 file = {:PDF/On_Diversity_in_Image_Captioning_Metrics_and_Methods.pdf:PDF},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 number = {2},
 pages = {1035-1049},
 title = {On Diversity in Image Captioning: Metrics and Methods},
 volume = {44},
 year = {2022}
}

@article{Wang2022endtoendtransformerbasedmodel,
author = {Wang, Yiyu and Xu, Jungang and Sun, Yingfei},
year = {2022},
month = {06},
pages = {2585-2594},
title = {End-to-End Transformer Based Model for Image Captioning},
volume = {36},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v36i3.20160}
}

@article{Wang2022TSFeatureEngineering,
 article-number = {17},
 author = {Wang, Can and Baratchi, Mitra and Bäck, Thomas and Hoos, Holger H. and Limmer, Steffen and Olhofer, Markus},
 doi = {10.3390/engproc2022018017},
 file = {:Fuzzy.bib (conflicted copy 2021-09-15 142929).sav:sav;:PDF/engproc-18-00017.pdf:PDF},
 groups = {representation learning},
 issn = {2673-4591},
 journal = {Engineering Proceedings},
 number = {1},
 title = {Towards Time-Series Feature Engineering in Automated Machine Learning for Multi-Step-Ahead Forecasting},
 url = {https://www.mdpi.com/2673-4591/18/1/17},
 volume = {18},
 year = {2022}
}

@misc{Wang2023,
 author = {Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
 copyright = {arXiv.org perpetual, non-exclusive license},
 doi = {10.48550/ARXIV.2302.10035},
 file = {:PDF/large-scale.pdf:PDF},
 groups = {review},
 keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey},
 url = {https://arxiv.org/abs/2302.10035},
 year = {2023}
}

@article{Wang2024PersonReIdentification,
 author = {Wang, Changshuo and Ning, Xin and Li, Weijun and Bai, Xiao and Gao, Xingyu},
 doi = {10.1109/TCSVT.2023.3328712},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 keywords = {Pedestrians;Three-dimensional displays;Point cloud compression;Feature extraction;Shape;Semantics;Geometry;Point cloud;3D shape representation;person re-identification;semantic guidance;local feature extraction},
 number = {6},
 pages = {4698-4712},
 title = {3D Person Re-Identification Based on Global Semantic Guidance and Local Feature Aggregation},
 volume = {34},
 year = {2024}
}

@inproceedings{Ward2002Corpus,
 address = {San Francisco, CA, USA},
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Henderson, John and Reeder, Florence},
 booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},
 file = {:PDF/Corpus-based Comprehensive and Diagnostic MT Evaluation- Initial Arabic, Chinese, French, and Spanish Result.pdf:PDF},
 location = {San Diego, California},
 numpages = {6},
 pages = {132–137},
 publisher = {Morgan Kaufmann Publishers Inc.},
 series = {HLT '02},
 title = {Corpus-Based Comprehensive and Diagnostic MT Evaluation: Initial Arabic, Chinese, French, and Spanish Results},
 url = {https://dl.acm.org/doi/pdf/10.5555/1289189.1289272},
 year = {2002}
}

@article{Wei2020,
 author = {Wei Chen and Weiping Wang and Li Liu and Michael S. Lew},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2010-08189.bib},
 eprint = {2010.08189},
 eprinttype = {arXiv},
 file = {:PDF/2010.08189v1.pdf:PDF},
 groups = {review},
 journal = {CoRR},
 timestamp = {Thu, 03 Nov 2022 08:31:05 +0100},
 title = {New Ideas and Trends in Deep Multimodal Content Understanding: {A} Review},
 url = {https://arxiv.org/abs/2010.08189},
 volume = {abs/2010.08189},
 year = {2020}
}

@misc{Werner2020speedingwordmoversdistance,
 archiveprefix = {arXiv},
 author = {Matheus Werner and Eduardo Laber},
 eprint = {1912.00509},
 file = {:PDF/888_paper.pdf:PDF},
 primaryclass = {cs.CL},
 title = {Speeding up Word Mover's Distance and its variants via properties of distances between embeddings},
 url = {https://arxiv.org/abs/1912.00509},
 year = {2020}
}

@inproceedings{Wiegreffepinter2019attention,
 address = {Hong Kong, China},
 author = {Wiegreffe, Sarah and Pinter, Yuval},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1002},
 editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
 file = {:PDF/D19-1002.pdf:PDF},
 pages = {11--20},
 publisher = {Association for Computational Linguistics},
 title = {Attention is not not Explanation},
 url = {https://aclanthology.org/D19-1002},
 year = {2019}
}

@inproceedings{Wijnhoven2010SGD,
 author = {Wijnhoven, R.G.J. and de With, P.H.N.},
 booktitle = {2010 20th International Conference on Pattern Recognition},
 doi = {10.1109/ICPR.2010.112},
 file = {:PDF/Fast_Training_of_Object_Detection_Using_Stochastic_Gradient_Descent.pdf:PDF},
 issn = {1051-4651},
 keywords = {Training;Support vector machines;Object detection;Optimization;Feature extraction;Computer vision;Pattern recognition;SVM;stochastic gradient descent;object recognition;detection;classification;histogram of oriented gradients;HOG},
 pages = {424-427},
 title = {Fast Training of Object Detection Using Stochastic Gradient Descent},
 year = {2010}
}

@article{Williams2004SimpleSG,
 author = {Ronald J. Williams},
 file = {:PDF/SimpleStatisticalGradient-foll.pdf:PDF},
 journal = {Machine Learning},
 pages = {229-256},
 title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
 volume = {8},
 year = {2004}
}

@inproceedings{Wolinski2014Morfeusz,
 address = {Reykjavik, Iceland},
 author = {Woli{\'n}ski, Marcin},
 booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
 file = {:PDF/morfeusz reloaded.pdf:PDF},
 publisher = {European Language Resources Association (ELRA)},
 title = {Morfeusz Reloaded},
 url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/768_Paper.pdf},
 year = {2014}
}

@book{Wolinski2019AutomatycznaAnalizaSkladnikowa,
 address = {Warszawa},
 author = {Woliński, Marcin},
 doi = {10.31338/uw.9788323536147},
 file = {:PDF/Automatyczna_analiza_składnikowa_ję.pdf:PDF},
 isbn = {978-83-235-3614-7},
 publisher = {Uniwersytet Warszawski},
 title = {Automatyczna analiza składnikowa języka polskiego},
 year = {2019}
}

@inproceedings{Wroblewska2018PolishCorpus,
 address = {Miyazaki, Japan},
 author = {Wr{\'o}blewska, Alina},
 booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
 file = {:PDF/polish corpus of annotated descriptions for images.pdf:PDF},
 publisher = {European Language Resources Association (ELRA)},
 title = {{P}olish Corpus of Annotated Descriptions of Images},
 url = {https://www.aclweb.org/anthology/L18-1337},
 year = {2018}
}

@article{Wu2015ImageCW,
 author = {Qi Wu and Chunhua Shen and Anton van den Hengel and Lingqiao Liu and Anthony R. Dick},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/WuSHLD15.bib},
 eprint = {1506.01144},
 eprinttype = {arXiv},
 file = {:PDF/Image Captioning with an Intermediate Attributes Layer.pdf:PDF},
 journal = {CoRR},
 timestamp = {Tue, 19 Mar 2019 13:03:53 +0100},
 title = {Image Captioning with an Intermediate Attributes Layer},
 url = {http://arxiv.org/abs/1506.01144},
 volume = {abs/1506.01144},
 year = {2015}
}

@article{Wu2016GooglesNM,
 author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
 eprint = {1609.08144},
 eprinttype = {arXiv},
 file = {:PDF/Google's Neural Machine Translation System- Bridging the Gap between Human and Machine Translation.pdf:PDF},
 journal = {CoRR},
 timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
 title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
 url = {http://arxiv.org/abs/1609.08144},
 volume = {abs/1609.08144},
 year = {2016}
}

@InProceedings{Wu2016WhatValue,
  author    = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and Van Den Hengel, Anton},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {What Value Do Explicit High Level Concepts Have in Vision to Language Problems?},
  year      = {2016},
  pages     = {205-212},
  doi       = {10.1109/CVPR.2016.29},
  file      = {:PDF/Wu_What_Value_Do_CVPR_2016_paper.pdf:PDF},
  groups    = {bez atencji, encoder-decoder-lit, global CNN features},
  issn      = {1063-6919},
  keywords  = {Visualization;Knowledge discovery;Feature extraction;Semantics;Vocabulary;Computer vision;Training},
}

@article{Wu2017AICAIChallenger,
 author = {Jiahong Wu and He Zheng and Bo Zhao and Yixin Li and Baoming Yan and Rui Liang and Wenjia Wang and Shipei Zhou and Guosen Lin and Yanwei Fu and Yizhou Wang and Yonggang Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1711-06475.bib},
 eprint = {1711.06475},
 eprinttype = {arXiv},
 file = {:PDF/AI Challenger A Large-scale Dataset for Going Deeper in Image Understanding.pdf:PDF},
 journal = {CoRR},
 timestamp = {Fri, 13 Aug 2021 14:56:27 +0200},
 title = {{AI} Challenger : {A} Large-scale Dataset for Going Deeper in Image Understanding},
 url = {http://arxiv.org/abs/1711.06475},
 volume = {abs/1711.06475},
 year = {2017}
}

@article{Wu2018DecoupledNO,
 author = {Yuehua Wu and Linchao Zhu and Lu Jiang and Yi Yang},
 file = {:PDF/Decoupled Novel Object Captioner.pdf:PDF},
 journal = {Proceedings of the 26th ACM international conference on Multimedia},
 title = {Decoupled Novel Object Captioner},
 year = {2018}
}

@article{Wu2018ICandVQA,
 author = {Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and Hengel, Anton van den},
 doi = {10.1109/TPAMI.2017.2708709},
 file = {:PDF/1603.02814v2.pdf:PDF},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 keywords = {Visualization;Knowledge discovery;Semantics;Computational modeling;Computer vision;Knowledge based systems;Resource description framework;Image captioning;visual question answering;concepts learning;recurrent neural networks;LSTM},
 number = {6},
 pages = {1367-1381},
 title = {Image Captioning and Visual Question Answering Based on Attributes and External Knowledge},
 volume = {40},
 year = {2018}
}

@article{Xia2020Boosting,
 address = {USA},
 author = {Xia, Pengfei and He, Jingsong and Yin, Jin},
 doi = {10.1007/s11042-020-09110-2},
 file = {:PDF/s11042-020-09110-2.pdf:PDF},
 issn = {1380-7501},
 issue_date = {Sep 2020},
 journal = {Multimedia Tools Appl.},
 keywords = {Encoder-decoder model, Feature fusion module, Image caption},
 number = {33–34},
 numpages = {15},
 pages = {24225–24239},
 publisher = {Kluwer Academic Publishers},
 title = {Boosting image caption generation with feature fusion module},
 url = {https://doi.org/10.1007/s11042-020-09110-2},
 volume = {79},
 year = {2020}
}

@inproceedings{Xia2021XGPTCG,
 author = {Qiaolin Xia and Haoyang Huang and Nan Duan and Dongdong Zhang and Lei Ji and Zhifang Sui and Edward Cui and Taroon Bharti and Xin Liu and Ming Zhou},
 booktitle = {NLPCC},
 file = {:PDF/XGPT- Cross-modal Generative Pre-Training for Image Captioning.pdf:PDF},
 title = {XGPT: Cross-modal Generative Pre-Training for Image Captioning},
 year = {2021}
}

@article{Xian2017,
 author = {Xian, Yang and Tian, Yingli},
 doi = {10.1109/TIP.2019.2917229},
 file = {:PDF/Self-Guiding_Multimodal_LSTMWhen_We_Do_Not_Have_a_Perfect_Training_Dataset_for_Image_Captioning.pdf:PDF},
 groups = {inception},
 journal = {IEEE Transactions on Image Processing},
 title = {Self-Guiding Multimodal LSTM-When We Do Not Have a Perfect Training Dataset for Image Captioning},
 volume = {PP},
 year = {2017}
}

@article{Xiao2019DAA,
 author = {Fen Xiao and Xue Gong and Yiming Zhang and Yanqing Shen and Jun Li and Xieping Gao},
 doi = {10.1016/j.neucom.2019.06.085},
 file = {:PDF/main.pdf:PDF},
 issn = {0925-2312},
 journal = {Neurocomputing},
 keywords = {Image captioning, Adaptive attention, Convolutional neural network, Long-short term memory},
 pages = {322-329},
 title = {DAA: Dual LSTMs with adaptive attention for image captioning},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231219309993},
 volume = {364},
 year = {2019}
}

@article{Xiao2019Dense,
 author = {Xinyu Xiao and Lingfeng Wang and Kun Ding and Shiming Xiang and Chunhong Pan},
 doi = {10.1016/j.patcog.2019.01.028},
 file = {:PDF/Dense semantic embedding network for image captioning.pdf:PDF},
 issn = {0031-3203},
 journal = {Pattern Recognition},
 keywords = {Image captioning, Retrieval, High-level semantic information, Visual concept, Densely embedding, Long short-term memory},
 pages = {285-296},
 title = {Dense semantic embedding network for image captioning},
 url = {https://www.sciencedirect.com/science/article/pii/S0031320319300494},
 volume = {90},
 year = {2019}
}

@inproceedings{Xu2015ShowAttendTell,
 author = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
 booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
 file = {:PDF/xuc15.pdf:PDF},
 groups = {Attention Over Grid of CNN Features, single layer LSTM, attention taxonomy},
 location = {Lille, France},
 numpages = {10},
 pages = {2048–2057},
 publisher = {JMLR.org},
 series = {ICML'15},
 title = {Show, attend and tell: neural image caption generation with visual attention},
 year = {2015}
}

@inproceedings{Xu2017,
 author = {Xu, Kaisheng and Wang, Hanli and Tang, Pengjie},
 booktitle = {2017 IEEE International Conference on Multimedia and Expo (ICME)},
 doi = {10.1109/ICME.2017.8019408},
 file = {:PDF/Image_captioning_with_deep_LSTM_based_on_sequential_residual.pdf:PDF},
 groups = {vgg16, resnet},
 pages = {361-366},
 title = {Image captioning with deep LSTM based on sequential residual},
 year = {2017}
}

@article{Xu2019SceneGC,
 author = {N. Xu and Anan Liu and Jing Liu and Weizhi Nie and Yuting Su},
 file = {:PDF/1-s2.0-S1047320318303535-main.pdf:PDF},
 journal = {J. Vis. Commun. Image Represent.},
 pages = {477-485},
 title = {Scene graph captioner: Image captioning based on structural visual representation},
 volume = {58},
 year = {2019}
}

@article{Xu2020Survey,
 author = {Xu, Pengfei and Chang, Xiaojun and Guo, Ling and Huang, Po-Yao and Chen, Xiaojiang and Hauptmann, Alexander G},
 file = {:PDF/A survey of scene graph Generation and application.pdf:PDF},
 journal = {EasyChair Preprint},
 number = {3385},
 title = {A survey of scene graph: Generation and application},
 url = {https://easychair.org/publications/preprint_open/SrPK},
 year = {2020}
}

@inproceedings{xu2021textcap,
 author = {Guanghui Xu and Mingkui Tan and Shuaicheng Niu and Yucheng Luo and Qing Du and Qi Wu},
 booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition},
 comment = {https://github.com/guanghuixu/AnchorCaptioner},
 file = {:PDF/Towards Accurate Text-based Image Captioning with Content Diversity Exploration.pdf:PDF},
 title = {Towards Accurate Text-based Image Captioning with Content Diversity Exploration},
 year = {2021}
}

@article{XU2023DeepCaptioning,
 author = {Liming Xu and Quan Tang and Jiancheng Lv and Bochuan Zheng and Xianhua Zeng and Weisheng Li},
 doi = {10.1016/j.neucom.2023.126287},
 file = {:PDF/1-s2.0-S0925231223004101-main.pdf:PDF},
 groups = {review},
 issn = {0925-2312},
 journal = {Neurocomputing},
 keywords = {Image caption, Feature representation, Visual encoding, Language generation, Reinforcement learning},
 pages = {126287},
 title = {Deep image captioning: A review of methods, trends and future challenges},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231223004101},
 volume = {546},
 year = {2023}
}

@inproceedings{Xue_2024_CVPR,
 author = {Xue, Le and Yu, Ning and Zhang, Shu and Panagopoulou, Artemis and Li, Junnan and Mart{\'\i}n-Mart{\'\i}n, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/Xue_ULIP-2_Towards_Scalable_Multimodal_Pre-training_for_3D_Understanding_CVPR_2024_paper.pdf:PDF},
 pages = {27091-27101},
 title = {ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding},
 year = {2024}
}

@inproceedings{Yagcioglu2015Distributed,
 address = {Beijing, China},
 author = {Yagcioglu, Semih and Erdem, Erkut and Erdem, Aykut and Cakici, Ruket},
 booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 doi = {10.3115/v1/P15-2018},
 editor = {Zong, Chengqing and Strube, Michael},
 file = {:PDF/P15-2018.pdf:PDF},
 groups = {Earlier Deep Models},
 pages = {106--111},
 publisher = {Association for Computational Linguistics},
 title = {A Distributed Representation Based Query Expansion Approach for Image Captioning},
 url = {https://aclanthology.org/P15-2018/},
 year = {2015}
}

@inproceedings{Yan2015Deep,
 author = {Yan, Fei and Mikolajczyk, Krystian},
 booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2015.7298966},
 file = {:PDF/Yan_Deep_Correlation_for_2015_CVPR_paper.pdf:PDF},
 groups = {Earlier Deep Models},
 issn = {1063-6919},
 keywords = {Correlation;Yttrium;Graphics processing units;Protocols;Training;Libraries;Visualization},
 pages = {3441-3450},
 title = {Deep correlation for matching images and text},
 year = {2015}
}

@article{Yan2021Task,
 author = {Yan, Chenggang and Hao, Yiming and Li, Liang and Yin, Jian and Liu, Anan and Mao, Zhendong and Chen, Zhenyu and Gao, Xingyu},
 comment = {podzial wg. karpathy
podłącenie atencji Task-Adaptive do architektury koder-dekoder w architekturze transformer
ewaluacja na ms-coco na dwa sposoby
1. 82783, 40405,  40775 trening, walidacj, testy
2. 113287, 5000, 5000 trening, walidacj, testy

zalety
zastosowanie atencji task-adaptive powoduje ze wektory adaptujace ucza sie wedzy zwiazanej ze zdaniem
niwelowanie bledu zwizanego z cechami wizualnymi oraz atencja do nich przypisana, gdyz task-adaptive atenction rozrzedza atencje przez wstawianie tych wektorow},
 doi = {10.1109/TCSVT.2021.3067449},
 file = {:PDF/Task-Adaptive_Attention_for_Image_Captioning.pdf:PDF},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 pages = {1-1},
 title = {Task-Adaptive Attention for Image Captioning},
 year = {2021}
}

@InProceedings{Yang2011Corpus,
  author    = {Yang, Yezhou and Teo, Ching Lik and Daum\'{e}, Hal and Aloimonos, Yiannis},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  title     = {Corpus-guided sentence generation of natural images},
  year      = {2011},
  address   = {USA},
  pages     = {444–454},
  publisher = {Association for Computational Linguistics},
  series    = {EMNLP '11},
  abstract  = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.},
  file      = {:PDF/Corpus-Guided Sentence Generation of Natural Images.pdf:PDF},
  groups    = {template},
  isbn      = {9781937284114},
  location  = {Edinburgh, United Kingdom},
  numpages  = {11},
}

@inproceedings{Yang2016,
author = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Cohen, William W. and Salakhutdinov, Ruslan R.},
title = {Review networks for caption generation},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2369–2377},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@misc{Yang2017ImageCW,
      title={Image Captioning with Object Detection and Localization},
      author={Zhongliang Yang and Yu-Jin Zhang and Sadaqat ur Rehman and Yongfeng Huang},
      year={2017},
      eprint={1706.02430},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@article{Yang2018Unsupervised,
 author = {Yang Feng and Lin Ma and Wei Liu and Jiebo Luo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1811-10787.bib},
 comment = {https://github.com/fengyang0317/unsupervised_captioning},
 eprint = {1811.10787},
 eprinttype = {arXiv},
 file = {:PDF/Unsupervised Image Captioning.pdf:PDF},
 groups = {Other deep learning methods},
 journal = {CoRR},
 timestamp = {Thu, 28 Nov 2019 07:55:53 +0100},
 title = {Unsupervised Image Captioning},
 url = {http://arxiv.org/abs/1811.10787},
 volume = {abs/1811.10787},
 year = {2018}
}

@article{Yang2019AutoEncodingSG,
 author = {X. Yang and Kaihua Tang and Hanwang Zhang and J. Cai},
 comment = {https://github.com/yangxuntu/SGAE},
 file = {:PDF/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf:PDF},
 groups = {Scene graphs, Two-layer LSTM},
 journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {10677-10686},
 printed = {yes},
 title = {Auto-Encoding Scene Graphs for Image Captioning},
 url = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf},
 year = {2019}
}

@InProceedings{Yang2019Collate,
  author    = {Yang, Xu and Zhang, Hanwang and Cai, Jianfei},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Learning to Collocate Neural Modules for Image Captioning},
  year      = {2019},
  pages     = {4249-4259},
  abstract  = {We do not speak word by word from scratch; our brain quickly structures a pattern like STH DO STH AT SOMEPLACE and then fills in the detailed descriptions. To render existing encoder-decoder image captioners such humanlike reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the “inner pattern” connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (i.e., question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design - one for function words and three for visual content words (e.g., noun, adjective, and verb), 2) soft module fusion and multistep module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (e.g., adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDErD on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, e.g., by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.},
  doi       = {10.1109/ICCV.2019.00435},
  issn      = {2380-7504},
  keywords  = {Visualization;Training;Task analysis;Cognition;Dogs;Decoding;Neural networks},
}

@article{Yang2020Deconfounded,
 author = {Xu Yang and Hanwang Zhang and Jianfei Cai},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2003-03923.bib},
 eprint = {2003.03923},
 eprinttype = {arXiv},
 file = {:PDF/Deconfounded image captioning- A causal retrospect.pdf:PDF},
 journal = {CoRR},
 timestamp = {Fri, 12 Jun 2020 16:01:21 +0200},
 title = {Deconfounded Image Captioning: {A} Causal Retrospect},
 url = {https://arxiv.org/abs/2003.03923},
 volume = {abs/2003.03923},
 year = {2020}
}

@inproceedings{Yang2020FashionCT,
 author = {Xuewen Yang and Heming Zhang and Di Jin and Yingru Liu and Chi-Hao Wu and Jianchao Tan and Dongliang Xie and Jue Wang and Xin Wang},
 booktitle = {ECCV},
 file = {:PDF/Fashion Captioning- Towards Generating Accurate Descriptions with Semantic Rewards.pdf:PDF},
 title = {Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards},
 year = {2020}
}

@misc{Yang2021Reformer,
 archiveprefix = {arXiv},
 author = {Xuewen Yang and Yingru Liu and Xin Wang},
 eprint = {2107.14178},
 file = {:PDF/ReFormer- The Relational Transformer for Image Captioning.pdf:PDF},
 groups = {relations},
 primaryclass = {cs.CV},
 title = {ReFormer: The Relational Transformer for Image Captioning},
 year = {2021}
}

@article{Yang2022HumanCentric,
 address = {USA},
 author = {Yang, Zuopeng and Wang, Pengbo and Chu, Tianshu and Yang, Jie},
 doi = {10.1016/j.patcog.2022.108545},
 file = {:PDF/1-s2.0-S0031320322000267-main.pdf:PDF},
 issn = {0031-3203},
 issue_date = {Jun 2022},
 journal = {Pattern Recogn.},
 keywords = {Feature hierarchization, Image captioning, Human-centric},
 number = {C},
 numpages = {11},
 publisher = {Elsevier Science Inc.},
 title = {Human-Centric Image Captioning},
 url = {https://doi.org/10.1016/j.patcog.2022.108545},
 volume = {126},
 year = {2022}
}

@article{Yang2024image,
 author = {Yang, Zhen and Zhou, Ziwei and Wang, Chaoyang and Xu, Liang},
 file = {:PDF/IJCS_51_9_17.pdf:PDF},
 journal = {IAENG International Journal of Computer Science},
 number = {9},
 title = {Image Guidance Encoder-Decoder Model in Image Captioning and Its Application.},
 volume = {51},
 year = {2024}
}

@inproceedings{YanGan2018Image,
 author = {Yan, Shiyang and Wu, Fangyu and Smith, Jeremy S. and Lu, Wenjin and Zhang, Bailing},
 booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
 doi = {10.1109/ICPR.2018.8545049},
 file = {:PDF/Image Captioning using Adversarial Networks and Reinforcement Learning.pdf:PDF},
 pages = {248-253},
 title = {Image Captioning using Adversarial Networks and Reinforcement Learning},
 year = {2018}
}

@InProceedings{Yao2017ICCV,
 author={Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
  title={Boosting Image Captioning with Attributes},
  year={2017},
  volume={},
  number={},
  pages={4904-4912},
  keywords={Correlation;Image representation;Hidden Markov models;Semantics;Detectors;Neural networks;Computer vision},
  doi={10.1109/ICCV.2017.524},
  ISSN={2380-7504}}

@inproceedings{Yao2018Exploring,
 author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
 booktitle = {Proceedings of the European conference on computer vision (ECCV)},
 file = {:PDF/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf:PDF},
 groups = {Spatial and semantic graphs., Two-layer LSTM},
 pages = {684--699},
 printed = {yes},
 title = {Exploring visual relationship for image captioning},
 url = {https://arxiv.org/pdf/1809.07041.pdf},
 year = {2018}
}

@inproceedings{Yao2019HierarchyParsing,
 author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
 doi = {10.1109/ICCV.2019.00271},
 file = {:PDF/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
 groups = {Hierarchical trees, Two-layer LSTM},
 issn = {2380-7504},
 keywords = {Hip;Semantics;Task analysis;Decoding;Visualization;Context modeling;Image segmentation},
 pages = {2621-2629},
 title = {Hierarchy Parsing for Image Captioning},
 year = {2019}
}

@inproceedings{yatskar-etal-2014-see,
 address = {Dublin, Ireland},
 author = {Yatskar, Mark and Galley, Michel and Vanderwende, Lucy and Zettlemoyer, Luke},
 booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014)},
 doi = {10.3115/v1/S14-1015},
 editor = {Bos, Johan and Frank, Anette and Navigli, Roberto},
 file = {:PDF/S14-1015.pdf:PDF},
 pages = {110--120},
 publisher = {Association for Computational Linguistics and Dublin City University},
 title = {See No Evil, Say No Evil: Description Generation from Densely Labeled Images},
 url = {https://aclanthology.org/S14-1015},
 year = {2014}
}

@article{Ye2018Attentive,
 author = {Ye, Senmao and Han, Junwei and Liu, Nian},
 doi = {10.1109/TIP.2018.2855406},
 file = {:PDF/Attentive_Linear_Transformation_for_Image_Captioning.pdf:PDF},
 issn = {1941-0042},
 journal = {IEEE Transactions on Image Processing},
 keywords = {Visualization;Adaptation models;Semantics;Decoding;Task analysis;Feeds;Feature extraction;Image captioning;attention;linear transformation;CNN;LSTM},
 number = {11},
 pages = {5514-5524},
 title = {Attentive Linear Transformation for Image Captioning},
 volume = {27},
 year = {2018}
}

@article{Yoav2014Word2VecExplained,
 author = {Yoav Goldberg and Omer Levy},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/GoldbergL14.bib},
 eprint = {1402.3722},
 eprinttype = {arXiv},
 file = {:PDF/1402.3722v1.pdf:PDF},
 groups = {word2vec},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
 title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
 url = {http://arxiv.org/abs/1402.3722},
 volume = {abs/1402.3722},
 year = {2014}
}

@inproceedings{You2016Semantic,
 author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR.2016.503},
 file = {:PDF/You_Image_Captioning_With_CVPR_2016_paper.pdf:PDF},
 groups = {global CNN features, attention taxonomy},
 keywords = {Visualization;Semantics;Recurrent neural networks;Feature extraction;Natural languages;Computer vision},
 pages = {4651-4659},
 title = {Image Captioning with Semantic Attention},
 year = {2016}
}

@article{Youngetal2014Image,
 author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
 doi = {10.1162/tacl_a_00166},
 file = {:PDF/From image descriptions to visual denotations- New similarity metrics for semantic inference over event descriptions.pdf:PDF},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {67--78},
 title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
 volume = {2},
 year = {2014}
}

@article{Yu2012Fuzzy,
 author = {Yu, Daren and An, Shuang and Hu, Qinghua},
 doi = {10.1080/18756891.2011.9727817},
 file = {:PDF/Fuzzy_Mutual_Information_Based_min-Redundancy_and_.pdf:PDF},
 journal = {International Journal of Computational Intelligence Systems},
 title = {Fuzzy Mutual Information Based min-Redundancy and Max-Relevance Heterogeneous Feature Selection},
 volume = {4},
 year = {2012}
}

@article{Yu2020Multimodal,
 author = {Yu, Jun and Li, Jing and Yu, Zhou and Huang, Qingming},
 doi = {10.1109/TCSVT.2019.2947482},
 file = {:PDF/Multimodal_Transformer_With_Multi-View_Visual_Representation_for_Image_Captioning.pdf:PDF},
 issn = {1558-2205},
 journal = {IEEE Transactions on Circuits and Systems for Video Technology},
 keywords = {Visualization;Feature extraction;Hidden Markov models;Adaptation models;Task analysis;Decoding;Computational modeling;Image captioning;multi-view learning;deep learning},
 number = {12},
 pages = {4467-4480},
 title = {Multimodal Transformer With Multi-View Visual Representation for Image Captioning},
 volume = {30},
 year = {2020}
}

@article{Zadeh1965Fuzzy,
 author = {L.A. Zadeh},
 doi = {10.1016/S0019-9958(65)90241-X},
 file = {:PDF/Zadeh65.pdf:PDF},
 issn = {0019-9958},
 journal = {Information and Control},
 number = {3},
 pages = {338-353},
 title = {Fuzzy sets},
 url = {https://www.sciencedirect.com/science/article/pii/S001999586590241X},
 volume = {8},
 year = {1965}
}

@article{Zadeh1973Outline,
 author = {L. A. {Zadeh}},
 doi = {10.1109/TSMC.1973.5408575},
 file = {:PDF/Zadeh1973.pdf:PDF},
 journal = {IEEE Transactions on Systems, Man, and Cybernetics},
 number = {1},
 pages = {28-44},
 title = {Outline of a New Approach to the Analysis of Complex Systems and Decision Processes},
 volume = {SMC-3},
 year = {1973}
}

@article{Zaremba2015ReinforcementLN,
 author = {Wojciech Zaremba and Ilya Sutskever},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/ZarembaS15.bib},
 eprint = {1505.00521},
 eprinttype = {arXiv},
 file = {:PDF/REINFORCEMENT LEARNING NEURAL TURING MACHINES - REVISED.pdf:PDF},
 journal = {CoRR},
 timestamp = {Mon, 13 Aug 2018 16:46:53 +0200},
 title = {Reinforcement Learning Neural Turing Machines},
 url = {http://arxiv.org/abs/1505.00521},
 volume = {abs/1505.00521},
 year = {2015}
}

@inproceedings{Zeiler2014ZFNet,
 address = {Cham},
 author = {Zeiler, Matthew D. and Fergus, Rob},
 booktitle = {Computer Vision -- ECCV 2014},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 file = {:PDF/978-3-319-10590-1.pdf:PDF},
 isbn = {978-3-319-10590-1},
 pages = {818--833},
 publisher = {Springer International Publishing},
 title = {Visualizing and Understanding Convolutional Networks},
 year = {2014}
}

@article{ZELASZCZYK2023302,
 author = {Maciej Żelaszczyk and Jacek Mańdziuk},
 doi = {10.1016/j.inffus.2023.01.008},
 file = {:PDF/Cross-modal text and visual generation- A systematic review. Part 1- Image to text.pdf:PDF},
 groups = {review},
 issn = {1566-2535},
 journal = {Information Fusion},
 keywords = {Image-to-text generation, Text-to-image generation, Image captioning, Cross-modal learning, Multimodal learning, Representation learning},
 pages = {302-329},
 title = {Cross-modal text and visual generation: A systematic review. Part 1: Image to text},
 url = {https://www.sciencedirect.com/science/article/pii/S1566253523000143},
 volume = {93},
 year = {2023}
}

@inproceedings{Zellers2018Scenegraphs,
 author = {Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
 booktitle = {Conference on Computer Vision and Pattern Recognition},
 file = {:PDF/Neural Motifs- Scene Graph Parsing with Global Context.pdf:PDF},
 title = {Neural Motifs: Scene Graph Parsing with Global Context},
 year = {2018}
}

@inproceedings{Zellers2019Vcr,
 author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
 booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/from recognition to cognition.pdf:PDF},
 title = {From Recognition to Cognition: Visual Commonsense Reasoning},
 year = {2019}
}

@inproceedings{Zhang1993Fuzzy,
 author = {W. {Zhang} and M. {Sugeno}},
 booktitle = {[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems},
 doi = {10.1109/FUZZY.1993.327529},
 file = {:PDF/A_Fuzzy_approach_Zhang_1993.pdf:PDF},
 pages = {564-569 vol.1},
 timestamp = {2021-09-13},
 title = {A fuzzy approach to scene understanding},
 year = {1993}
}

@misc{zhang2017actorcriticsequencetrainingimage,
 archiveprefix = {arXiv},
 author = {Li Zhang and Flood Sung and Feng Liu and Tao Xiang and Shaogang Gong and Yongxin Yang and Timothy M. Hospedales},
 eprint = {1706.09601},
 file = {:PDF/ac_nips2017.pdf:PDF},
 groups = {Other deep learning methods},
 primaryclass = {cs.CV},
 title = {Actor-Critic Sequence Training for Image Captioning},
 url = {https://arxiv.org/abs/1706.09601},
 year = {2017}
}

@inproceedings{Zhang2019CVPR,
 author = {Zhang, Lu and Zhang, Jianming and Lin, Zhe and Lu, Huchuan and He, You},
 booktitle = {CVPR},
 comment = {https://github.com/zhangludl/code-and-dataset-for-CapSal},
 file = {:PDF/CapSal_Leveraging_Captioning_to_Boost_Semantics_for_Salient_Object_Detection_CVPR_2019_paper.pdf:PDF},
 title = {CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection},
 year = {2019}
}

@article{Zhang2019HQ,
 author = {Zhang, Zongjian and Wu, Qiang and Wang, Yang and Chen, Fang},
 doi = {10.1109/TMM.2018.2888822},
 file = {:PDF/High-Quality_Image_Captioning_With_Fine-Grained_and_Semantic-Guided_Visual_Attention.pdf:PDF},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Visualization;Semantics;Feature extraction;Decoding;Task analysis;Object oriented modeling;Image resolution;Image captioning;attention mechanism;fine-grained resolution;semantic guidance;fully convolutional network-long short term memory framework},
 number = {7},
 pages = {1681-1693},
 title = {High-Quality Image Captioning With Fine-Grained and Semantic-Guided Visual Attention},
 volume = {21},
 year = {2019}
}

@article{Zhang2019VAA,
 author = {Zhang, Zhengyuan and Zhang, Wenkai and Diao, Wenhui and Yan, Menglong and Gao, Xin and Sun, Xian},
 doi = {10.1109/ACCESS.2019.2942154},
 file = {:PDF/VAA_Visual_Aligning_Attention_Model_for_Remote_Sensing_Image_Captioning.pdf:PDF;:PDF/VAA_Visual_Aligning_Attention_Model_for_Remote_Sensing_Image_Captioning.pdf:PDF},
 journal = {IEEE Access},
 keywords = {Visualization;Remote sensing;Training;Task analysis;Feature extraction;Decoding;Solid modeling;Image captioning;remote sensing image captioning;attention mechanism;visual aligning},
 pages = {137355-137364},
 title = {VAA: Visual Aligning Attention Model for Remote Sensing Image Captioning},
 volume = {7},
 year = {2019}
}

@article{ZHANG2020,
 author = {Xiaodan Zhang and Shengfeng He and Xinhang Song and Rynson W.H. Lau and Jianbin Jiao and Qixiang Ye},
 doi = {10.1016/j.neucom.2018.02.112},
 file = {:PDF/Image captioning via semantic element embedding.pdf:PDF},
 issn = {0925-2312},
 journal = {Neurocomputing},
 keywords = {Image captioning, Element embedding, CNN, LSTM},
 pages = {212-221},
 title = {Image captioning via semantic element embedding},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231219309130},
 volume = {395},
 year = {2020}
}

@inproceedings{Zhang2020BERTScoreET,
 author = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
 file = {:PDF/BERTScore- Evaluating Text Generation with BERT.pdf:PDF},
 publisher = {OpenReview.net},
 timestamp = {Wed, 03 Jun 2020 10:08:32 +0200},
 title = {BERTScore: Evaluating Text Generation with {BERT}},
 url = {https://openreview.net/forum?id=SkeHuCVFDr},
 year = {2020}
}

@article{zhang2021DeepRelationEmbedding,
 author = {Zhang, Yifan and Zhou, Wengang and Wang, Min and Tian, Qi and Li, Houqiang},
 doi = {10.1109/TIP.2020.3038354},
 file = {:PDF/Deep_Relation_Embedding_for_Cross-Modal_Retrieval.pdf:PDF},
 groups = {embeddings, relations},
 journal = {IEEE Transactions on Image Processing},
 pages = {617-627},
 title = {Deep Relation Embedding for Cross-Modal Retrieval},
 volume = {30},
 year = {2021}
}

@inproceedings{Zhang2021Revisiting,
 address = {Los Alamitos, CA, USA},
 author = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR46437.2021.00553},
 file = {:PDF/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf:PDF},
 groups = {Early fusion and vision-and-language pre-training., BERT},
 keywords = {Training;Visualization;Computational modeling;Object detection;Benchmark testing;Feature extraction;Transformers},
 pages = {5575-5584},
 publisher = {IEEE Computer Society},
 title = {{ VinVL: Revisiting Visual Representations in Vision-Language Models }},
 url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00553},
 year = {2021}
}

@inproceedings{Zhang2021RSTNet,
 author = {Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR46437.2021.01521},
 file = {:PDF/RSTNet_Captioning_with_Adaptive_Attention_on_Visual_and_Non-Visual_Words.pdf:PDF},
 issn = {2575-7075},
 keywords = {Geometry;Visualization;Adaptation models;Predictive models;Transformers;Time measurement;Servers},
 pages = {15460-15469},
 title = {RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words},
 year = {2021}
}

@article{Zhang2021Vinvl,
 author = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
 file = {:PDF/2101.00529.pdf},
 journal = {CVPR 2021},
 title = {VinVL: Making Visual Representations Matter in Vision-Language Models},
 year = {2021}
}

@book{zhang2023dive,
 author = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
 file = {:PDF/2106.11342v5.pdf:PDF},
 note = {\url{https://D2L.ai}},
 publisher = {Cambridge University Press},
 title = {Dive into Deep Learning},
 year = {2023}
}

@article{Zhang2024PointGT,
 author = {Zhang, Huang and Wang, Changshuo and Yu, Long and Tian, Shengwei and Ning, Xin and Rodrigues, Joel},
 doi = {10.1109/TMM.2024.3374580},
 file = {:PDF/ssrn-4603211.pdf:PDF},
 journal = {IEEE Transactions on Multimedia},
 keywords = {Point cloud compression;Feature extraction;Three-dimensional displays;Kernel;Bidirectional control;Robustness;Optimization;deep learning;3D point cloud classification and segmentation;local neighborhood;geometric transformation;attention mechanism},
 pages = {1-12},
 title = {PointGT: A Method for Point-Cloud Classification and Segmentation Based on Local Geometric Transformation},
 year = {2024}
}

@article{Zhao2019Object,
 author = {Z. {Zhao} and P. {Zheng} and S. {Xu} and X. {Wu}},
 doi = {10.1109/TNNLS.2018.2876865},
 file = {:PDF/Object_detection_Zhong_2019.pdf:PDF},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 number = {11},
 pages = {3212-3232},
 title = {Object Detection With Deep Learning: A Review},
 volume = {30},
 year = {2019}
}

@article{Zhao_Wu_Zhang_2020,
 abstractnote = {Generating stylized captions for images is a challenging task since it requires not only describing the content of the image accurately but also expressing the desired linguistic style appropriately. In this paper, we propose MemCap, a novel stylized image captioning method that explicitly encodes the knowledge about linguistic styles with memory mechanism. Rather than relying heavily on a language model to capture style factors in existing methods, our method resorts to memorizing stylized elements learned from training corpus. Particularly, we design a memory module that comprises a set of embedding vectors for encoding style-related phrases in training corpus. To acquire the style-related phrases, we develop a sentence decomposing algorithm that splits a stylized sentence into a style-related part that reflects the linguistic style and a content-related part that contains the visual content. When generating captions, our MemCap first extracts content-relevant style knowledge from the memory module via an attention mechanism and then incorporates the extracted knowledge into a language model. Extensive experiments on two stylized image captioning datasets (SentiCap and FlickrStyle10K) demonstrate the effectiveness of our method.},
 author = {Zhao, Wentian and Wu, Xinxiao and Zhang, Xiaoxun},
 doi = {10.1609/aaai.v34i07.6998},
 file = {:PDF/MemCap- Memorizing Style Knowledge for Image Captioning .pdf:PDF},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {07},
 pages = {12984-12992},
 title = {MemCap: Memorizing Style Knowledge for Image Captioning},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/6998},
 volume = {34},
 year = {2020}
}

@inproceedings{Zhong2020Comprehensive,
 address = {Cham},
 author = {Zhong, Yiwu and Wang, Liwei and Chen, Jianshu and Yu, Dong and Li, Yin},
 booktitle = {Computer Vision -- ECCV 2020},
 comment = {- użycie grafu wiedzy
- naturalnie panujemy nad tworzeniem opisu dzieki wyborowi podgrafow wiedzy
- wejście to graf wiedzy dla obrazu
- stworzona sieć neuronowa wybiera podgrafy, następnie dekodowane do zdań z użyciem atencji
- pierwsza kompleksowa metoda do tworzenia dokładnych, zróżnicowanych oraz kontrolowanych opisów opartych na obszarach obrazu

siec sGPN(subgraph proposal network) ucząca sie identyfikacji znaczących subgrafow, ktore nastepnie sa dekodowane przez sieć LSTM w elu produkcji zdań

identyfikacja znaczących podgrafow - grafowa siec konwolucyjna plus funkcja rankingowa do subgrafow
metoda rankingowa - parujemy zdanie referencyjne prawdziwe z wierzchołkami podgrafu. Ten podgraf ktory zawiera najwięcej rzeczowników/

skutecznosc - najelpsza z 20 najlepiej działających na wyjściu NIE ŚREDNIA
82,783 TRENING ms-coco
4k dla walidacji i 1k dla testów wybranych przypadkowo z zbioru validacyjnego ms-coco(40,504 obrazów)
grafy uczone na visual genome},
 doi = {https://link.springer.com/chapter/10.1007/978-3-030-58568-6_13},
 editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
 file = {:PDF/Comprehensive Image Captioning via Scene Graph Decomposition.pdf:PDF},
 groups = {mix},
 isbn = {978-3-030-58568-6},
 pages = {211--229},
 publisher = {Springer International Publishing},
 title = {Comprehensive Image Captioning via Scene Graph Decomposition},
 url = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590205.pdf},
 year = {2020}
}

@article{zhongang2021,
 author = {Zhongang Qi and Saeed Khorram and Li Fuxin},
 doi = {10.1016/j.artint.2020.103435},
 file = {:PDF/Embedding deep networks into visual explanations.pdf:PDF},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 keywords = {Deep neural networks, Embedding, Visual explanations},
 pages = {103435},
 title = {Embedding deep networks into visual explanations},
 url = {https://www.sciencedirect.com/science/article/pii/S000437022030182X},
 volume = {292},
 year = {2021}
}

@inproceedings{Zhou2015CVPR,
 author = {Zhou, Bolei and Jagadeesh, Vignesh and Piramuthu, Robinson},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {:PDF/Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf:PDF},
 title = {ConceptLearner: Discovering Visual Concepts From Weakly Labeled Image Collections},
 url = {https://www.researchgate.net/publication/268525469_ConceptLearner_Discovering_Visual_Concepts_from_Weakly_Labeled_Image_Collections},
 year = {2015}
}

@inproceedings{Zhou2020More,
 author = {Zhou, Yuanen and Wang, Meng and Liu, Daqing and Hu, Zhenzhen and Zhang, Hanwang},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 doi = {10.1109/CVPR42600.2020.00483},
 file = {:PDF/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper.pdf:PDF},
 issn = {2575-7075},
 keywords = {Visualization;Grounding;Task analysis;Training;Measurement;Computational modeling;Image edge detection},
 pages = {4776-4785},
 title = {More Grounded Image Captioning by Distilling Image-Text Matching Model},
 year = {2020}
}

@inproceedings{Zhou2020Unified,
 author = {Luowei Zhou and Hamid Palangi and Lei Zhang and Houdong Hu and Jason J. Corso and Jianfeng Gao},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/ZhouPZHCG20.bib},
 booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
 comment = {https://github.com/LuoweiZhou/VLP},
 doi = {10.1609/AAAI.V34I07.7005},
 file = {:PDF/Unified Vision-Language Pre-Training for Image Captioning and VQA.pdf:PDF},
 groups = {Early fusion and vision-and-language pre-training., BERT},
 pages = {13041--13049},
 publisher = {{AAAI} Press},
 timestamp = {Thu, 11 Apr 2024 13:33:56 +0200},
 title = {Unified Vision-Language Pre-Training for Image Captioning and {VQA}},
 url = {https://doi.org/10.1609/aaai.v34i07.7005},
 year = {2020}
}

@inproceedings{Zhou2021Semi,
 author = {Zhou, Yuanen and Zhang, Yong and Hu, Zhenzhen and Wang, Meng},
 booktitle = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
 comment = {https://github.com/YuanEZhou/satic},
 doi = {10.1109/ICCVW54120.2021.00350},
 file = {:PDF/Semi-Autoregressive_Transformer_for_Image_Captioning.pdf:PDF},
 pages = {3132-3136},
 title = {Semi-Autoregressive Transformer for Image Captioning},
 year = {2021}
}

@article{Zhou2022Collaborative,
 author = {Zhou, Dongming and Yang, Jing and Bao, Riqiang},
 doi = {10.1007/s10489-021-02943-w},
 file = {:PDF/s10489-021-02943-w.pdf:PDF},
 journal = {Applied Intelligence},
 pages = {1-16},
 title = {Collaborative strategy network for spatial attention image captioning},
 volume = {52},
 year = {2022}
}

@misc{Zhou2022Compact,
 archiveprefix = {arXiv},
 author = {Yuanen Zhou and Zhenzhen Hu and Daqing Liu and Huixia Ben and Meng Wang},
 comment = {https://github.com/YuanEZhou/CBTrans},
 eprint = {2201.01984},
 file = {:PDF/Compact Bidirectional Transformer for Image Captioning .pdf:PDF},
 primaryclass = {cs.CV},
 printed = {yes},
 title = {Compact Bidirectional Transformer for Image Captioning},
 year = {2022}
}

@misc{Zhou2023,
 author = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
 copyright = {arXiv.org perpetual, non-exclusive license},
 doi = {10.48550/ARXIV.2302.09419},
 file = {:PDF/A Comprehensive Survey on Pretrained Foundation Models- A History from BERT to ChatGPT.pdf:PDF},
 groups = {review},
 keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
 url = {https://arxiv.org/abs/2302.09419},
 year = {2023}
}

@article{Zhou_Niu_Wang_Gao_Zhang_Hua_2020,
 abstractnote = {For visual-semantic embedding, the existing methods normally treat the relevance between queries and candidates in a bipolar way – relevant or irrelevant, and all “irrelevant” candidates are uniformly pushed away from the query by an equal margin in the embedding space, regardless of their various proximity to the query. This practice disregards relatively discriminative information and could lead to suboptimal ranking in the retrieval results and poorer user experience, especially in the long-tail query scenario where a matching candidate may not necessarily exist. In this paper, we introduce a continuous variable to model the relevance degree between queries and multiple candidates, and propose to learn a coherent embedding space, where candidates with higher relevance degrees are mapped closer to the query than those with lower relevance degrees. In particular, the new ladder loss is proposed by extending the triplet loss inequality to a more general inequality chain, which implements variable push-away margins according to respective relevance degrees. In addition, a proper Coherent Score metric is proposed to better measure the ranking results including those “irrelevant” candidates. Extensive experiments on multiple datasets validate the efficacy of our proposed method, which achieves significant improvement over existing state-of-the-art methods.},
 author = {Zhou, Mo and Niu, Zhenxing and Wang, Le and Gao, Zhanning and Zhang, Qilin and Hua, Gang},
 doi = {10.1609/aaai.v34i07.7006},
 file = {:PDF/Ladder Loss for Coherent Visual-Semantic Embedding .pdf:PDF},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {07},
 pages = {13050-13057},
 title = {Ladder Loss for Coherent Visual-Semantic Embedding},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/7006},
 volume = {34},
 year = {2020}
}

@article{Zhu2018CaptTransf,
 article-number = {739},
 author = {Zhu, Xinxin and Li, Lixiang and Liu, Jing and Peng, Haipeng and Niu, Xinxin},
 doi = {10.3390/app8050739},
 file = {:PDF/applsci-08-00739.pdf:PDF},
 issn = {2076-3417},
 journal = {Applied Sciences},
 number = {5},
 title = {Captioning Transformer with Stacked Attention Modules},
 url = {https://www.mdpi.com/2076-3417/8/5/739},
 volume = {8},
 year = {2018}
}

@article{ZHU2018TripleAttention,
 author = {Xinxin Zhu and Lixiang Li and Jing Liu and Ziyi Li and Haipeng Peng and Xinxin Niu},
 doi = {10.1016/j.neucom.2018.08.069},
 file = {:PDF/1-s2.0-S0925231218310324-main.pdf:PDF;:PDF/1-s2.0-S0925231218310324-main.pdf:PDF},
 issn = {0925-2312},
 journal = {Neurocomputing},
 keywords = {Image caption, Deep learning, LSTM, CNN, Attention},
 pages = {55-65},
 title = {Image captioning with triple-attention and stack parallel LSTM},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231218310324},
 volume = {319},
 year = {2018}
}

@misc{zhu2021autocaptionimagecaptioningneural,
 archiveprefix = {arXiv},
 author = {Xinxin Zhu and Weining Wang and Longteng Guo and Jing Liu},
 eprint = {2012.09742},
 file = {:PDF/2012.09742v3.pdf:PDF},
 groups = {Neural Architecture Search for RNN, Boosting LSTM with Self-Attention},
 primaryclass = {cs.CV},
 title = {AutoCaption: Image Captioning with Neural Architecture Search},
 url = {https://arxiv.org/abs/2012.09742},
 year = {2021}
}

@inproceedings{Zhu2024FeatureEngineering,
 address = {New York, NY, USA},
 author = {Zhu, Qian and Wang, Dakuo and Ma, Shuai and Wang, April Yi and Chen, Zixin and Khurana, Udayan and Ma, Xiaojuan},
 booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
 doi = {10.1145/3643834.3661517},
 file = {:PDF/2405.14107v1.pdf:PDF},
 groups = {representation learning},
 isbn = {9798400705830},
 keywords = {Computational Notebooks, Feature Recommendation, human-AI Collaboration},
 location = {Copenhagen, Denmark},
 numpages = {16},
 pages = {1789–1804},
 publisher = {Association for Computing Machinery},
 series = {DIS '24},
 title = {Towards Feature Engineering with Human and AI’s Knowledge: Understanding Data Science Practitioners’ Perceptions in Human\&AI-Assisted Feature Engineering Design},
 url = {https://doi.org/10.1145/3643834.3661517},
 year = {2024}
}

@article{Zhuang2020Tranfer,
 author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
 doi = {10.1109/JPROC.2020.3004555},
 file = {:PDF/A Comprehensive Survey on Transfer Learning.pdf:PDF},
 journal = {Proceedings of the IEEE},
 pages = {1-34},
 title = {A Comprehensive Survey on Transfer Learning},
 volume = {PP},
 year = {2020}
}

@Misc{fung2025embodiedaiagentsmodeling,
  author        = {Pascale Fung and Yoram Bachrach and Asli Celikyilmaz and Kamalika Chaudhuri and Delong Chen and Willy Chung and Emmanuel Dupoux and Hongyu Gong and Hervé Jégou and Alessandro Lazaric and Arjun Majumdar and Andrea Madotto and Franziska Meier and Florian Metze and Louis-Philippe Morency and Théo Moutakanni and Juan Pino and Basile Terver and Joseph Tighe and Paden Tomasello and Jitendra Malik},
  title         = {Embodied AI Agents: Modeling the World},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2506.22355},
  file          = {:PDF/2506.22355v3 (1).pdf:PDF},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2506.22355},
}

@Unknown{Bain2009WittgensteinNLP,
  author = {Bain, Robert and Festa, Andrew and Lee, Ga and Zhang, Andy},
  file   = {:PDF/17871-Article Text-27183-1-10-20250531.pdf:PDF},
  month  = {09},
  title  = {Wittgenstein's Philosophy of Language The Philosophical Origins of Modern NLP Thinking},
  year   = {2022},
}

@Book{Shanker1998WittgensteinRemarksOnAI,
  author    = {Shanker, Stuart.},
  publisher = {Routledge},
  title     = {Wittgenstein's remarks on the foundations of AI / Stuart Shanker.},
  year      = {1998},
  address   = {London ;},
  edition   = {1st ed.},
  isbn      = {1-134-85991-0},
  abstract  = {Wittgenstein's Remarks on the Foundations of AI is a valuable contribution to the study of Wittgenstein's theories and his controversial attack on artificial intelligence, which successfully crosses a number of disciplines, including philosophy, psychology, logic, artificial intelligence and cognitive science, to provide a stimulating and searching analysis.},
  booktitle = {Wittgenstein's remarks on the foundations of AI},
  keywords  = {Artificial intelligence ; Wittgenstein Ludwig 1889-1951 -- Contributions in artificial intelligence},
  language  = {eng},
  lccn      = {97013425},
}

@InProceedings{Gan2017Semantic,
  author    = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Semantic Compositional Networks for Visual Captioning},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {5630-5637},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.127},
  file      = {:PDF/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf:PDF},
  groups    = {global CNN features},
  issn      = {1063-6919},
  keywords  = {Semantics;Visualization;Pediatrics;Feature extraction;Mouth;Tensile stress;Training},
}

@InProceedings{Wu2016WhatValueVector,
  author    = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and Van Den Hengel, Anton},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {What Value Do Explicit High Level Concepts Have in Vision to Language Problems?},
  year      = {2016},
  pages     = {205},
  doi       = {10.1109/CVPR.2016.29},
  file      = {:PDF/Wu_What_Value_Do_CVPR_2016_paper.pdf:PDF},
  groups    = {bez atencji, encoder-decoder-lit, global CNN features},
  issn      = {1063-6919},
  keywords  = {Visualization;Knowledge discovery;Feature extraction;Semantics;Vocabulary;Computer vision;Training},
}

@InProceedings{Rennie2017SelfCriticalSTLowFeatures,
  author    = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Self-Critical Sequence Training for Image Captioning }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {1183},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.131},
  file      = {:PDF/Self- critical sequence training for image captioning.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Training;Inference algorithms;Measurement;Logic gates;Predictive models;Learning (artificial intelligence)},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.131},
}

@InProceedings{Lu2017KnowingWT,
  author    = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
  year      = {2017},
  pages     = {3242-3249},
  doi       = {10.1109/CVPR.2017.345},
  file      = {:PDF/Lu_Knowing_When_to_CVPR_2017_paper.pdf:PDF;:PDF/Knowing_When_to_Look_Adaptive_Attention_via_a_Visual_Sentinel_for_Image_Captioning.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Visualization;Adaptation models;Decoding;Context modeling;Computational modeling;Logic gates;Mathematical model},
}

@InProceedings{Fang2015CVPR,
  author    = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Lawrence Zitnick, C. and Zweig, Geoffrey},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {From Captions to Visual Concepts and Back},
  year      = {2015},
  pages     = {1473-1480},
  file      = {:PDF/Fang_From_Captions_to_2015_CVPR_paper.pdf:PDF},
  groups    = {Compositional architectures, global CNN features},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf},
}

@InProceedings{Ge2019ExploringAdditive,
  author    = {Ge, Hongwei and Yan, Zehang and Zhang, Kai and Zhao, Mingde and Sun, Liang},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {{Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style}},
  year      = {2019},
  address   = {Los Alamitos, CA, USA},
  pages     = {1757-1759},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/ICCV.2019.00184},
  file      = {:PDF/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features, hidden state reconstruction},
  keywords  = {Feature extraction;Semantics;Visualization;Decoding;Training;Computational modeling;Cognition},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00184},
}


@InProceedings{Chen2018RegularizingAdditive,
  author    = {X. Chen and L. Ma and W. Jiang and J. Yao and W. Liu},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Regularizing RNNs for Caption Generation by Reconstructing the Past with the Present},
  year      = {2018},
  address   = {Los Alamitos, CA, USA},
  pages     = {7997},
  publisher = {IEEE Computer Society},
  comment   = {https://github.com/chenxinpeng/ARNet},
  doi       = {10.1109/CVPR.2018.00834},
  file      = {:PDF/Chen_Regularizing_RNNs_for_CVPR_2018_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features, hidden state reconstruction},
  keywords  = {computer vision;pattern recognition}
}

@Misc{Wang2017SkeletonAdditive,
  author        = {Yufei Wang and Zhe Lin and Xiaohui Shen and Scott Cohen and Garrison W. Cottrell},
  title         = {Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1704.06972},
  file          = {:PDF/1704.06972v1.pdf:PDF},
  pages         = {4},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1704.06972},
}

@InProceedings{Gu2018StackCaptioningAdditive,
  author    = {Gu, Jiuxiang and Cai, Jianfei and Wang, Gang and Chen, Tsuhan},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  title     = {Stack-captioning: coarse-to-fine learning for image captioning},
  year      = {2018},
  pages     = {6841},
  publisher = {AAAI Press},
  series    = {AAAI'18/IAAI'18/EAAI'18},
  articleno = {837},
  file      = {:PDF/3504035.3504872.pdf:PDF},
  groups    = {Other deep learning methods, Additive attention over a grid of features, Multi-stage generation, attention taxonomy},
  isbn      = {978-1-57735-800-8},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {8},
}

@misc{Sugano2016SeeingW,
      title={Seeing with Humans: Gaze-Assisted Neural Image Captioning},
      author={Yusuke Sugano and Andreas Bulling},
      year={2016},
      eprint={1608.05203},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@Misc{cornia2018payingattentionsaliencyimageRegiony,
  author        = {Marcella Cornia and Lorenzo Baraldi and Giuseppe Serra and Rita Cucchiara},
  title         = {Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1706.08474},
  file          = {:PDF/2018_TOMM.pdf:PDF},
  groups        = {Exploiting human attention},
  pages         = {1},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1706.08474},
}

@InProceedings{Ramanishka2017TopDownRegiony,
  author    = {Ramanishka, Vasili and Das, Abir and Zhang, Jianming and Saenko, Kate},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Top-Down Visual Saliency Guided by Captions }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  pages     = {3136},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR.2017.334},
  file      = {:PDF/Top-Down_Visual_Saliency_Guided_by_Captions.pdf:PDF},
  groups    = {Exploiting human attention},
  issn      = {1063-6919},
  keywords  = {Visualization;Decoding;Computational modeling;Heating systems;Analytical models;Predictive models},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.334},
}

@Misc{Tavalkoyi2017PayingAttentionRegiony,
  author        = {Hamed R. Tavakoli and Rakshith Shetty and Ali Borji and Jorma Laaksonen},
  title         = {Paying Attention to Descriptions Generated by Image Captioning Models},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1704.07434},
  file          = {:PDF/1704.07434v3.pdf:PDF},
  pages         = {6},
  primaryclass  = {cs.CV},
}

@Article{RTAVAKOLI201710,
  author   = {Hamed {R. Tavakoli} and Ali Borji and Jorma Laaksonen and Esa Rahtu},
  journal  = {Neurocomputing},
  title    = {Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features},
  year     = {2017},
  issn     = {0925-2312},
  pages    = {10-18},
  volume   = {244},
  abstract = {This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, (1) the contextual information of a scene along with low-level visual cues modulates attention, (2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble’s members.},
DOI={10.1016/j.neucom.2017.03.018},
  file     = {:PDF/1-s2.0-S0925231217305131-main.pdf:PDF},
  keywords = {Visual attention, Saliency prediction, Fixation prediction, Inter-image similarity, Extreme learning machines},
}

@Article{HUANG2006489,
  author   = {Guang Huang and Qin-Yu Zhu and Chee-Kheong Siew},
  journal  = {Neurocomputing},
  title    = {Extreme learning machine: Theory and applications},
  year     = {2006},
  issn     = {0925-2312},
  note     = {Neural Networks},
  number   = {1},
  pages    = {489-501},
  volume   = {70},
  abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.},
  doi      = {10.1016/j.neucom.2005.12.126},
  file     = {:PDF/1-s2.0-S0925231206000385-main.pdf:PDF},
  keywords = {Feedforward neural networks, Back-propagation algorithm, Extreme learning machine, Support vector machine, Real-time learning, Random node},
}

@Misc{Tavalkoyi2017PayingAttentionIstotneRegiony,
  author        = {Hamed R. Tavakoli and Rakshith Shetty and Ali Borji and Jorma Laaksonen},
  title         = {Paying Attention to Descriptions Generated by Image Captioning Models},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1704.07434},
  file          = {:PDF/1704.07434v3.pdf:PDF},
  pages         = {6},
  primaryclass  = {cs.CV},
}

@InProceedings{Quin2019LookBack,
  author    = {Qin, Yu and Du, Jiajun and Zhang, Yonghua and Lu, Hongtao},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Look Back and Predict Forward in Image Captioning},
  year      = {2019},
  pages     = {8359-8367},
  abstract  = {Most existing attention-based methods on image captioning focus on the current word and visual information in one time step and generate the next word, without considering the visual and linguistic coherence. We propose Look Back (LB) method to embed visual information from the past and Predict Forward (PF) approach to look into future. LB method introduces attention value from the previous time step into the current attention generation to suit visual coherence of human. PF model predicts the next two words in one time step and jointly employs their probabilities for inference. Then the two approaches are combined together as LBPF to further integrate visual information from the past and linguistic information in the future to improve image captioning performance. All the three methods are applied on a classic base decoder, and show remarkable improvements on MSCOCO dataset with small increments on parameter counts. Our LBPF model achieves BLEU-4 / CIDEr / SPICE scores of 37.4 / 116.4 / 21.2 with cross-entropy loss and 38.3 / 127.6 / 22.0 with CIDEr optimization. Our three proposed methods can be easily applied on most attention-based encoder-decoder models for image captioning.},
  doi       = {10.1109/CVPR.2019.00856},
  file      = {:PDF/Look_Back_and_Predict_Forward_in_Image_Captioning.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Vision + Language;Deep Learning ; Image and Video Synthesis},
}

@InProceedings{Ke2019ReflectiveRegions,
  author    = {Ke, Lei and Pei, Wenjie and Li, Ruiyu and Shen, Xiaoyong and Tai, Yu-Wing},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Reflective Decoding Network for Image Captioning},
  year      = {2019},
  pages     = {8889},
  doi       = {10.1109/ICCV.2019.00898},
  file      = {:PDF/Reflective_Decoding_Network_for_Image_Captioning.pdf:PDF},
  groups    = {Attention Over Visual Regions, Reflective attention},
  keywords  = {Decoding;Visualization;Feature extraction;Syntactics;Task analysis;Rivers;Random access memory},
}

@Article{Wang2020ShowRecallRegions,
  author       = {Wang, Li and Bai, Zechen and Zhang, Yonghua and Lu, Hongtao},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Show, Recall, and Tell: Image Captioning with Recall Mechanism},
  year         = {2020},
  number       = {07},
  pages        = {12180},
  volume       = {34},
  abstractnote = {Generating natural and accurate descriptions in image captioning has always been a challenge. In this paper, we propose a novel recall mechanism to imitate the way human conduct captioning. There are three parts in our recall mechanism : recall unit, semantic guide (SG) and recalled-word slot (RWS). Recall unit is a text-retrieval module designed to retrieve recalled words for images. SG and RWS are designed for the best use of recalled words. SG branch can generate a recalled context, which can guide the process of generating caption. RWS branch is responsible for copying recalled words to the caption. Inspired by pointing mechanism in text summarization, we adopt a soft switch to balance the generated-word probabilities between SG and RWS. In the CIDEr optimization step, we also introduce an individual recalled-word reward (WR) to boost training. Our proposed methods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICE scores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 / 129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathy test split, which surpass the results of other state-of-the-art methods.},
  doi          = {10.1609/aaai.v34i07.6898},
  file         = {:PDF/6898-Article Text-10127-1-10-20200525.pdf:PDF},
  groups       = {Attention Over Visual Regions, Two-layer LSTM}
}

@InProceedings{Vinyals2015ShowEmpirically,
  author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Show and tell: A neural image caption generator},
  year      = {2015},
  pages     = {3159},
  abstract  = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  doi       = {10.1109/CVPR.2015.7298935},
  file      = {:PDF/Show_and_tell_A_neural_image_caption_generator.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Logic gates;Measurement;Training;Visualization;Recurrent neural networks;Google},
}



@Article{Lu2018NeuralBTVisualSentinel,
  author     = {Jiasen Lu and Jianwei Yang and Dhruv Batra and Devi Parikh},
  journal    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title      = {Neural Baby Talk},
  year       = {2018},
  pages      = {7223},
  file       = {:PDF/neural baby talk.pdf:PDF},
  groups     = {Visual sentinel, Two-layer LSTM},
  shorttitle = {Neural Baby ...},
}

@Article{Lu2018NeuralBTTemplates,
  author  = {Jiasen Lu and Jianwei Yang and Dhruv Batra and Devi Parikh},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title   = {Neural Baby Talk},
  year    = {2018},
  pages   = {7222},
  file    = {:PDF/neural baby talk.pdf:PDF},
  groups  = {Visual sentinel, Two-layer LSTM},
}

@Article{Cornia2019ShowCASterowanie,
  author  = {Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
  year    = {2019},
  pages   = {8303},
  comment = {https://github.com/aimagelab/show-control-and-tell},
  file    = {:PDF/Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_Grounded_Captions.pdf:PDF},
  groups  = {Visual sentinel},
}

@Article{Donahue2017LongTermGlobalFeatures,
  author   = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  year     = {2017},
  issn     = {1939-3539},
  number   = {4},
  pages    = {679},
  volume   = {39},
  abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.},
  doi      = {10.1109/TPAMI.2016.2599174},
  file     = {:PDF/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf:PDF},
  keywords = {Visualization;Computational modeling;Computer architecture;Data models;Logic gates;Predictive models;Recurrent neural networks;Computer vision;convolutional nets;deep learning;transfer learning},
}

@Article{Cornia2019ShowCAVisualSentinel,
  author     = {Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  journal    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title      = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
  year       = {2019},
  pages      = {8301},
  comment    = {https://github.com/aimagelab/show-control-and-tell},
  file       = {:PDF/Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_Grounded_Captions.pdf:PDF},
  groups     = {Visual sentinel},
  shorttitle = {Show, Control and Tell...},
}

@Article{Deng2020DensenetAdaptiveVisualSentinel,
  author     = {Zhenrong Deng and Zhouqin Jiang and Rushi Lan and Wenming Huang and Xiaonan Luo},
  journal    = {Signal Processing: Image Communication},
  title      = {Image captioning using DenseNet network and adaptive attention},
  year       = {2020},
  issn       = {0923-5965},
  pages      = {115836},
  volume     = {85},
  doi        = {https://doi.org/10.1016/j.image.2020.115836},
  file       = {:PDF/1-s2.0-S092359652030059X-main.pdf:PDF},
  keywords   = {Image captioning, DenseNet, LSTM, Adaptive attention mechanism},
  shorttitle = {Image captioning ...},
}

@misc{Luo2021DualLevelCTTransformator,
      title={Dual-Level Collaborative Transformer for Image Captioning},
      author={Yunpeng Luo and Jiayi Ji and Xiaoshuai Sun and Liujuan Cao and Yongjian Wu and Feiyue Huang and Chia-Wen Lin and Rongrong Ji},
      year={2021},
      eprint={2101.06462},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@InProceedings{Li2019EntangledSemantic,
  author    = {Li, Guang and Zhu, Linchao and Liu, Ping and Yang, Yi},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Entangled Transformer for Image Captioning},
  year      = {2019},
  pages     = {8930},
  doi       = {10.1109/ICCV.2019.00902},
  file      = {:PDF/Entangled_Transformer_for_Image_Captioning.pdf:PDF},
  groups    = {Early self-attention approaches, Gating mechanisms.},
  issn      = {2380-7504},
  keywords  = {Semantics;Visualization;Decoding;Encoding;Logic gates;Snow;Proposals},
}

@Article{goodfellow2017deep,
  author  = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  journal = {Cambridge Massachusetts},
  title   = {Deep learning (adaptive computation and machine learning series)},
  year    = {2017},
  pages   = {404},
  file    = {:PDF/deep-learning-adaptive-computation-and-machine-learning-series BY Ian Goodfellow.pdf:PDF},
}




@InBook{Anderson2013,
  author    = {Anderson, David Leech},
  editor    = {M{\"u}ller, Vincent C.},
  pages     = {321--333},
  publisher = {Springer Berlin Heidelberg},
  title     = {Machine Intentionality the Moral Status of Machines and the Composition Problem},
  year      = {2013},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-31674-6},
  doi       = {10.1007/978-3-642-31674-6_24},
  file      = {:PDF/978-3-642-31674-6.pdf:PDF},
  groups    = {wstep},
  url       = {https://doi.org/10.1007/978-3-642-31674-6_24},
}

@InProceedings{Biggs2021WittPictureInv,
  title={Wittgenstein’s Picture-Investigations},
  author={Biggs, Michael AR},
  booktitle={International Conference on Theory and Application of Diagrams},
  pages={103--117},
  year={2021},
  organization={Springer International Publishing},
  file      = {:PDF/preprint_unformatted_27_ (1).pdf:PDF},
  groups    = {wstep}
}

@book{Burzynska2006teorie,
  author    = {Burzy\'{n}ska, Anna and Markowski, Micha\l{} Pawe\l{}},
  title     = {Teorie literatury XX wieku: podr\k{e}cznik},
  publisher = {Znak},
  year      = {2006},
  address   = {Krak\'{o}w},
pages={231-279},
  isbn      = {83-240-0737-7}
}

@Article{Caldas2023MachineInternationalities,
  author = {Caldas Vianna, Bruno},
  title  = {Machine intentionalities},
  year   = {2023},
  month  = {01},
  file   = {:PDF/Machine_Intentionalities.pdf:PDF},
  groups = {wstep},
}

@Article{Dreyfus1996What,
  author  = {Dreyfus, Hubert},
  journal = {Artif. Intell.},
  title   = {What Computers Still Can't Do.},
  year    = {1996},
  month   = {01},
  pages   = {143-150},
  volume  = {80},
  doi     = {10.1016/0004-3702(95)00086-0},
  groups  = {wstep},
}

@Book{Floridi2011philosophy,
  author    = {Floridi, L.},
  publisher = {OUP Oxford},
  title     = {The Philosophy of Information},
  year      = {2011},
  isbn      = {9780199232383},
  file      = {:PDF/[Luciano_Floridi]_The_Philosophy_of_Information-0199232385.pdf:PDF},
  groups    = {wstep},
  lccn      = {2010940315},
  url       = {https://books.google.pl/books?id=4d0TDAAAQBAJ},
}

@Article{Gubelmann2023LooselyWittgenstein,
  author  = {Gubelmann, Reto},
  journal = {Grazer Philosophische Studien},
  title   = {A Loosely Wittgensteinian Conception of the Linguistic Understanding of Large Language Models like BERT, GPT-3, and ChatGPT},
  year    = {2023},
  month   = {04},
  pages   = {485-523},
  volume  = {99},
  doi     = {10.1163/18756735-00000182},
  file    = {:PDF/paper_understanding_manuscript_grazer_postacc_prelayout.pdf:PDF},
  groups  = {wstep},
}

@Misc{Henighan2020scalinglawsautoregressivegenerative,
  author        = {Tom Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish},
  title         = {Scaling Laws for Autoregressive Generative Modeling},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2010.14701},
  file          = {:PDF/2010.14701v2.pdf:PDF},
  groups        = {wstep},
  primaryclass  = {cs.LG},
}

@Book{Ingarden1970Studia,
  author    = {Ingarden, Roman},
  publisher = {Państwowe Wydawnictwo Naukowe},
  title     = {Studia z estetyki. T. 3},
  year      = {1970},
  address   = {Warszawa},
  series    = {Dzieła filozoficzne},
  booktitle = {Studia z estetyki. T. 3},
  groups    = {wstep},
  keywords  = {Estetyka -- teoria},
  language  = {pol},
  pages     = {18-27},
}



@article{Kaplan2020ScalingLF,
 author = {Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
 file = {:PDF/2001.08361v1.pdf:PDF},
 groups = {wstep},
 journal = {ArXiv},
 title = {Scaling Laws for Neural Language Models},
 url = {https://api.semanticscholar.org/CorpusID:210861095},
 volume = {abs/2001.08361},
 year = {2020}
}

@Book{Kockelman2017art,
  author    = {Kockelman, P.},
  publisher = {Oxford University Press},
  title     = {The Art of Interpretation in the Age of Computation},
  year      = {2017},
  isbn      = {9780190636531},
  file      = {:PDF/AIAC.pdf:PDF},
  groups    = {wstep},
  lccn      = {2016032470},
  url       = {https://books.google.pl/books?id=8TYlDwAAQBAJ},
}

@Book{Lacey2002dictionary,
  author    = {Alan Robert Lacey},
  publisher = {Routledge \& Kegan Paul},
  title     = {A Dictionary of Philosophy},
  year      = {1986},
  address   = {London},
  file      = {:PDF/1539210339 - [Alan_Lacey]_A_Dictionary_Of_Philosophy(Bookos.org).pdf:PDF},
  groups    = {wstep}}

@Article{Liu2021WittgensteininML,
  author  = {Liu, Lydia},
  journal = {Critical Inquiry},
  title   = {Wittgenstein in the Machine},
  year    = {2021},
  month   = {03},
  pages   = {425-455},
  volume  = {47},
  doi     = {10.1086/713551},
  file    = {:PDF/liu-2021-wittgenstein-in-the-machine.pdf:PDF},
  groups  = {wstep},
}

@Book{Ryan2015narrative,
  author    = {Ryan, M.L.},
  publisher = {Johns Hopkins University Press},
  title     = {Narrative as Virtual Reality 2: Revisiting Immersion and Interactivity in Literature and Electronic Media},
  year      = {2015},
  isbn      = {9781421417981},
  series    = {Book collections on Project MUSE},
  file      = {:PDF/revayat-vagheiyate-majazi.pdf:PDF},
  groups    = {wstep},
  url       = {https://books.google.pl/books?id=2KHYCgAAQBAJ},
}

@Book{Shanker1998wittgenstein,
  author    = {Shanker, S.},
  publisher = {Routledge},
  title     = {Wittgenstein's Remarks on the Foundations of AI},
  year      = {1998},
  isbn      = {9780415097949},
  file      = {:PDF/10.4324_9780203049020_previewpdf.pdf:PDF},
  groups    = {wstep},
  lccn      = {97013425},
  url       = {https://books.google.pl/books?id=jdcbJKid_7wC},
}

@Book{Eco2008Interpretacja,
  author     = {Eco, Umberto and Bieroń, Tomasz and Collini, Stefan and Rorty, Richard},
  publisher  = {Społeczny Instytut Wydawniczy Znak},
  title      = {Interpretacja i nadinterpretacja},
  year       = {2008},
  address    = {Kraków},
  edition    = {Wyd. 2.},
  isbn       = {9788324010172},
  booktitle  = {Interpretacja i nadinterpretacja},
  file       = {:PDF/U.-Eco-Interpretacja-i-nadinterpretacja.pdf:PDF},
  groups     = {wstep},
  keywords   = {Krytyka literacka -- metody ; Teoria literatury ; Semiotics and literature ; Criticism},
  language   = {pol},
  translator = {Weinsberg, Tomasz},
}

@Book{Umberto2009Semiotyka,
  author    = {Eco, Umberto and Czerwiński, Maciej and Wydawnictwo Uniwersytetu Jagiellońskiego},
  publisher = {Wydawnictwo Uniwersytetu Jagiellońskiego},
  title     = {Teoria semiotyki / Umberto Eco ; przekł. Maciej Czerwiński.},
  year      = {2009},
  address   = {Kraków},
  isbn      = {9788323327431},
  series    = {Eidos},
  booktitle = {Teoria semiotyki},
  file      = {:PDF/Teoria semiotyki Eco Umberto e-book.pdf:PDF;:PDF/U. ECO - TEORIA SEMIOTYKI.pdf:PDF},
  groups    = {wstep},
  keywords  = {Semiotyka ; Semiotics},
  language  = {pol},
}


@Article{Wittgenstein1997traktat,
  author  = {Wittgenstein, Ludwig},
  journal = {Wydawnictwo Naukowe PWN},
  title   = {Traktat logiczno-filozoficzny},
  year    = {1997},
  pages   = {9},
  file    = {:PDF/filozofia-ludwig-wittgenstein-traktatus-logico-philosophicus.pdf:PDF},
  groups  = {wstep},
}

@Misc{Zeng2020intrinsicimagecaptioningevaluation,
  author        = {Chao Zeng and Sam Kwong},
  title         = {Intrinsic Image Captioning Evaluation},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2012.07333},
  file          = {:PDF/2012.07333v1 (1).pdf:PDF},
  groups        = {wstep},
  primaryclass  = {cs.CV},
}

@Article{Buschman2007TopBottomrohtua,
  author   = {Timothy J. Buschman and Earl K. Miller},
  journal  = {Science},
  title    = {Top-Down Versus Bottom-Up Control of Attention in the Prefrontal and Posterior Parietal Cortices},
  year     = {2007},
  number   = {5820},
  pages    = {1860-1862},
  volume   = {315},
  abstract = {Attention can be focused volitionally by “top-down” signals derived from task demands and automatically by “bottom-up” signals from salient stimuli. The frontal and parietal cortices are involved, but their neural activity has not been directly compared. Therefore, we recorded from them simultaneously in monkeys. Prefrontal neurons reflected the target location first during top-down attention, whereas parietal neurons signaled it earlier during bottom-up attention. Synchrony between frontal and parietal areas was stronger in lower frequencies during top-down attention and in higher frequencies during bottom-up attention. This result indicates that top-down and bottom-up signals arise from the frontal and sensory cortex, respectively, and different modes of attention may emphasize synchrony at different frequencies.},
  doi      = {10.1126/science.1138071},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.1138071},
  file     = {:PDF/science.1138071.pdf:PDF},
  groups   = {topBottom},
  url      = {https://www.science.org/doi/abs/10.1126/science.1138071},
}

@Article{Pinto2013BottomTop,
  author   = {Pinto, Yair and van der Leij, Andries R. and Sligte, Ilja G. and Lamme, Victor A. F. and Scholte, H. Steven},
  journal  = {Journal of Vision},
  title    = {Bottom-up and top-down attention are independent},
  year     = {2013},
  issn     = {1534-7362},
  month    = {07},
  number   = {3},
  pages    = {16-16},
  volume   = {13},
  abstract = {What is the relationship between top-down and bottom-up attention? Are both types of attention tightly interconnected, or are they independent? We investigated this by testing a large representative sample of the Dutch population on two attentional tasks: a visual search task gauging the efficiency of top-down attention and a singleton capture task gauging bottom-up attention. On both tasks we found typical performance—i.e., participants displayed a significant search slope on the search task and significant slowing caused by the unique, but irrelevant, object on the capture task. Moreover, the high levels of significance we observed indicate that the current set-up provided very high signal to noise ratios, and thus enough power to accurately unveil existing effects. Importantly, in this robust investigation we did not observe any correlation in performance between tasks. The use of Bayesian statistics strongly confirmed that performance on both tasks was uncorrelated. We argue that the current results suggest that there are two attentional systems that operate independently. We hypothesize that this may have implications beyond our understanding of attention. For instance, it may be that attention and consciousness are intertwined differently for top-down attention than for bottom-up attention.},
  doi      = {10.1167/13.3.16},
  eprint   = {https://arvojournals.org/arvo/content\_public/journal/jov/933551/i1534-7362-13-3-16.pdf},
  file     = {:PDF/i1534-7362-13-3-16.pdf:PDF},
  groups   = {topBottom},
  url      = {https://doi.org/10.1167/13.3.16},
}

@Article{Corbetta2002TopBottom,
  author  = {Corbetta, Maurizio and Shulman, Gordon},
  journal = {Nature reviews. Neuroscience},
  title   = {Control of Goal-Directed and Stimulus-Driven Attention in the Brain},
  year    = {2002},
  month   = {04},
  pages   = {201-15},
  volume  = {3},
  doi     = {10.1038/nrn755},
  file    = {:PDF/corbetta.pdf:PDF},
  groups  = {topBottom},
}

@Article{Egly1994topBottom,
  author  = {Egly, Robert and Driver, Jon and Rafal, Robert},
  journal = {Journal of experimental psychology. General},
  title   = {Shifting Visual Attention Between Objects and Locations: Evidence From Normal and Parietal Lesion Subjects},
  year    = {1994},
  month   = {06},
  pages   = {161-77},
  volume  = {123},
  doi     = {10.1037/0096-3445.123.2.161},
  file    = {:PDF/Egly_etal_94_JEPG_ShiftingVisualAttention.pdf:PDF},
  groups  = {topBottom},
}

@Article{Egly1994,
  author  = {Egly, Robert and Driver, Jon and Rafal, Robert},
  journal = {Journal of experimental psychology. General},
  title   = {Shifting Visual Attention Between Objects and Locations: Evidence From Normal and Parietal Lesion Subjects},
  year    = {1994},
  month   = {06},
  pages   = {161-77},
  volume  = {123},
  doi     = {10.1037/0096-3445.123.2.161},
}

@Article{Borji2013State,
  author   = {Borji, Ali and Itti, Laurent},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {State-of-the-Art in Visual Attention Modeling},
  year     = {2013},
  issn     = {1939-3539},
  month    = {Jan},
  number   = {1},
  pages    = {185-207},
  volume   = {35},
  abstract = {Modeling visual attention-particularly stimulus-driven, saliency-based attention-has been a very active research area over the past 25 years. Many different models of attention are now available which, aside from lending theoretical contributions to other fields, have demonstrated successful applications in computer vision, mobile robotics, and cognitive systems. Here we review, from a computational perspective, the basic concepts of attention implemented in these models. We present a taxonomy of nearly 65 models, which provides a critical comparison of approaches, their capabilities, and shortcomings. In particular, 13 criteria derived from behavioral and computational studies are formulated for qualitative comparison of attention models. Furthermore, we address several challenging issues with models, including biological plausibility of the computations, correlation with eye movement datasets, bottom-up and top-down dissociation, and constructing meaningful performance measures. Finally, we highlight current research trends in attention modeling and provide insights for future.},
  doi      = {10.1109/TPAMI.2012.89},
  file     = {:PDF/State-of-the-Art_in_Visual_Attention_Modeling.pdf:PDF},
  groups   = {topBottom},
  keywords = {Computational modeling;Visualization;Hidden Markov models;Feature extraction;Humans;Solid modeling;Search problems;Visual attention;bottom-up attention;top-down attention;saliency;eye movements;regions of interest;gaze control;scene interpretation;visual search;gist},
}

@Article{Zhao2024RegionFeatures,
  author   = {Kai Zhao and Wei Xiong},
  journal  = {International Journal of Applied Earth Observation and Geoinformation},
  title    = {Exploring region features in remote sensing image captioning},
  year     = {2024},
  issn     = {1569-8432},
  pages    = {103672},
  volume   = {127},
  abstract = {Remote sensing image captioning (RSIC), an emerging field of cross-modal tasks, has become a popular research topic in recent years. Feature extraction underlies all RSIC tasks, with current tasks using grid features. Compared with grid features, region features provide object-level location-related information; however, these features have not been considered in the RSIC tasks. Therefore, this study examined the performance of region features on RSIC tasks. We generated region annotations based on published RSIC datasets to address the need for region-related datasets. We extracted region features according to the labeled data and proposed a Region Attention Transformer model. To solve the information loss problem owing to the region of interest pooling during region feature extraction, we proposed region-grid features and used geometry relationships for estimating correlations between different region features. We compared the performances of the models using grid and region features. The results showed that region features performed well in RSIC tasks, and region features forced the model to pay more attention to object regions when generating object-related words. This study describes a novel method of using features in RSIC tasks. Our region annotations are available at https://github.com/zk-1019/exploring.},
  doi      = {10.1016/j.jag.2024.103672},
  file     = {:PDF/1-s2.0-S1569843224000268-main.pdf:PDF},
  groups   = {features},
  keywords = {Transformer model, Image processing, Model training, Remote sensing image captioning},
}

@Article{Yan2024GridFeatures,
  author   = {Yan, Jie and Xie, Yuxiang and Guo, Yanming and Wei, Yingmei and Luan, Xidao},
  journal  = {Complex {\&} Intelligent Systems},
  title    = {Exploring better image captioning with grid features},
  year     = {2024},
  issn     = {2198-6053},
  number   = {3},
  pages    = {3541-3556},
  volume   = {10},
  abstract = {Nowadays, Artificial Intelligence Generated Content (AIGC) has shown promising prospects in both computer vision and natural language processing communities. Meanwhile, as an essential aspect of AIGC, image to captions has received much more attention. Recent vision-language research is developing from the bulky region visual representations based on object detectors toward more convenient and flexible grid ones. However, this kind of research typically concentrates on image understanding tasks like image classification, with less attention paid to content generation tasks. In this paper, we explore how to capitalize on the expressive features embedded in the grid visual representations for better image captioning. To this end, we present a Transformer-based image captioning model, dubbed FeiM, with two straightforward yet effective designs. We first design the feature queries that consist of a limited set of learnable vectors, which act as the local signals to capture specific visual information from global grid features. Then, taking augmented global grid features and the local feature queries as inputs, we develop a feature interaction module to query relevant visual concepts from grid features, and to enable interaction between the local signal and overall context. Finally, the refined grid visual representations and the linguistic features pass through a Transformer architecture for multi-modal fusion. With the two novel and simple designs, FeiM can fully leverage meaningful visual knowledge to improve image captioning performance. Extensive experiments are performed on the competitive MSCOCO benchmark to confirm the effectiveness of the proposed approach, and the results show that FeiM yields more eminent results than existing advanced captioning models.},
  day      = {01},
  doi      = {10.1007/s40747-023-01341-8},
  file     = {:PDF/s40747-023-01341-8.pdf:PDF},
  groups   = {features},
}

@Article{lipton2015criticalreviewrecurrentneural,
  author        = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  title         = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  year          = {2015},
  archiveprefix = {arXiv},
  eprint        = {1506.00019},
  file          = {:PDF/1506.00019v4.pdf:PDF},
  primaryclass  = {cs.LG},
}

@Book{Eco1972Pejzaz,
  author     = {Eco, Umberto and Weinsberg, Adam and Czerwiński, Marcin},
  publisher  = {Państwowy Instytut Wydawniczy},
  title      = {Pejzaż semiotyczny},
  year       = {1972},
  address    = {Warszawa},
  series     = {Biblioteka Myśli Współczesnej},
  booktitle  = {Pejzaż semiotyczny},
  groups     = {wstep},
  keywords   = {Semiotyka i kultura ; Semantics (Philosophy)},
  language   = {pol},
  translator = {Weinsberg, Adam},
}

@Misc{ZestawienieCNN,
  author       = {PyTorch Team},
  howpublished = {\url{https://pytorch.org/vision/main/models.html}},
  note         = {Accessed: 2025-09-10},
  title        = {Torchvision Models},
  year         = {2025},
}

@Book{Sartre1998TechniczyObraz,
  author     = {Sartre, Jean and Elkaïm-Sartre, Arlette and Krajewski, Janusz and Szeżyńska-Maćkowiak, Krystyna},
  publisher  = {Muza},
  title      = {Egzystencjalizm jest humanizmem.},
  year       = {1998},
  address    = {Warszawa},
  isbn       = {8372001049},
  series     = {Spectrum},
  booktitle  = {Egzystencjalizm jest humanizmem},
  file       = {:PDF/Sartre_J_P_Egzystencjalizm jest humanizmem.pdf:PDF},
  keywords   = {Egzystencjalizm ; Existentialism},
  language   = {pol},
  pages      = {24},
  translator = {Janusz Krajewski},
}

@Misc{openai2023dalle3,
  author = {OpenAI},
  note   = {Accessed 2024.07.12},
  title  = {DALL·E 3 System Card},
  year   = {2023},
  url    = {https://openai.com/research/dall-e-3-system-card},
}

@Article{Lin2023EmbeddedAI,
  author   = {Lin, Hsiao},
  journal  = {Computer},
  title    = {Embedded Artificial Intelligence: Intelligence on Devices},
  year     = {2023},
  issn     = {1558-0814},
  number   = {9},
  pages    = {90-93},
  volume   = {56},
  abstract = {Embedded artificial intelligence (AI) seamlessly integrates AI into everyday devices. By bringing AI closer to the data source, embedded AI empowers real-time decision making, enhanced efficiency, and new possibilities across diverse domains.},
  doi      = {10.1109/MC.2023.3280397},
  file     = {:PDF/10224582.pdf:PDF},
  keywords = {Artificial intelligence;Embedded systems;Intelligent systems;Real-time systems;Decision making},
}

@Article{Lu2020DyingRelu,
  author   = {Lu, Lu and Yeonjong, Shin and Yanhui, Su and Karniadakis, George, Em},
  journal  = {Communications in Computational Physics},
  title    = {Dying ReLU and Initialization: Theory and Numerical Examples},
  year     = {2020},
  issn     = {1991-7120},
  number   = {5},
  pages    = {1671--1706},
  volume   = {28},
  abstract = {<p style="text-align: justify;">The dying ReLU refers to the problem when ReLU neurons become inactive
and only output 0 for any input. There are many empirical and heuristic explanations
of why ReLU neurons die. However, little is known about its theoretical analysis. In
this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate
the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric
probability distributions, which suffers from the dying ReLU. We thus propose a new
initialization procedure, namely, a randomized asymmetric initialization. We show
that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are
provided to demonstrate the effectiveness of the new initialization procedure.</p>},
  doi      = {https://doi.org/10.4208/cicp.OA-2020-0165},
  url      = {https://global-sci.com/article/79737/dying-relu-and-initialization-theory-and-numerical-examples},
}

@InProceedings{Sutskever2013SGD,
  author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  title     = {On the importance of initialization and momentum in deep learning},
  year      = {2013},
  address   = {Atlanta, Georgia, USA},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  number    = {3},
  pages     = {1139--1147},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {28},
  abstract  = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
  file      = {:PDF/sutskever13.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v28/sutskever13.pdf},
}

@Article{Graves2013Graves,
  author     = {Alex Graves},
  journal    = {CoRR},
  title      = {Generating Sequences With Recurrent Neural Networks},
  year       = {2013},
  volume     = {abs/1308.0850},
  biburl     = {https://dblp.org/rec/journals/corr/Graves13.bib},
  eprint     = {1308.0850},
  eprinttype = {arXiv},
  file       = {:PDF/1308.0850v5.pdf:PDF},
}

@Article{Rao2017brief,
  author  = {Rao, V Chandra Sekhar},
  journal = {Journal for Research Scholars and Professionals of English Language Teaching},
  title   = {A brief study of words used in denotation and connotation},
  year    = {2017},
  number  = {1},
  pages   = {1--5},
  volume  = {1},
  file    = {:PDF/A_Brief_Study_of_Words_Used_in_Denotatio.pdf:PDF},
}


@Book{Winograd1986understanding,
  author    = {Winograd, T. and Flores, F.},
  publisher = {Ablex Publishing Corporation},
  title     = {Understanding Computers and Cognition: A New Foundation for Design},
  year      = {1986},
  isbn      = {9780893910501},
  series    = {Language and being},
  file      = {:PDF/Understanding Computers and Cognition_A New Foundation for Design - Terry Winograd.pdf:PDF},
  groups    = {wstep, datasets},
  lccn      = {lc85003856},
  url       = {https://books.google.pl/books?id=2sRC8vcDYNEC},
}

@InProceedings{matsuda2024deneb,
  author    = {Kazuki Matsuda and Yuiga Wada and Komei Sugiura},
  booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
  title     = {DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning},
  year      = {2024},
  pages     = {3570--3586},
  file      = {:PDF/2409.19255v2.pdf:PDF},
  groups    = {datasets},
}

@InProceedings{wada2024,
  author    = {Wada, Yuiga and Kaneda, Kanta and Saito, Daichi and Sugiura, Komei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {{Polos: Multimodal Metric Learning from Human Feedback for Image Captioning}},
  year      = {2024},
  file      = {:PDF/2402.18091v1.pdf:PDF},
  groups    = {datasets},
}

@InProceedings{zeng2024hicescore,
  author    = {Zeng, Zequn and Sun, Jianqiao and Zhang, Hao and Wen, Tiansheng and Su, Yudi and Xie, Yan and Wang, Zhengjue and Chen, Bo},
  booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
  title     = {Hicescore: A hierarchical metric for image captioning evaluation},
  year      = {2024},
  pages     = {866--875},
  groups    = {metryki},
}

@Article{kim2025expert,
  author  = {Kim, Hyunjong and Kim, Sangyeop and Jeong, Jongheon and Cho, Yeongjae and Cho, Sungzoon},
  journal = {arXiv preprint arXiv:2506.24016},
  title   = {EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations},
  year    = {2025},
  file    = {:PDF/2025.findings-acl.1367.pdf:PDF},
  groups  = {metryki},
}

@Article{mccaffreyimage,
  author = {McCaffrey, Ryan and Karakozis, Ioannis Christos},
  title  = {Image Caption Validation},
  file   = {:PDF/COS598B_spr2018_ImageCaptionValidation.pdf:PDF},
  groups = {metryki},
}

@InProceedings{yu2023rome,
  author    = {Yu, Boxi and Zhong, Zhiqing and Li, Jiaqi and Yang, Yixing and He, Shilin and He, Pinjia},
  booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {ROME: Testing image captioning systems via recursive object melting},
  year      = {2023},
  pages     = {766--778},
  groups    = {metryki},
}

@Article{sarto2025image,
  author  = {Sarto, Sara and Cornia, Marcella and Cucchiara, Rita},
  journal = {arXiv preprint arXiv:2503.14604},
  title   = {Image captioning evaluation in the age of multimodal llms: Challenges and future perspectives},
  year    = {2025},
  file    = {:PDF/2503.14604v2.pdf:PDF},
  groups  = {metryki},
}

@Article{berger2025improving,
  author  = {Berger, Uri and Abend, Omri and Frermann, Lea and Stanovsky, Gabriel},
  journal = {arXiv preprint arXiv:2501.04513},
  title   = {Improving image captioning by mimicking human reformulation feedback at inference-time},
  year    = {2025},
  file    = {:PDF/2501.04513v1.pdf:PDF},
  groups  = {metryki},
}

@Article{kasai2021transparent,
  author  = {Kasai, Jungo and Sakaguchi, Keisuke and Dunagan, Lavinia and Morrison, Jacob and Bras, Ronan Le and Choi, Yejin and Smith, Noah A},
  journal = {arXiv preprint arXiv:2111.08940},
  title   = {Transparent human evaluation for image captioning},
  year    = {2021},
  file    = {:PDF/2111.08940v2.pdf:PDF},
  groups  = {metryki},
}

@Article{Sarhan2023Understanding,
  author    = {Sarhan, Habiba and Hegelich, Simon},
  journal   = {Frontiers in Political Science},
  title     = {Understanding and evaluating harms of AI-generated image captions in political images},
  year      = {2023},
  pages     = {1245684},
  volume    = {5},
  file      = {:PDF/fpos-05-1245684.pdf:PDF},
  groups    = {metryki},
  publisher = {Frontiers Media SA},
}

@InProceedings{Yu2022Automated,
  author    = {Yu, Boxi and Zhong, Zhiqing and Qin, Xinran and Yao, Jiayi and Wang, Yuancheng and He, Pinjia},
  booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
  title     = {Automated testing of image captioning systems},
  year      = {2022},
  address   = {New York, NY, USA},
  pages     = {467–479},
  publisher = {Association for Computing Machinery},
  series    = {ISSTA 2022},
  doi       = {10.1145/3533767.3534389},
  file      = {:PDF/ISSTA22a.pdf:PDF},
  groups    = {metryki},
  isbn      = {9781450393799},
  keywords  = {AI software, Metamorphic testing, image captioning, testing},
  location  = {Virtual, South Korea},
  numpages  = {13},
}

@Article{vazquez2024taxonomy,
  author  = {V{\'a}zquez, Adriana Fern{\'a}ndez de Caleya and Garrido-Merch{\'a}n, Eduardo C},
  journal = {arXiv preprint arXiv:2407.01556},
  title   = {A Taxonomy of the Biases of the Images created by Generative Artificial Intelligence},
  year    = {2024},
  file    = {:PDF/2407.01556v1.pdf:PDF},
  groups  = {metryki},
}

@InProceedings{Lee2025Toward,
  author    = {Lee, Saehyung and Yoon, Seunghyun and Bui, Trung and Shi, Jing and Yoon, Sungroh},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  title     = {Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage},
  year      = {2025},
  editor    = {Singh, Aarti and Fazel, Maryam and Hsu, Daniel and Lacoste-Julien, Simon and Berkenkamp, Felix and Maharaj, Tegan and Wagstaff, Kiri and Zhu, Jerry},
  pages     = {33815--33832},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {267},
  file      = {:PDF/lee25aj.pdf:PDF},
  groups    = {metryki},
  pdf       = {https://raw.githubusercontent.com/mlresearch/v267/main/assets/lee25aj/lee25aj.pdf},
}

@Article{Zhao2024Semantic,
  author         = {Zhao, Fengzhi and Yu, Zhezhou and Wang, Tao and Lv, Yi},
  journal        = {Entropy},
  title          = {Image Captioning Based on Semantic Scenes},
  year           = {2024},
  issn           = {1099-4300},
  number         = {10},
  volume         = {26},
  article-number = {876},
  doi            = {10.3390/e26100876},
  file           = {:PDF/entropy-26-00876-v2.pdf:PDF},
  pubmedid       = {39451952},
}

@Article{Khan2025Fault,
  author         = {Khan, Abdul Saboor and Abbass, Muhammad Jamshed and Khan, Abdul Haseeb},
  journal        = {Sensors},
  title          = {Towards Fault Aware Image Captioning: A Review on Integrating Facial Expression Recognition (FER) and Object Detection},
  year           = {2025},
  issn           = {1424-8220},
  number         = {19},
  volume         = {25},
  article-number = {5992},
  doi            = {10.3390/s25195992},
  file           = {:PDF/sensors-25-05992.pdf:PDF},
  groups         = {metryki},
  pubmedid       = {41094815},
}

@Article{Mazur2024Poprawnosc,
  author  = {Mazur, Rafał},
  journal = {LingVaria},
  title   = {O poprawności językowej tekstów generowanych przez SI na przykładzie ChatuGPT},
  year    = {2024},
  number  = {1(37)},
  pages   = {119–138},
  volume  = {19},
  doi     = {10.12797/LV.19.2024.37.08},
  file    = {:PDF/09-Mazur-O-poprawnosci-jezykowej-tekstow-generowanych-przez-SI-na-przykladzie-chatuGPT.pdf:PDF},
  groups  = {bledy w SI},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:embeddings\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:inception\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:relations\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:graf\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:resnet\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:review\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:wazne_wazne\;0\;1\;0x7c7c7cff\;\;\;;
1 StaticGroup:vgg16\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:word2vec\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:transformer\;0\;1\;\;\;\;;
1 StaticGroup:datasets\;0\;1\;\;\;\;;
1 StaticGroup:metryki\;0\;1\;\;\;\;;
1 StaticGroup:literature review 2\;0\;1\;\;\;\;;
2 StaticGroup:retrieval\;0\;0\;\;\;\;;
2 StaticGroup:template\;0\;1\;\;\;\;;
2 StaticGroup:Encoding\;0\;1\;\;\;\;;
3 StaticGroup:global CNN features\;0\;1\;\;\;\;;
3 StaticGroup:Attention Over Grid of CNN Features\;0\;1\;\;\;\;;
4 StaticGroup:Additive attention over a grid of features\;0\;0\;\;\;\;;
4 StaticGroup:Exploiting human attention\;0\;1\;\;\;\;;
4 StaticGroup:Multi-level features\;0\;0\;\;\;\;;
4 StaticGroup:Review networks\;0\;1\;\;\;\;;
3 StaticGroup:Attention Over Visual Regions\;0\;0\;\;\;\;;
4 StaticGroup:Visual Policy\;0\;1\;\;\;\;;
4 StaticGroup:Geometric Transforms\;0\;1\;\;\;\;;
3 StaticGroup:Graph-based Encoding\;0\;0\;\;\;\;;
4 StaticGroup:Spatial and semantic graphs.\;0\;1\;\;\;\;;
4 StaticGroup:Scene graphs\;0\;1\;\;\;\;;
4 StaticGroup:Hierarchical trees\;0\;1\;\;\;\;;
3 StaticGroup:Self-Attention Encoding\;0\;0\;\;\;\;;
4 StaticGroup:Early self-attention approaches\;0\;0\;\;\;\;;
4 StaticGroup:Other\;0\;1\;\;\;\;;
4 StaticGroup:Vision Transformer.\;0\;0\;\;\;\;;
4 StaticGroup:Early fusion and vision-and-language pre-training.\;0\;0\;\;\;\;;
2 StaticGroup:Language Model\;0\;1\;\;\;\;;
3 StaticGroup:LSTM\;0\;1\;\;\;\;;
4 StaticGroup:single layer LSTM\;0\;0\;\;\;\;;
5 StaticGroup:Visual sentinel\;0\;1\;\;\;\;;
5 StaticGroup:hidden state reconstruction\;0\;0\;\;\;\;;
5 StaticGroup:Multi-stage generation\;0\;1\;\;\;\;;
4 StaticGroup:Two-layer LSTM\;0\;0\;\;\;\;;
5 StaticGroup:Two-layers and additive attention\;0\;1\;\;\;\;;
5 StaticGroup:Reflective attention\;0\;1\;\;\;\;;
4 StaticGroup:Boosting LSTM with Self-Attention\;0\;0\;\;\;\;;
4 StaticGroup:Neural Architecture Search for RNN\;0\;1\;\;\;\;;
3 StaticGroup:CNN language models\;0\;0\;\;\;\;;
3 StaticGroup:Transformer\;0\;0\;\;\;\;;
4 StaticGroup:Gating mechanisms.\;0\;1\;\;\;\;;
3 StaticGroup:BERT\;0\;1\;\;\;\;;
3 StaticGroup:Non-autoregressive Language Models\;0\;1\;\;\;\;;
2 StaticGroup:attention taxonomy\;0\;0\;\;\;\;;
1 StaticGroup:bez atencji\;0\;0\;\;\;\;;
1 StaticGroup:literature review\;0\;0\;\;\;\;;
2 StaticGroup:Earlier Deep Models\;0\;1\;\;\;\;;
2 StaticGroup:multimodal\;0\;1\;\;\;\;;
2 StaticGroup:encoder-decoder-lit\;0\;1\;\;\;\;;
2 StaticGroup:Compositional architectures\;0\;0\;\;\;\;;
2 StaticGroup:Describing novel objects\;0\;1\;\;\;\;;
2 StaticGroup:Other deep learning methods\;0\;0\;\;\;\;;
1 StaticGroup:opis polskiego\;0\;0\;\;\;\;;
1 StaticGroup:bledy w SI\;0\;0\;\;\;\;;
1 StaticGroup:representation learning\;0\;0\;\;\;\;;
1 StaticGroup:moje\;0\;1\;\;\;\;;
1 StaticGroup:Probabilistic Neural Network Language Model\;0\;0\;\;\;\;;
1 StaticGroup:rnn\;0\;1\;\;\;\;;
2 StaticGroup:lstm\;0\;1\;\;\;\;;
1 StaticGroup:cnn\;0\;1\;\;\;\;;
1 StaticGroup:mix\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:beam\;0\;1\;\;\;\;;
1 StaticGroup:preprocessing\;0\;1\;\;\;\;;
1 StaticGroup:lm\;0\;1\;\;\;\;;
1 StaticGroup:attention\;0\;1\;\;\;\;;
1 StaticGroup:negacja\;0\;1\;\;\;\;;
1 StaticGroup:wstep\;0\;0\;\;\;\;;
1 StaticGroup:zakonczenie\;0\;1\;\;\;\;;
1 StaticGroup:topBottom\;0\;0\;\;\;\;;
1 StaticGroup:features\;0\;1\;\;\;\;;
}
