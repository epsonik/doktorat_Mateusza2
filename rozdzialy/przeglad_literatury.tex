\chapter{Przegląd literatury}
Nadrzędnym celem automatycznego generowania podpisów do obrazów jest stworzenie systemu zdolnego do wygenerowania trafnego i spójnego językowo podpisu dla danej sceny wizualnej. Wymaga to pokonania tzw. luki semantycznej - rozbieżności między niskopoziomową, numeryczną reprezentacją obrazu a wysokopoziomową, symboliczną strukturą języka naturalnego.

Realizacja tego celu napotyka na szereg  wyzwań. Podstawową trudnością jest inherentna subiektywność i wieloznaczność interpretacji wizualnej, gdzie identyfikacja istotnych elementów sceny może mieć wiele wariantów. System musi zatem radzić sobie z tą wieloznacznością, generując podpisy uwzględniające najbardziej prawdopodobne interpretacje. Ponadto specyfika zastosowania często wymaga dostosowania stylu i kontekstu podpisów -- podpis do obrazu w encyklopedii będzie różnił się od podpisu tego samego obrazu w mediach społecznościowych. Wyzwaniem jest więc generowanie podpisów, które są nie tylko gramatycznie poprawne, ale także precyzyjne, zwięzłe i adekwatne do treści obrazu, przy jednoczesnym zachowaniu spójności językowej oraz stylu zbliżonego do języka naturalnego.

Analiza metod automatycznego generowania podpisów pozwala wyróżnić różnorodne formaty narracyjne. Obejmują one rozbudowane akapity, enumeratywne listy opisujące poszczególne elementy obrazu oraz zwięzłe zdania syntetyzujące całą scenę. W pracy skoncentrowano się na modelach generujących pojedyncze zdanie, które stanowi syntetyczny opis zawartości obrazu. Systemy te będą określane mianem "systemów automatycznego podpisywania obrazów", a generowane przez nie zdanie - "podpisem automatycznym". 

Przyjęta metodologia formułowania podpisów opierała się na czterech zasadach. Nadrzędnym wymogiem była wierność deskryptywna, definiowana jako precyzyjne odzwierciedlenie obserwowalnych elementów, takich jak obiekty, ich atrybuty, relacje przestrzenne i akcje. Wymóg ten dopełnia obiektywizm, który wyklucza wszelkie subiektywne oceny, interpretacje, nacechowanie emocjonalne oraz wnioskowanie wykraczające poza bezpośredni materiał wizualny. Ponadto każdy podpis musiał charakteryzować się zwięzłością i ograniczać się jedynie do informacji niezbędnych do zrozumienia sceny. Finalnie każdy podpis musiał spełniać formalne standardy poprawności stylistycznej i gramatycznej.

Rozdział przedstawia systematyczny przegląd literatury, śledząc ewolucję metod automatycznego podpisywania obrazów. Rozpoczyna się od analizy wczesnych podejść kompozycyjnych, by następnie przejść do omówienia paradygmatu koder-dekoder, który zdominował erę głębokiego uczenia. W dalszej części szczegółowo opisano mechanizm uwagi oraz modele oparte na architekturze transformatora. Rozdział zamyka prezentacja alternatywnych kierunków badawczych oraz podsumowanie, które identyfikuje lukę badawczą adresowaną w rozprawie.

\section{Metody kompozycyjne}
Wczesne systemy automatycznego generowania podpisów do obrazów konstruowały podpisy na podstawie predefiniowanych reguł lub wzorców, wykorzystując w tym celu uprzednio zidentyfikowane komponenty wizualne. Mimo iż podejścia te cechowały się niską elastycznością oraz ograniczoną zdolnością do adaptacji w nowych kontekstach, znacząco wpłynęły na rozwój badań. W tej grupie można wyróżnić metody oparte na szablonach oraz wyszukiwaniu~\cite{Bai2018Survey}.

Metody oparte na szablonach wykorzystywały gotowe szablony budowy zdań, gdzie puste miejsca uzupełniano rezultatami analizy obrazu. W podejściu tym sztywno zdefiniowany szablon determinuje składnię zdania, podczas gdy zadaniem algorytmów jest jedynie dostarczenie odpowiednich informacji do wstawienia w luki. Przykładowo, struktura szablonu może przyjąć formę:
"Na obrazie widzimy [ilość] [kategoria obiektu] podczas [czynność]".

Proces generowania podpisu rozpoczynał się od detekcji obiektów, relacji i atrybutów. Następnie, dla każdego obiektu, w zależności od zastosowanej metody, pozyskiwano istotne informacje, takie jak kolor, rozmiar, kształt, kategoria oraz relacje przestrzenne z innymi obiektami. W kolejnym kroku selekcjonowano najbardziej odpowiedni szablon z predefiniowanego zbioru według kryteriów takich jak liczba wykrytych obiektów, ich typy oraz ogólna charakterystyka obrazu. W ostatniej fazie szablon uzupełniano zidentyfikowanymi obiektami i ich atrybutami. 

W pracy~\cite{Yang2011Corpus} zaproponowano metodę wykorzystującą szablon o stałej strukturze składniowej: [rzeczownik, czasownik, scena, przyimek]. Detektor obiektów oraz deskryptor sceny GIST~\cite{Torralba2003Vi} dostarczały kandydatów na rzeczowniki i sceny. Na ich podstawie model językowy Gigaword~\cite{Graff2003Gigaword} generował zbiór pasujących czasowników i przyimków. Z tak powstałego zbioru zdań kandydujących, Ukryte Modele Markowa (HMM)~\cite{Payandeh2023DeepRepresentationLearning} selekcjonowały finalny podpis jako najbardziej prawdopodobną sekwencję słów.

Inne podejście~\cite{Kulkarni2011SimpleImgDesc} modelowało scenę wizualną za pomocą struktury grafowej, gdzie węzły reprezentowały obiekty, atrybuty i relacje. Do wnioskowania o najbardziej prawdopodobnej konfiguracji grafu dla danego obrazu zastosowano warunkowe pola losowe (CRF)~\cite{Sutton2012CRF}. Wydedukowane komponenty sceny były następnie wstawiane w predefiniowany szablon.

Działanie metody~\cite{Ushiku2015Common} polega na stworzeniu wspólnej przestrzeni multimodalnej (\gls{gls:multimodal}), w której fragmenty obrazów i odpowiadające im semantycznie frazy tekstowe są umieszczane blisko siebie. Zamiast generować zdanie słowo po słowie, model ten wykorzystuje tę przestrzeń do wyszukiwania dla danego obrazu całych, bogatych w znaczenie fraz. Taka struktura pozwala na współdzielenie kontekstu semantycznego między frazami, co sprawia, że generowane podpisy są bardziej naturalne i zbliżone do ludzkich. Ostateczne zdanie jest konstruowane z najlepiej dopasowanych fraz przy użyciu algorytmu przeszukiwania wiązkowego (beam search)~\cite{Ushiku2012Efficient}.

Ograniczenia sztywnych szablonów motywowały rozwój metod bardziej elastycznych. W pracy~\cite{Mitchel2012} zaproponowano dwuetapowy proces. Składał się z ekstrakcji treści, w tym słów kluczowych opisujących obiekty, akcje, relacje oraz generowania języka. Zbiór słów kluczowych był filtrowany z użyciem statystyk współwystępowania w celu eliminacji semantycznie niespójnych kombinacji. Dopiero z tak oczyszczonych danych, trójgramowy model językowy Europarl~\cite{Koehn2005Europarl} generował zdanie końcowe.

W pracach~\cite{Elliott2013Image, Elliott2015Describing} wprowadzono wizualną reprezentację zależności (\gls{VDR}) – strukturę kodującą relacje przestrzenne między obiektami a regionami obrazu. Do jej konstruowania zdefiniowano wizualną gramatykę zależności, na podstawie której opracowano dwa modele generujące podpisy z wykorzystaniem szablonów. W dalszym rozwoju tej koncepcji zautomatyzowano proces budowy \gls{VDR}, aby naśladował ludzki sposób podpisywania obrazów, wykorzystując w tym celu detektor obiektów oraz zbiór wzorcowych podpisów.

Odmienną strategię przyjęto w metodach opartych na wyszukiwaniu~\cite{Bai2018Survey}. Paradygmat ten zakłada, że w obszernej bazie danych obrazów z podpisami można odnaleźć przykłady wizualnie podobne do obrazu wejściowego, a ich istniejące podpisy przenieść, połączyć lub zaadaptować do sformułowania nowego.

Proces rozpoczynał się od ekstrakcji z obrazu wejściowego globalnych deskryptorów wizualnych, takich jak \gls{HOG} oraz \gls{SIFT}~\cite{Lowe2004SIFT}. Opisywały one ogólną zawartość obrazu, uwzględniając informacje o kolorach, kształtach, teksturach czy obiektach i ich konturach. Należy podkreślić, że były to cechy projektowane ręcznie, w odróżnieniu od reprezentacji uczonych automatycznie w późniejszych modelach neuronowych. 

W kolejnym kroku uzyskane deskryptory wykorzystywano do indeksowania obrazów w bazie danych. Dla zadanego obrazu wejściowego system wyszukiwał obrazy o najbardziej zbliżonych wektorach cech, gdzie podobieństwo kwantyfikowano miarami odległości takimi jak odległość euklidesowa lub kosinusowa. Finalny podpis był konstruowany poprzez transfer najbardziej dopasowanego podpisu, fuzję kilku z nich lub ich adaptację do nowego kontekstu.

Jedną z pierwszych prac w tym nurcie była publikacja~\cite{Farhadi2010} w której mapowano obraz na trójelementową przestrzeń semantyczną [obiekt, akcja, scena]. Tego typu reprezentacja była uzyskiwana za pomocą losowych pól Markova~\gls{MRF}~\cite{Li1994MarkovMRFinCV}. Wygenerowany wektor semantyczny służył do wyszukania w bazie najbardziej pasującego zdania, gdzie jako miarę podobieństwa zastosowano~\cite{Lin1998AnID}.

Inne podejście, zaprezentowane w pracy~\cite{Ordonez2011Im2Text}, bazowało na wieloaspektowym wyszukiwaniu podobieństwa. Obrazy charakteryzowane za pomocą deskryptorów GIST porównywano pod względem kształtu obiektów, ich atrybutów, akcji oraz typu sceny. Dla każdego z nich tworzono odrębne rankingi podobieństwa, które następnie agregowano w jeden wynik końcowy. Do agregacji wykorzystywano model regresji liniowej oraz maszyny wektorów nośnych (\gls{SVM}) optymalizowany względem metryki BLEU.

Rozwinięciem tej koncepcji była metodologia opisana w pracy~\cite{Kuznetsova2012Collective}. Zamiast tworzyć jeden globalny ranking, autorzy zastosowali strategię wyszukiwania podobnych obrazów dla każdej pojedynczej encji np. obiektu, atrybutu wykrytej w obrazie wejściowym. W rezultacie powstawał zbiór fraz odpowiadających poszczególnym elementom sceny, a finalny podpis generowano z odfiltrowanego zbioru. W późniejszych badaniach rozwiązanie to zostało udoskonalone poprzez wprowadzenie hierarchicznej, drzewiastej reprezentacji zdań.

W podobnym duchu, w pracy~\cite{Gupta2012Choosing} z podpisów przypisanych do podobnych obrazów wyodrębniano frazy, które następnie szeregowano i transformowano do uporządkowanych, trójelementowych form gramatycznych. Finalny podpis generowano z trzech najwyżej ocenionych struktur.

Istotnym postępem było opracowanie metod opartych na tworzeniu wspólnej, multimodalnej przestrzeni osadzeń, w której zarówno obrazy, jak i ich podpisy były reprezentowane jako wektory. W metodzie~\cite{Hodosh2013Senetence} wspólną przestrzeń konstruowano za pomocą metody \gls{KICA}~\cite{Bach2003Kernel} i optymalizowano algorytmem RankSVM~\cite{Joachim2002Optimizing}. W tak zbudowanej przestrzeni zdania szeregowano na podstawie podobieństwa kosinusowego między wektorami cech obrazów a wektorami cech podpisów.

W podejściu~\cite{Socher2014Grounded} podpisy ze zbioru treningowego były parsowane na drzewa zależności z wykorzystaniem narzędzia Stanford Parser~\cite{Etal2006generating}, a cechy obrazów służące do wyszukiwania pochodziły z modelu Imagenet~\cite{Szegedy2014Going}. Z kolei w pracy~\cite{KirosSZ2014Unifying} trenowano wspólną przestrzeń osadzeń obraz-zdanie, która odzwierciedlała regularności językowe. Co istotne, strukturalne elementy zdania, takie jak znaczniki części mowy, były oddzielone od jego treści i nie były trenowane równolegle z przestrzenią osadzeń, celem redukcji szumu semantycznego. Dzięki tak zdefiniowanej przestrzeni osadzeń możliwe stało się wykonywanie operacji arytmetycznych na wektorach reprezentujących obrazy i zdania, a w efekcie wyszukiwanie semantycznie podobnych obrazów. 

Innym przykładem była praca~\cite{Mason2014nonparametric}, gdzie podpisy konstruowano na podstawie częstości słów w podpisach do obrazów podobnych do zadanego. Celem było zminimalizowanie błędu estymacji wizualnej poprzez statystyczną agregację najczęściej występujących terminów.

Zarówno metody oparte na szablonach, jak i na wyszukiwaniu wykazywały istotne ograniczenia. Ich skuteczność była ściśle uzależniona od zakresu i jakości korpusu istniejących podpisów. Prowadziło to do generowania podpisów, które były jedynie rekompozycją gotowych konstrukcji składniowych zaczerpniętych z istniejących zbiorów danych, a nie autentycznym podpisem do obrazu. Taka zależność skutkowała generowaniem podpisów często nieadekwatnych, szczególnie gdy scena wizualna zawierała elementy nietypowe lub rzadko spotykane w zbiorach danych. 
 
\section{Podstawowe modele neuronowe do automatycznego podpisywania obrazów}
\label{sekcja:podstawowe_modele_neuronowe}
Ograniczenia metod kompozycyjnych skłoniły badaczy do poszukiwania rozwiązań \gls{gls:end-to-end}. W tym podejściu model uczy się mapowania z domeny wizualnej do językowej w ramach jednej, zintegrowanej sieci neuronowej. W tym celu zaadaptowano z dziedziny tłumaczenia maszynowego (\gls{NMT}) architekturę koder-dekoder. Podejście to zdominowało nowoczesne systemy do automatycznego generowania podpisów do obrazów.

Architektura koder-dekoder składa się z dwóch komplementarnych komponentów. Zadaniem kodera jest przetworzenie obrazu na wektorową reprezentację o stałej długości, zwaną wektorem kontekstu. Jest to skondensowana informacja semantyczna o obrazie. Następnie dekoder rekurencyjny generuje sekwencyjnie podpis warunkując każde kolejne słowo na wektorze kontekstu oraz słowach wygenerowanych w poprzednich krokach.

W standardowej implementacji rolę kodera pełnią sieci szkieletowe wstępnie wytrenowane na wielkoskalowych zbiorach danych do zadania klasyfikacji obrazów. Funkcję dekodera najczęściej realizują sieci \gls{RNN}, zwłaszcza warianty o rozszerzonej pamięci, takie jak \gls{LSTM}~\cite{Hochreiter1997LSTM} czy \gls{GRU}~\cite{Cho2014Properties} (opisane szczegółowo w Sekcji~\ref{sekcja:sieci-rekurencyjne}). Ich zdolność do modelowania długoterminowych zależności w danych sekwencyjnych umożliwiała generowanie spójnych i gramatycznie poprawnych zdań.

Kierunek rozwoju tej architektury wyznaczył kanoniczny model~\cite{Vinyals2015Show}. Wczesne badania koncentrowały się głównie na ewolucji komponentu kodera, badając zastosowanie różnych ekstraktorów cech, takich jak VGG~\cite{Chen2015Minds, Mao2014DeepCW}, Googlenet~\cite{Wu2016WhatValue, You2016Semantic}, a w późniejszym okresie bardziej zaawansowanych architektur, jak Resnet~\cite{Rennie2017SelfCriticalST, Yao2017ICCV} czy Oxfordnet~\cite{Jia2015Guiding}. W podejściach tych globalny wektor cech pozyskiwano z aktywacji jednej z ostatnich warstw w pełni połączonych sieci, co dostarczało bogatej semantycznie, syntetycznej reprezentacji całej sceny wizualnej.

Równolegle eksplorowano alternatywne strategie modyfikujące przepływ informacji. Prace takie jak~\cite{Mao2014DeepCW} skupiały się na mapowaniu cech wizualnych i reprezentacji słów do wspólnej przestrzeni multimodalnej. Inne, jak~\cite{Gan2017Semantic, Yao2017ICCV}, zamiast globalnej reprezentacji, wykorzystywały detekcję obiektów lub tagów semantycznych jako wejście dla dekodera LSTM. Pojawiły się również podejścia dwuetapowe, gdzie najpierw identyfikowano kluczowe atrybuty obrazu, a dopiero w drugim kroku model językowy używał ich do generowania podpisu~\cite{Wu2016WhatValue}.

Z kolei w pracy~\cite{Chen2015Minds} zaproponowano integrację sieci RNN z dodatkową warstwą ukrytą, która rekonstruowała cechy wizualne na podstawie generowanych słów. Dzięki temu model mógł uwzględniać długoterminowe zależności wizualne poprzez porównywanie wewnętrznej reprezentacji obrazu z jego rzeczywistymi cechami. 

Mimo wysokiej skuteczności, podstawowa architektura koder-dekoder wykazywała inherentne ograniczenia. Koder syntetyzował wynikową, wielowymiarową mapę cech z warstw sieci szkieletowej do płaskiej struktury. Dekoder generując każde słowo podpisu bazował na tej jednej, uproszczonej reprezentacji. Prowadziło to do do utraty istotnych detali z płytszych warstw sieci szkieletowej, takich jak relacje między obiektami czy ich atrybuty. Jednakże efektywne wykorzystanie całej złożonej mapy cech wymagało rozwiązania do efektywnego przetwarzania takiej ilości danych. Odpowiedzią stał się mechanizm uwagi, który umożliwia efektywne przetwarzanie całej mapy cech obrazu.

\section{Wprowadzenie i rozwój mechanizmu uwagi}
Mechanizm uwagi zastąpił statyczny wektor kontekstu wizualnego (omówiony w Sekcji~\ref{sekcja:podstawowe_modele_neuronowe}) wektorem, który jest obliczany na nowo dla każdego generowanego słowa w zdaniu wyjściowym. W każdym kroku dekodowania model uczy się alokować wagi uwagi dla poszczególnych fragmentów całej mapy cech. Wagi te kwantyfikują stopień istotności danego fragmentu obrazu dla predykcji bieżącego słowa. Ostateczny wektor kontekstu, przekazywany do dekodera, jest zatem ważonym zbiorem wejściowych cech obrazu. Pozwala to na wzmocnienie istotności jedynie tych cech obrazu, które są istotne dla aktualnie generowanego słowa, ograniczając tym samym szum globalnego kontekstu sceny wizualnej.

Pierwsze implementacje mechanizmu uwagi w zadaniu automatycznego generowania podpisów do obrazów opierały się na architekturze odgórnej (top--down), wywodzącej się z prac nad tłumaczeniem maszynowym~\cite{Bahdanau2015NeuralMT}. W tym paradygmacie stan ukryty dekodera RNN w danym kroku czasowym pełni funkcję zapytania. Na jego podstawie obliczane są wagi dla zbioru wektorów cech wizualnych, które stanowią regularną siatkę przestrzenną pochodzącą z jednej z ostatnich warstw sieci szkieletowej. Ważona suma tych wektorów tworzy wektor kontekstu, który jest następnie wykorzystywany przez dekoder do predykcji kolejnego słowa w sekwencji.

Praca~\cite{Xu2015ShowAttendTell} zaadaptowała tę koncepcję, wprowadzając dwa kontrastujące ze sobą warianty uwagi. Deterministyczna uwaga miękka (soft attention) agreguje informacje z całej sceny wizualnej, konstruując wektor kontekstu jako ważoną sumę wszystkich cech. Wiąże się to jednak ze znacznym kosztem obliczeniowym. W przeciwieństwie do niej uwaga twarda (hard attention) w każdym kroku czasowym wybiera tylko jeden fragment siatki cech, co czyni ją znacznie bardziej efektywną obliczeniowo, lecz wymaga optymalizacji metodami uczenia ze wzmocnieniem. Pomimo wyższych wymagań obliczeniowych, uwaga miękka, dzięki swojej zdolności do agregacji informacji z całej sceny, stała się punktem wyjścia dla dalszych badań.

Ważnym kierunkiem rozwoju stało się wzbogacanie wejściowej reprezentacji wizualnej o dodatkowe informacje z zewnętrznych źródeł. W pracy~\cite{Tavalkoyi2017PayingAttentionIstotneRegiony} autorzy wykorzystali w tym celu predykcje z wyspecjalizowanych modeli istotności wizualnej~\cite{HUANG2006489, RTAVAKOLI201710}. Celem integracji tego typu wektorów było dostarczenie modelowi bardziej granularnych i semantycznie bogatych informacji. Jeszcze inną strategią było agregowanie reprezentacji z wielu heterogenicznych architektur sieci szkieletowych jak Resnet~\cite{He2015Deep}, InceptionV3~\cite{Szegedy2016RethinkingTI}, Densenet~\cite{huang2017densely} oraz Inception-ResnetV2~\cite{Szegedy2017InceptionV4} w celu uzyskania bardziej kompletnego i zróżnicowanego podpisu do sceny wizualnej~\cite{Jiang2018RecurrentFusion}. 

Równoległym nurtem badań była próba integracji danych o ludzkiej percepcji. W pracy~\cite{Sugano2016SeeingW} zaproponowano wykorzystanie znormalizowanych histogramów skupienia wzroku jako dodatkowego sygnału dla modelu. Histogramy te, kodujące przestrzenny rozkład ludzkiej uwagi, były scalane z cechami obrazu i tekstu. Dzięki temu generowane podpisy lepiej odwzorowywały ludzki sposób postrzegania i priorytetyzacji informacji wizualnej.

Ostatnią z istotnych modyfikacji było udoskonalenie samego procesu iteracyjnego . Zamiast jednokrotnego obliczania wektora kontekstu w każdym kroku, w pracy~\cite{Yang2016} zaproponowano architekturę z dodatkową rekurencyjną siecią recenzującą. Sieć ta, we współpracy z mechanizmem uwagi, iteracyjnie udoskonalała stany ukryte dekodera, generując udoskonalony wektor kontekstu, który służył jako wejście dla modułu uwagi w kolejnej iteracji. Takie podejście pozwalało na bardziej precyzyjne i kontekstowe skupianie uwagi w miarę postępów generowania zdania.

Ograniczeniem uwagi odgórnej, operującej na regularnej siatce cech, jest arbitralny i niezależny od treści obrazu charakter tego rodzaju podziału przestrzennego. W konsekwencji kolejny etap ewolucji mechanizmu uwagi polegał na zastąpieniu sztywnej siatki cech elastycznym zbiorem semantycznie istotnych regionów obrazu. Podejście to, określane mianem uwagi oddolnej, wprowadza wstępny, sterowany danymi etap, którego celem jest identyfikacja wyróżniających się obiektów i ich atrybutów.

Nowatorską pracą w tym nurcie jest model zaproponowany w~\cite{Anderson2018BottomUpAT}, który skutecznie łączy mechanizmy uwagi oddolnej i odgórnej. Pierwszy, oddolny (bottom-up) jest sterowany wyłącznie danymi wejściowymi. W jego ramach detektor obiektów Faster R-CNN~\cite{Ren2015Faster} identyfikuje w obrazie zbiór propozycji regionów o wysokim znaczeniu semantycznym. Każdy region jest reprezentowany przez dedykowany wektor cech. Dopiero w drugim, odgórnym (top-down) etapie mechanizm uwagi, sterowany kontekstem z dekodera, oblicza rozkład wag dla tak przygotowanego zbioru wektorów. W ten sposób uwaga nie jest rozpraszana na mało istotne tło, lecz od początku operuje na bogatych semantycznie reprezentacjach obiektów. Przełożyło się to na wzrost precyzji i szczegółowości podpisów.

Paradygmat ten stał się podstawą dla dalszych badań. Model~\cite{Anderson2018BottomUpAT} kierował uwagę na jeden nowy region w każdym kroku generowania słowa. W pracy~\cite{Huang2019adaptively} zaproponowano adaptacyjny moduł uwagi, który w każdym kroku decyduje, czy skierować uwagę na kolejny region, czy też wygenerować kolejne słowo na podstawie dotychczasowego kontekstu. Umożliwiło to opisanie jednego regionu wieloma słowami. Rola i skuteczność wykorzystania regionów jako podstawowych jednostek dla mechanizmu uwagi zostały potwierdzone w późniejszych pracach, takich jak~\cite{Quin2019LookBackRegions, Ke2019ReflectiveRegions, Wang2020ShowRecallRegions}.

Dotychczas omówione mechanizmy uwagi modelowały relacje między dekoderem a zbiorem cech wizualnych. Uwaga własna, wprowadzona w pracy~\cite{Vaswani2017AttentionIA} modeluje interakcje wewnątrz pojedynczego zbioru reprezentacji. Zgodnie z tym założeniem każdy element sekwencji wejściowej określa swoją nową reprezentację, uwzględniając wszystkie pozostałe elementy. Model uczy się oceniać wzajemne dopasowanie między każdą parą elementów, na podstawie czego tworzy wagi uwagi. Wagi te określają, jak ważny jest każdy inny element dla zrozumienia tego bieżącego. Nowa, wzbogacona o kontekst reprezentacja elementu powstaje jako ważona suma informacji od wszystkich elementów w sekwencji.

Początkowe zastosowania uwagi własnej w generowaniu podpisów do obrazów koncentrowały się na jednoczesnym modelowaniu relacji wizualnych i semantycznych. Przykładowo, architektura~\cite{Li2019Entangled} rozdzielała analizę wizualną, w ramach której uwaga własna bada relacje między regionami, od analizy semantycznej, polegającej na identyfikacji zależności między kategoriami obiektów. Podobne w pracy \cite{Yang2019Collate} uwaga własna posłużyła do modelowania interakcji między parami obiektów, co wzbogaciło reprezentację obiektów przed ich przetworzeniem przez dekoder. 

Istotnym ograniczeniem wczesnych modeli był brak uwzględnienia relacji przestrzennych między obiektami. W odpowiedzi, w~\cite{Herdade2019Image} zaproponowano moduł relacji geometrycznych, który koryguje wagi uwagi, dodając do nich składnik oparty na cechach geometrycznych ramek ograniczających obiekty. W modelu znormalizowanej uwagi w~\cite{Guo2020Normalized} posunięto się o krok dalej. Finalne wagi uwagi zdefiniowano jako  wypadkową zarówno podobieństwa wizualnego, jak i względnego położenia geometrycznego par obiektów.

Standardowe modele uwagi analizują głównie bezpośrednie zależności między parami elementów, aby modelować bardziej złożone relacje. Autorzy~\cite{Pan2020XLinear} zaproponowali architekturę łączącą uwagę własną z metodą redukcji dwuliniowej (\gls{gls:bilinear-pooling}). Pozwoliło to na modelowanie, jak dany fragment obrazu wpływa na interakcje między innymi. Dążenie do kodowania wielopoziomowych zależności było również motywacją prac \cite{Cornia2020Meshed, Cornia2020SMART}, w których standardowy mechanizm uwagi rozszerzono o zewnętrzny zbiór uczących się wektorów pamięci, służących do kodowania i przechowywania globalnych zależności między obiektami.

Wyzwaniem w metodach z uwagą własną pozostaje ich złożoność obliczeniowa oraz tendencja do generowania gęstych macierzy uwagi, w których wiele wag jest bliskich zeru, lecz niezerowych. Może to rozpraszać uwagę modelu. W celu przeciwdziałania temu zjawisku, w pracy \cite{Jiang2021Multigate} zastosowano mechanizm bramek, który maskuje wagi uwagi poniżej wyznaczanego progu, przy jednoczesnym zachowaniu istotnych elementów obrazu. 

Równolegle do ewolucji głównych paradygmatów uwagi rozwijano szereg mechanizmów pomocniczych. Jednym z wyzwań było generowanie słów o charakterze funkcjonalnym, jak spójniki, przyimki, które nie mają bezpośredniego odzwierciedlenia na obrazie. W odpowiedzi, w pracy \cite{Lu2017KnowingWT} zaproponowano koncepcję strażnika wizualnego. Jest to mechanizm stosujący bramki, który decyduje, czy w danym kroku predykcja dekodera powinna być warunkowana kontekstem wizualnym, czy też wyłącznie informacjami z już wygenerowanej sekwencji słów. Koncepcję tę rozwijano w wielu kierunkach. Jednym z nich było uzależnienie decyzji bramki od dodatkowych, zewnętrznych sygnałów, takich jak predefiniowane szablony zdań \cite{Lu2018NeuralBTTemplates} lub wyróżnione regiony obrazu \cite{Cornia2019ShowCASterowanie}. Inne podejście koncentrowało się na bezpośrednim sterowaniu przepływem informacji wizualnej, gdzie mechanizm strażnika wizualnego dynamicznie aktywował lub dezaktywował cechy obrazu na wejściu do dekodera \cite{Deng2020DensenetAdaptive}.

Innym kierunkiem badań było zwiększenie zdolności ekspresji samego dekodera poprzez zastosowanie architektur wielowarstwowych. Ułożenie kilku warstw rekurencyjnych w stos \cite{Gao2019Deliberate} pozwala na naukę hierarchicznych reprezentacji lingwistycznych. Niższe warstwy mogą modelować zależności składniowe, podczas gdy wyższe – abstrakcyjne relacje semantyczne. Głębszy model językowy jest w stanie formułować bardziej precyzyjne zapytania do mechanizmu uwagi, co przekłada się na poprawę jakości podpisów. Implementacje tego podejścia przybierały różne formy, od prostych, dwuwarstwowych architektur typu LRCN \cite{Donahue2017LongTerm}, przez moduły uwagi refleksyjnej modelujące relacje wewnątrz zdania \cite{Ke2019Reflective}, aż po rozdzielenie strumieni przetwarzania dla cech wizualnych i językowych w osobnych modułach LSTM w celu ich lepszej integracji \cite{Wang2019Hierarchical}.

Naturalną konsekwencją stało się łączenie tych nurtów w architekturach hybrydowych. Przykładem jest model z pracy~\cite{Gao2019Deliberate}, który integruje wielowarstwowy dekoder z mechanizmem strażnika wizualnego. Architektura ta realizuje dwuetapowe udoskonalanie podpisu, gdzie pierwsza warstwa LSTM generuje wstępny podpis, a druga wyposażona w strażnika wizualnego koryguje go, blokując wpływ cech wizualnych przy generowaniu słów o funkcji czysto syntaktycznej.

\section{Architektura transformatorowa}
Architektura transformatorowa zmieniła paradygmat przetwarzania danych sekwencyjnych, eliminując pętle rekurencyjne na rzecz operacji opartych wyłącznie na mechanizmie uwagi~\cite{Vaswani2017AttentionIA}. Rezygnacja z sekwencyjnego przetwarzania umożliwiła efektywne przetwarzanie wsadowe i obliczenia równoległe. Jednocześnie zastosowanie uwagi własnej do modelowania globalnych zależności znacząco zwiększyło zdolność sieci do przechwytywania długoterminowych kontekstów, zarówno wizualnych, jak i językowych.

Struktura transformatora jest oparta na architekturze koder-dekoder. Koder, składający się ze stosu identycznych warstw, przetwarza na wejściu sekwencję wektorów cech charakteryzujących regiony obrazu. W każdej warstwie mechanizm uwagi własnej aktualizuje reprezentację danego regionu na podstawie kontekst dostarczany przez wszystkie pozostałe. W ten sposób powstają bogate w kontekst reprezentacje wizualne, kodujące złożone relacje między obiektami. 

Dekoder transformatora generuje sekwencję wyjściową słowo po słowie. Jego warstwy, oprócz warstwy uwagi własnej i sieci w pełni połączonych, zawierają moduł uwagi krzyżowej (\gls{gls:cross-attention}). Uwaga własna w dekoderze jest maskowana, co gwarantuje, że predykcja bieżącego słowa zależy wyłącznie od słów już wygenerowanych. Następnie, w module uwagi krzyżowej, reprezentacje językowe kierują uwagę na wyjście z kodera wizualnego. Ten mechanizm pozwala dekoderowi na dynamiczne integrowanie najbardziej relewantnych informacji wizualnych w procesie generowania każdego kolejnego słowa. 

Elastyczność architektury transformatorowej sprawiła, że stała się ona podstawą dla licznych modyfikacji i rozszerzeń w dziedzinie generowania podpisów do obrazów~\cite{Luo2021DualLevelCTTransformator, Herdade2019Image, Guo2020Normalized}. Część badań skupiła się na udoskonaleniu sterowania mechanizmem uwagi. Przykładowo, w pracy~\cite{Yan2021Task} wprowadzono rozwiązanie, w którym podczas generowania słów o charakterze czysto kontekstowym uwaga dekodera jest kierowana nie na cechy obrazu, a na zbiór dodatkowych, trenowanych wektorów. Podejście to jest funkcjonalnym odpowiednikiem mechanizmu strażnika wizualnego, zaimplementowanego w architekturze transformatorowej.

Innym kierunkiem rozwoju jest integracja modelu z zewnętrznymi źródłami wiedzy. W pracy~\cite{Sarto22Retrieval} standardowy dekoder rozszerzono o mechanizm wyszukiwania. Identyfikuje on w zewnętrznej bazie danych obrazy podobne wizualnie wraz z ich podpisami. Dalej mechanizm bramkowania dynamicznie decyduje, czy w danym kroku generowania słowa polegać na kontekście lokalnym, czy na informacjach pochodzących z odnalezionych obrazów referencyjnych.

Mechanizmy stosujące bramki stały się wszechstronnym narzędziem kontroli przepływu informacji w transformatorach. Są one wykorzystywane do dynamicznego ważenia wpływu poszczególnych warstw kodera na proces dekodowania \cite{Cornia2020Meshed}, do filtrowania macierzy uwagi w celu skupienia się na kluczowych zależnościach \cite{Jiang2021Multigate}, a także do warunkowania istotności cech wizualnych w zależności od globalnego kontekstu zdania \cite{Ji2020ImprovingIC}.

Podsumowując, architektura transformatorowa, dzięki efektywności obliczeniowej oraz wysokiej zdolności do modelowania złożonych zależności, stanowi obecnie istotną gałąź w dziedzinie automatycznego podpisywania obrazów.

\section{Alternatywne architektury i kierunki badań}
W poszukiwaniu alternatyw dla standardowych dekoderów opartych na sieciach rekurencyjnych, w badaniach nad automatycznym podpisywaniem obrazów eksplorowano zastąpienie mechanizmu rekurencji splotami oraz zmianę paradygmatu generowania z autoregresywnego na nieautoregresywny.

Pierwsza polega na zastosowaniu architektur w pełni opartych na sieciach splotowych. Była motywowana teoretyczną przewagą CNN w modelowaniu dalekosiężnych zależności oraz możliwością prowadzenia równoległych obliczeń~\cite{Aneja2018CVPR}. W modelach tego typu wektor cech obrazu jest łączony z osadzeniami słów, a następnie przetwarzany przez blok warstw splotowych. W odróżnieniu od modeli RNN, które przetwarzają sekwencje słowo po słowie, dekoder CNN operuje na całej sekwencji jednocześnie, co znacząco przyspiesza proces uczenia. Mimo iż podejście to wykazało pewne zalety, takie jak generowanie bardziej zróżnicowanych podpisów i odporność na problem zanikającego gradientu, jego ogólna skuteczność w testach porównawczych okazała się niższa niż modeli RNN z mechanizmem uwagi. W konsekwencji, wraz z gwałtownym rozwojem i dominacją modeli transformatorowych, badania nad dekoderami CNN w tym zastosowaniu straciły na intensywności.

Znacznie bardziej fundamentalną zmianą paradygmatu okazało się generowanie nieautoregresywne. Modele te eliminują sekwencyjną zależność słów generując wszystkie tokeny wyjściowe jednocześnie i niezależnie. W celu implementacji tego paradygmatu opracowano szereg strategii. Jedną z nich jest wykorzystanie celu treningowego analogicznego do maskowanego modelowania języka (\gls{gls:mlm}), gdzie model uczy się przewidywać losowo zamaskowane słowa na podstawie kontekstu wizualnego i pozostałych słów~\cite{gao2019maskednonautoregressiveimagecaptioning}. Inne podejścia opierają się na iteracyjnym udoskonalaniu, gdzie początkowy, surowy podpis jest generowany w jednym kroku, a następnie wielokrotnie poprawiany przez dedykowane moduły \cite{Fei2020, guo2021fastsequencegenerationmultiagent}.

Główną zaletą modeli nieautoregresywnych jest znacząca redukcja latencji podczas inferencji, wynikająca z równoległego generowania całej sekwencji, co jest kluczowe dla zastosowań czasu rzeczywistego. Co więcej, wyeliminowanie zależności sekwencyjnych zapobiega zjawisku propagacji błędu, w którym pojedynczy błąd kaskadowo wpływa na resztę generowanego zdania. Może to również prowadzić do tworzenia bardziej zróżnicowanych podpisów, gdyż predykcja nie jest ograniczona przez wcześniej wygenerowaną, deterministyczną ścieżkę. Podejście to nie jest jednak pozbawione istotnych wad. Brak jawnych zależności między słowami stwarza ryzyko błędów gramatycznych, powtórzeń oraz utraty spójności semantycznej. Osiągnięcie jakości porównywalnej z modelami autoregresywnymi często wymaga zastosowania skomplikowanych architektur lub wieloetapowych procedur treningowych, co częściowo niweluje pierwotną przewagę w zakresie prostoty i szybkości.