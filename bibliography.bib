% Encoding: UTF-8

@InProceedings{Banner2021Force,
  author    = {Deléarde, Robin and Kurtz, Camille and Dejean, Philippe and Wendling, Laurent},
  booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title     = {Force Banner for the recognition of spatial relations},
  year      = {2021},
  pages     = {6065-6072},
  doi       = {10.1109/ICPR48806.2021.9412316},
  file      = {:PDF/Force_Banner_for_the_recognition_of_spatial_relations.pdf:PDF},
}

@Article{Mocanu2010From,
  author  = {Mocanu, Irina},
  journal = {Advances in Electrical and Computer Engineering},
  title   = {From Content-Based Image Retrieval by Shape to Image Annotation},
  year    = {2010},
  month   = {11},
  volume  = {10},
  doi     = {10.4316/aece.2010.04008},
  file    = {:PDF/From_Content-Based_Image_Retrieval_by_Shape_to_Ima.pdf:PDF},
}

@Article{Yu2012Fuzzy,
  author  = {Yu, Daren and An, Shuang and Hu, Qinghua},
  journal = {International Journal of Computational Intelligence Systems},
  title   = {Fuzzy Mutual Information Based min-Redundancy and Max-Relevance Heterogeneous Feature Selection},
  year    = {2012},
  month   = {03},
  volume  = {4},
  doi     = {10.1080/18756891.2011.9727817},
  file    = {:PDF/Fuzzy_Mutual_Information_Based_min-Redundancy_and_.pdf:PDF},
}

@InProceedings{Sanchez2005Fuzzy,
  author    = {Sanchez, Luciano and Su{\'a}rez, M Rosario and Couso, In{\'e}s},
  booktitle = {Proc. Internat. Conf. on Machine Intelligence (ACIDCA-ICMI05), Tozeur, Tunisia},
  title     = {A fuzzy definition of Mutual Information with application to the design of Genetic Fuzzy Classifiers},
  year      = {2005},
  pages     = {602--609},
  file      = {:PDF/A fuzzy definition of Mutual Information with application to the design of Genetic Fuzzy Classifiers.pdf:PDF},
  url       = {https://sci2s.ugr.es/keel/pdf/keel/congreso/SPS123.pdf},
}

@Article{Salem2018Fuzzy,
  author  = {Salem, Omar and Wang, Liwei},
  journal = {International Journal of Software Innovation},
  title   = {Fuzzy Mutual Information Feature Selection Based on Representative Samples},
  year    = {2018},
  month   = {01},
  pages   = {58-72},
  volume  = {6},
  doi     = {10.4018/IJSI.2018010105},
  file    = {:PDF/FuzzyMutualInformationFeatureSelectionBasedonRepresentativeSamples.pdf:PDF},
}

@Article{Lim2014Scene,
  author  = {Lim, Chern Hong and Risnumawan, Anhar and Chan, Chee Seng},
  journal = {IEEE Transactions on Fuzzy Systems},
  title   = {Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding},
  year    = {2014},
  month   = {10},
  volume  = {22},
  doi     = {10.1109/TFUZZ.2014.2298233},
  file    = {:PDF/Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding.pdf:PDF},
}

@InProceedings{Sanchez2007Some,
  author    = {Sanchez, Luciano and Suarez, M. Rosario and Villar, J. R. and Couso, Ines},
  booktitle = {2007 IEEE International Fuzzy Systems Conference},
  title     = {Some Results about Mutual Information-based Feature Selection and Fuzzy Discretization of Vague Data},
  year      = {2007},
  pages     = {1-6},
  doi       = {10.1109/FUZZY.2007.4295665},
  file      = {:PDF/Some_Results_about_Mutual_Information-based_Featur.pdf:PDF},
}

@InProceedings{Yao2017ICCV,
  author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Boosting Image Captioning With Attributes},
  year      = {2017},
  month     = {Oct},
  file      = {:PDF/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features},
  url       = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf},
}

@InProceedings{Aneja2018CVPR,
  author    = {Aneja, Jyoti and Deshpande, Aditya and Schwing, Alexander G.},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Convolutional Image Captioning},
  year      = {2018},
  month     = {June},
  file      = {:PDF/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf:PDF},
  groups    = {CNN language models},
  url       = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf},
}

@InProceedings{Lee2021Umic,
  author    = {Lee, Hwanhee and Yoon, Seunghyun and Dernoncourt, Franck and Bui, Trung and Jung, Kyomin},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  title     = {{UMIC}: An Unreferenced Metric for Image Captioning via Contrastive Learning},
  year      = {2021},
  address   = {Online},
  month     = aug,
  pages     = {220--226},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.acl-short.29},
  file      = {:PDF/lee2021umic.pdf:PDF},
  url       = {https://aclanthology.org/2021.acl-short.29},
}

@InProceedings{Levinboim2021Quality,
  author    = {Levinboim, Tomer and Thapliyal, Ashish V. and Sharma, Piyush and Soricut, Radu},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Quality Estimation for Image Captions Based on Large-scale Human Evaluations},
  year      = {2021},
  address   = {Online},
  month     = jun,
  pages     = {3157--3166},
  publisher = {Association for Computational Linguistics},
  abstract  = {Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and *without* access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on *previously unseen images*. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems.},
  comment   = {-rodzaj esymatora jakosci(QE) z szczegolnym uwzglednieniem osob niewidomych i niedowidzacych. Jak ich najlepiej poinformowac o jakosci podpisu
- badanie jakosci podpisow z perspektywy czlowieka
- badanie jakosci podpisow jezeli nie znamy podpisu referencyjnego
- metoda bez uzycia obrazow referencyjnych poprawnych
-podpisy uzyskane z modeli opartych na metodzie Transformers i zbiorze ConceptualCaptions (3.3m trening 15kvalidacja) Zbiór ten wg @Piyush2018 daje lepsze wyniki dla obrazow bez konkretnej domeny dziedzinowej(czyli dowolnych)
- zauwazono ze lepsze wyniki maja podpisy z wieksza iloscia informacji albo mniejsza iloscia bledow
- wzor oceny: dla kazdego obrazu jest 10 ocen (0 lub 1). Wyciagamy srednia dla ok 9 obrazowz ocen i zaokraglamy
pˆ = round(mean(ri) ∗ 8)/8,
gdzie ri to 
Produkty:

- zbiór Caption-Quality 65k par obraz-podpis zawierajacy oceny ludzi do wygenerowaych podpisow na przestrzeni wieloletnich badan modeli img-capt
- moze byc uzyty do badania finalnej jakosci modelu a nie w czasie rzeczywistym (klasyfikacja binarna, dobry lub nie podpis)
-  T2 Test Set oceny ludzi do podpisow dla 5 najelpszych modeli w wyzwaniu  Conceptual Captions Challenge Workshop at CVPR 2019.
-  zbudwanie rankingu na podstawie zbudowanego QE ktory decyduje czy podpis jest warty pokazania ludziom niepelnosprawnym},
  doi       = {10.18653/v1/2021.naacl-main.253},
  file      = {:PDF/Quality Estimation for Image Captions Based on Large-scale Human Evaluations.pdf:PDF},
  url       = {https://aclanthology.org/2021.naacl-main.253},
}

@Article{Yan2021Task,
  author  = {Yan, Chenggang and Hao, Yiming and Li, Liang and Yin, Jian and Liu, Anan and Mao, Zhendong and Chen, Zhenyu and Gao, Xingyu},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  title   = {Task-Adaptive Attention for Image Captioning},
  year    = {2021},
  pages   = {1-1},
  comment = {podzial wg. karpathy
podłącenie atencji Task-Adaptive do architektury koder-dekoder w architekturze transformer
ewaluacja na ms-coco na dwa sposoby 
1. 82783, 40405,  40775 trening, walidacj, testy
2. 113287, 5000, 5000 trening, walidacj, testy

zalety
zastosowanie atencji task-adaptive powoduje ze wektory adaptujace ucza sie wedzy zwiazanej ze zdaniem
niwelowanie bledu zwizanego z cechami wizualnymi oraz atencja do nich przypisana, gdyz task-adaptive atenction rozrzedza atencje przez wstawianie tych wektorow},
  doi     = {10.1109/TCSVT.2021.3067449},
  file    = {:PDF/Task-Adaptive_Attention_for_Image_Captioning.pdf:PDF},
}

@InProceedings{Lu2017KnowingWT,
  author    = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
  year      = {2017},
  month     = {July},
  pages     = {3242-3250},
  abstract  = {Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.},
  doi       = {10.1109/CVPR.2017.345},
  file      = {:PDF/Lu_Knowing_When_to_CVPR_2017_paper.pdf:PDF;:PDF/Knowing_When_to_Look_Adaptive_Attention_via_a_Visual_Sentinel_for_Image_Captioning.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Visualization;Adaptation models;Decoding;Context modeling;Computational modeling;Logic gates;Mathematical model},
}


@Article{Shakarami2020Efficient,
  author    = {Shakarami, Ashkan and Tarrah, Hadis},
  journal   = {Optik},
  title     = {An efficient image descriptor for image classification and CBIR},
  year      = {2020},
  pages     = {164833},
  volume    = {214},
  file      = {:PDF/An efficient image descriptor for image classification and CBIR.pdf:PDF},
  publisher = {Elsevier},
  url       = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7198219/pdf/main.pdf},
}

@Article{Pathak2021Conent,
  author   = {Debanjan Pathak and U.S.N. Raju},
  journal  = {Optik},
  title    = {Content-based image retrieval using feature-fusion of GroupNormalized-Inception-Darknet-53 features and handcraft features},
  year     = {2021},
  issn     = {0030-4026},
  pages    = {167754},
  volume   = {246},
  abstract = {This paper proposed an efficient image retrieval framework by feature-fusion of high-level features from the improved version of DarkNet-53, named GroupNormalized-Inception-Darknet-53 (GN-Inception-Darknet-53), and handcraft features extracted from both RGB and HSI color models. To extract the more detailed features of an image, we augmented one inception layer, which includes 1 × 1, 3 × 3, and 5 × 5 kernels in place of an existing 3 × 3 kernel. To make the normalization process of the proposed model less dependent on batch size, Group Normalization (GN) layer is used instead of Batch Normalization (BN). A modified version of dot-diffused block truncation coding (DDBTC) is used to extract handcraft features in RGB color space. For HSI color space, interchannel voting between hue, saturation, and intensity components is used as color feature. To extract shape features histogram of orientated gradient (HOG) is applied on RGB color space. To evaluate the efficiency of our proposed method, Average Precision Rate (APR), Average Recall Rate (ARR), F-Measure, Average Normalized Modified Retrieval Rank (ANMRR), and Total Minimum Retrieval Epoch (TMRE) are calculated for Corel-1 K, Corel-5 K, Corel-10 K, VisTex, Stex and Color Brodatz datasets. In all datasets, the proposed method shows the best results for all the instances with a minimum average improvement of 7.02%.},
  doi      = {https://doi.org/10.1016/j.ijleo.2021.167754},
  file     = {:PDF/Content-based image retrieval using feature-fusion of GroupNormalized-Inception-Darknet-53 features and handcraft features.pdf:PDF},
  keywords = {CBIR, CNN, Feature-fusion, Inception layer, Group normalization, DDBTC},
  url      = {https://www.sciencedirect.com/science/article/pii/S0030402621013486},
}

@Article{Dong2020Novel,
  author  = {Dong, Wenhui and Yau, Stephen S. -T.},
  journal = {IEEE Access},
  title   = {A Novel Image Description With the Stretched Natural Vector Method: Application to Face Recognition},
  year    = {2020},
  pages   = {100084-100094},
  volume  = {8},
  doi     = {10.1109/ACCESS.2020.2997857},
  file    = {:PDF/A_Novel_Image_Description_With_the_Stretched_Natural_Vector_Method_Application_to_Face_Recognition.pdf:PDF},
}

@Article{Villanueva2019mapping,
  author  = {Villanueva Jr, Jaime M and Subramanian, Anantharam and Ahir, Vishal and Pollock, Andrew},
  journal = {SMU Data Science Review},
  title   = {Mapping Relationships and Positions of Objects in Images Using Mask and Bounding Box Data},
  year    = {2019},
  number  = {3},
  pages   = {11},
  volume  = {2},
  file    = {:PDF/Mapping Relationships and Positions of Objects in Images.pdf:PDF},
  url     = {https://scholar.smu.edu/cgi/viewcontent.cgi?article=1122&context=datasciencereview},
}

@Article{Fan2020Fuzzy,
  author    = {Fan, Hong and Yang, Mei and Wang, Yankun and Zeng, Jia},
  journal   = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title     = {Fuzzy Positioning Modeling of Natural Language Location Description},
  year      = {2020},
  pages     = {41--46},
  volume    = {43},
  file      = {:PDF/FUZZY POSITIONING MODELING OF NATURAL LANGUAGE LOCATION DESCRIPTION.pdf:PDF},
  publisher = {Copernicus GmbH},
  url       = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B4-2020/41/2020/isprs-archives-XLIII-B4-2020-41-2020.pdf},
}

@InProceedings{Yao2018Exploring,
  author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  booktitle = {Proceedings of the European conference on computer vision (ECCV)},
  title     = {Exploring visual relationship for image captioning},
  year      = {2018},
  pages     = {684--699},
  file      = {:PDF/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf:PDF},
  groups    = {Spatial and semantic graphs., Two-layer LSTM},
  printed   = {yes},
  url       = {https://arxiv.org/pdf/1809.07041.pdf},
}

@InProceedings{Fang2015CVPR,
  author    = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Lawrence Zitnick, C. and Zweig, Geoffrey},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {From Captions to Visual Concepts and Back},
  year      = {2015},
  month     = {June},
  file      = {:PDF/Fang_From_Captions_to_2015_CVPR_paper.pdf:PDF},
  groups    = {Compositional architectures, global CNN features},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf},
}

@InProceedings{Zhou2015CVPR,
  author    = {Zhou, Bolei and Jagadeesh, Vignesh and Piramuthu, Robinson},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {ConceptLearner: Discovering Visual Concepts From Weakly Labeled Image Collections},
  year      = {2015},
  month     = {June},
  file      = {:PDF/Zhou_ConceptLearner_Discovering_Visual_2015_CVPR_paper.pdf:PDF},
  url       = {https://www.researchgate.net/publication/268525469_ConceptLearner_Discovering_Visual_Concepts_from_Weakly_Labeled_Image_Collections},
}

@Article{Santoro2017Simple,
  author    = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  title     = {A simple neural network module for relational reasoning},
  year      = {2017},
  volume    = {30},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  file      = {:PDF/a-simple-neural-network-module-for-relational-reasoning-Paper.pdf:PDF},
  publisher = {Curran Associates, Inc.},
  timestamp = {Wed, 24 Jul 2019 11:32:12 +0200},
  url       = {https://proceedings.neurips.cc/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf},
}

@InProceedings{Peyre2017ICCV,
  author    = {Peyre, Julia and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  title     = {Weakly-Supervised Learning of Visual Relations},
  year      = {2017},
  month     = {Oct},
  file      = {:PDF/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf:PDF},
  url       = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Peyre_Weakly-Supervised_Learning_of_ICCV_2017_paper.pdf},
}

@InProceedings{Peyre2019ICCV,
  author    = {Peyre, Julia and Laptev, Ivan and Schmid, Cordelia and Sivic, Josef},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Detecting Unseen Visual Relations Using Analogies},
  year      = {2019},
  month     = {October},
  file      = {:PDF/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf:PDF},
  url       = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf},
}

@InProceedings{Pavlopoulos2019Survey,
  author    = {Pavlopoulos, John and Kougia, Vasiliki and Androutsopoulos, Ion},
  booktitle = {Proceedings of the Second Workshop on Shortcomings in Vision and Language},
  title     = {A Survey on Biomedical Image Captioning},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  month     = jun,
  pages     = {26--36},
  publisher = {Association for Computational Linguistics},
  abstract  = {Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing datasets, evaluation measures, and state of the art methods. Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets.},
  doi       = {10.18653/v1/W19-1803},
  file      = {:PDF/A Survey on Biomedical Image Captioning.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/W19-1803},
}

@Article{Jaiswal2021Image,
  author  = {Jaiswal, Tarun and others},
  journal = {Turkish Journal of Computer and Mathematics Education (TURCOMAT)},
  title   = {Image Captioning through Cognitive IOT and Machine-Learning Approaches},
  year    = {2021},
  number  = {9},
  pages   = {333--351},
  volume  = {12},
  file    = {:PDF/Image Captioning through IOT .pdf:PDF},
  url     = {https://www.turcomat.org/index.php/turkbilmat/article/download/3077/2642},
}

@Article{Lu2021Chinese,
  author     = {Lu, Huimin and Yang, Rui and Deng, Zhenrong and Zhang, Yonglin and Gao, Guangwei and Lan, Rushi},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Chinese Image Captioning via Fuzzy Attention-Based DenseNet-BiLSTM},
  year       = {2021},
  issn       = {1551-6857},
  month      = mar,
  number     = {1s},
  volume     = {17},
  abstract   = {Chinese image description generation tasks usually have some challenges, such as single-feature extraction, lack of global information, and lack of detailed description of the image content. To address these limitations, we propose a fuzzy attention-based DenseNet-BiLSTM Chinese image captioning method in this article. In the proposed method, we first improve the densely connected network to extract features of the image at different scales and to enhance the model’s ability to capture the weak features. At the same time, a bidirectional LSTM is used as the decoder to enhance the use of context information. The introduction of an improved fuzzy attention mechanism effectively improves the problem of correspondence between image features and contextual information. We conduct experiments on the AI Challenger dataset to evaluate the performance of the model. The results show that compared with other models, our proposed model achieves higher scores in objective quantitative evaluation indicators, including BLEU, BLEU, METEOR, ROUGEl, and CIDEr. The generated description sentence can accurately express the image content.},
  address    = {New York, NY, USA},
  articleno  = {14},
  doi        = {10.1145/3422668},
  file       = {:PDF/Chinese Image Captioning via Fuzzy Attention-based DenseNet-BiLSTM.pdf:PDF},
  issue_date = {April 2021},
  keywords   = {Image captioning, fuzzy attention, DenseNet, BiLSTM},
  numpages   = {18},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3422668},
}

@Article{Xu2020Survey,
  author  = {Xu, Pengfei and Chang, Xiaojun and Guo, Ling and Huang, Po-Yao and Chen, Xiaojiang and Hauptmann, Alexander G},
  journal = {EasyChair Preprint},
  title   = {A survey of scene graph: Generation and application},
  year    = {2020},
  number  = {3385},
  file    = {:PDF/A survey of scene graph Generation and application.pdf:PDF},
  url     = {https://easychair.org/publications/preprint_open/SrPK},
}

@Article{Afridi2020Multimodal,
  author     = {Tariq Habib Afridi and Aftab Alam and Muhammad Numan Khan and Jawad Khan and Young{-}Koo Lee},
  journal    = {CoRR},
  title      = {A Multimodal Memes Classification: {A} Survey and Open Research Issues},
  year       = {2020},
  volume     = {abs/2009.08395},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2009-08395.bib},
  eprint     = {2009.08395},
  eprinttype = {arXiv},
  file       = {:PDF/A Multimodal Memes Classification A Surveyand Open Research Issues.pdf:PDF},
  timestamp  = {Wed, 23 Sep 2020 15:51:46 +0200},
  url        = {https://arxiv.org/abs/2009.08395},
}

@Article{Yang2019AutoEncodingSG,
  author  = {X. Yang and Kaihua Tang and Hanwang Zhang and J. Cai},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Auto-Encoding Scene Graphs for Image Captioning},
  year    = {2019},
  pages   = {10677-10686},
  comment = {https://github.com/yangxuntu/SGAE},
  file    = {:PDF/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf:PDF},
  groups  = {Scene graphs, Two-layer LSTM},
  printed = {yes},
  url     = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf},
}

@InProceedings{Zhong2020Comprehensive,
  author    = {Zhong, Yiwu and Wang, Liwei and Chen, Jianshu and Yu, Dong and Li, Yin},
  booktitle = {Computer Vision -- ECCV 2020},
  title     = {Comprehensive Image Captioning via Scene Graph Decomposition},
  year      = {2020},
  address   = {Cham},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages     = {211--229},
  publisher = {Springer International Publishing},
  abstract  = {We address the challenging problem of image captioning by revisiting the representation of image scene graph. At the core of our method lies the decomposition of a scene graph into a set of sub-graphs, with each sub-graph capturing a semantic component of the input image. We design a deep model to select important sub-graphs, and to decode each selected sub-graph into a single target sentence. By using sub-graphs, our model is able to attend to different components of the image. Our method thus accounts for accurate, diverse, grounded and controllable captioning at the same time. We present extensive experiments to demonstrate the benefits of our comprehensive captioning model. Our method establishes new state-of-the-art results in caption diversity, grounding, and controllability, and compares favourably to latest methods in caption quality. Our project website can be found at http://pages.cs.wisc.edu/{\textasciitilde}yiwuzhong/Sub-GC.html.},
  comment   = {- użycie grafu wiedzy
- naturalnie panujemy nad tworzeniem opisu dzieki wyborowi podgrafow wiedzy
- wejście to graf wiedzy dla obrazu
- stworzona sieć neuronowa wybiera podgrafy, następnie dekodowane do zdań z użyciem atencji
- pierwsza kompleksowa metoda do tworzenia dokładnych, zróżnicowanych oraz kontrolowanych opisów opartych na obszarach obrazu

siec sGPN(subgraph proposal network) ucząca sie identyfikacji znaczących subgrafow, ktore nastepnie sa dekodowane przez sieć LSTM w elu produkcji zdań

identyfikacja znaczących podgrafow - grafowa siec konwolucyjna plus funkcja rankingowa do subgrafow
metoda rankingowa - parujemy zdanie referencyjne prawdziwe z wierzchołkami podgrafu. Ten podgraf ktory zawiera najwięcej rzeczowników/

skutecznosc - najelpsza z 20 najlepiej działających na wyjściu NIE ŚREDNIA
82,783 TRENING ms-coco
4k dla walidacji i 1k dla testów wybranych przypadkowo z zbioru validacyjnego ms-coco(40,504 obrazów)
grafy uczone na visual genome},
  doi       = {https://link.springer.com/chapter/10.1007/978-3-030-58568-6_13},
  file      = {:PDF/Comprehensive Image Captioning via Scene Graph Decomposition.pdf:PDF},
  groups    = {mix},
  isbn      = {978-3-030-58568-6},
  url       = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590205.pdf},
}

@Article{Milewski2020Scene,
  author     = {Victor Milewski and Marie{-}Francine Moens and Iacer Calixto},
  journal    = {CoRR},
  title      = {Are scene graphs good enough to improve Image Captioning?},
  year       = {2020},
  volume     = {abs/2009.12313},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2009-12313.bib},
  eprint     = {2009.12313},
  eprinttype = {arXiv},
  file       = {:PDF/Are scene graphs good enough to improve Image Captioning?.pdf:PDF},
  timestamp  = {Wed, 30 Sep 2020 16:16:22 +0200},
  url        = {https://arxiv.org/abs/2009.12313},
}

@InProceedings{Piyush2018Conceptual,
  author    = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
  year      = {2018},
  address   = {Melbourne, Australia},
  editor    = {Gurevych, Iryna and Miyao, Yusuke},
  month     = jul,
  pages     = {2556--2565},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
  doi       = {10.18653/v1/P18-1238},
  file      = {:PDF/Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:PDF},
  url       = {https://aclanthology.org/P18-1238},
}

@Article{Ng2020Understanding,
  author     = {Edwin G. Ng and Bo Pang and Piyush Sharma and Radu Soricut},
  journal    = {CoRR},
  title      = {Understanding Guided Image Captioning Performance across Domains},
  year       = {2020},
  volume     = {abs/2012.02339},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2012-02339.bib},
  eprint     = {2012.02339},
  eprinttype = {arXiv},
  file       = {:PDF/Understanding Guided Image Captioning Performance across Domains.pdf:PDF},
  timestamp  = {Wed, 09 Dec 2020 15:29:05 +0100},
  url        = {https://arxiv.org/abs/2012.02339},
}

@InProceedings{Changpinyo2021Cc12m,
  author    = {S. Changpinyo and P. Sharma and N. Ding and R. Soricut},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  year      = {2021},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {3557-3567},
  publisher = {IEEE Computer Society},
  abstract  = {The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pretraining data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [54] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for visionand-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.1},
  doi       = {10.1109/CVPR46437.2021.00356},
  file      = {:PDF/Conceptual 12M Pushing Web Scale Image Text PreTraining To Recognize Long-Tail Visual Concepts.pdf:PDF},
  keywords  = {visualization;computer vision;image recognition;pipelines;benchmark testing;data collection;knowledge discovery},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00356},
}

@Article{Su202125DVR,
  author     = {Yu{-}Chuan Su and Soravit Changpinyo and Xiangning Chen and Sathish Thoppay and Cho{-}Jui Hsieh and Lior Shapira and Radu Soricut and Hartwig Adam and Matthew Brown and Ming{-}Hsuan Yang and Boqing Gong},
  journal    = {CoRR},
  title      = {2.5D Visual Relationship Detection},
  year       = {2021},
  volume     = {abs/2104.12727},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-12727.bib},
  eprint     = {2104.12727},
  eprinttype = {arXiv},
  file       = {:PDF/2.5D Visual Relationship Detection.pdf:PDF},
  timestamp  = {Mon, 03 May 2021 17:38:30 +0200},
  url        = {https://arxiv.org/abs/2104.12727},
}

@InProceedings{Lan2017Fluency,
  author    = {Lan, Weiyu and Li, Xirong and Dong, Jianfeng},
  booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
  title     = {Fluency-Guided Cross-Lingual Image Captioning},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {1549–1557},
  publisher = {Association for Computing Machinery},
  series    = {MM '17},
  abstract  = {Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual setting. Different from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of fluency in the translated sentences, we propose in this paper a fluency-guided learning framework. The framework comprises a module to automatically estimate the fluency of the sentences and another module to utilize the estimated fluency scores to effectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both fluency and relevance of the generated captions in Chinese, but without using any manually written sentences from the target language.},
  doi       = {10.1145/3123266.3123366},
  file      = {:PDF/Fluency-Guided Cross-Lingual Image Captioning.pdf:PDF},
  isbn      = {9781450349062},
  keywords  = {english-chinese, cross-lingual image captioning, sentence fluency},
  location  = {Mountain View, California, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3123266.3123366},
}

@Article{Beretti2001Efficient,
  author  = {S. {Berretti} and A. {Del Bimbo} and E. {Vicario}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Efficient matching and indexing of graph models in content-based retrieval},
  year    = {2001},
  number  = {10},
  pages   = {1089-1105},
  volume  = {23},
  doi     = {10.1109/34.954600},
  file    = {:PDF/Efficient_Matching_and_Indexing_of_GraphModels_PAMI_Beretti2001.pdf:PDF},
}

@Article{Bloch1999Fuzzy,
  author    = {I. {Bloch}},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Fuzzy relative position between objects in image processing: a morphological approach},
  year      = {1999},
  number    = {7},
  pages     = {657-664},
  volume    = {21},
  doi       = {10.1109/34.777378},
  file      = {:PDF/Fuzzy_Relative_Position_PAMI_Bloch_1999.pdf:PDF},
  timestamp = {2021-09-13},
}

@InProceedings{Shabani2012Efficient,
  author    = {A. {Shabani} and P. {Matsakis}},
  booktitle = {2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)},
  title     = {Efficient computation of objects' spatial relations in digital images},
  year      = {2012},
  pages     = {1-4},
  doi       = {10.1109/CCECE.2012.6335023},
  file      = {:PDF/Efficient_computation_Shabani2012.pdf:PDF},
}

@InProceedings{Francis2018Fuzzy,
  author    = {J. {Francis} and F. {Rahbarnia} and P. {Matsakis}},
  booktitle = {2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
  title     = {Fuzzy NLG System for Extensive Verbal Description of Relative Positions},
  year      = {2018},
  pages     = {1-8},
  doi       = {10.1109/FUZZ-IEEE.2018.8491654},
  file      = {:PDF/Fuzzy_NLG_Francis_2018.pdf:PDF},
}

@InProceedings{Matsakis2016Fuzzy,
  author    = {P. {Matsakis} and M. {Naeem}},
  booktitle = {2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
  title     = {Fuzzy models of topological relationships based on the PHI-descriptor},
  year      = {2016},
  pages     = {1096-1104},
  doi       = {10.1109/FUZZ-IEEE.2016.7737810},
  file      = {:PDF/Fuzzy_models_Matsakis2016..pdf:PDF},
}

@Article{Matsakis2010Relative,
  author    = {P. Matsakis and L. Wawrzyniak and J. Ni},
  journal   = {International Journal of Geographical Information Science},
  title     = {Relative positions in words: a system that builds descriptions around Allen relations},
  year      = {2010},
  number    = {1},
  pages     = {1-23},
  volume    = {24},
  doi       = {10.1080/13658810802270587},
  eprint    = {https://doi.org/10.1080/13658810802270587},
  file      = {:PDF/Relative positions in words a system that builds descriptions around Allen relations.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/13658810802270587},
}

@Article{KuipersLevitt1988Navigation,
  author       = {Kuipers, Benjamin J. and Levitt, Todd S.},
  journal      = {AI Magazine},
  title        = {Navigation and Mapping in Large Scale Space},
  year         = {1988},
  month        = {Jun.},
  number       = {2},
  pages        = {25},
  volume       = {9},
  abstractnote = {In a large-scale space, structure is at a significantly larger scale than the observations available at an instant. To learn the structure of a large-scale space from observations, the observer must build a cognitive map of the environment by integrating observations over an extended period of time, inferring spatial structure from perceptions and the effects of actions. The cognitive map representation of large-scale space must account for a mapping, or learning structure from observations, and navigation, or creating and executing a plan to travel from one place to another. Approaches to date tend to be fragile either because they don’t build maps; or because they assume nonlocal observations, such as those available in preexisting maps or global coordinate systems, including active landmark beacons and geo-locating satellites. We propose that robust navigation and mapping systems for large-scale space can be developed by adhering to a natural, four-level semantic hierarchy of descriptions for representation, planning, and execution of plans in large-scale space. The four levels are sensorimotor interaction, procedural behaviors, topological mapping, and metric mapping. Effective systems represent the environment, relative to sensors, at all four levels and formulate robust system behavior by moving flexibly between representational levels at run time. We demonstrate our claims in three implemented models: Tour, the Qualnav system simulator, and the NX robot.},
  doi          = {10.1609/aimag.v9i2.674},
  file         = {:PDF/674-Article Text-671-1-10-20080129.pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/674},
}

@InProceedings{Miyajima1994Spatial,
  author    = {K. {Miyajima} and A. {Ralescu}},
  booktitle = {Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference},
  title     = {Spatial organization in 2D images},
  year      = {1994},
  pages     = {100-105 vol.1},
  doi       = {10.1109/FUZZY.1994.343710},
  file      = {:PDF/Spatial_organizatoin_Miyaima1994.pdf:PDF},
}

@Article{Cohn2001Qualititative,
  author  = {Cohn, Anthony and Hazarika, Shyamanta},
  journal = {Fundam. Inform.},
  title   = {Qualitative Spatial Representation and Reasoning: An Overview},
  year    = {2001},
  month   = {04},
  pages   = {1-29},
  volume  = {46},
  file    = {:PDF/FI-paper-2001.pdf:PDF},
  url     = {https://www.researchgate.net/publication/220444929_Qualitative_Spatial_Representation_and_Reasoning_An_Overview},
}

@Article{Bloch2005Fuzzy,
  author   = {Isabelle Bloch},
  journal  = {Image and Vision Computing},
  title    = {Fuzzy spatial relationships for image processing and interpretation: a review},
  year     = {2005},
  issn     = {0262-8856},
  note     = {Discrete Geometry for Computer Imagery},
  number   = {2},
  pages    = {89-110},
  volume   = {23},
  abstract = {In spatial reasoning, relationships between spatial entities play a major role. In image interpretation, computer vision and structural recognition, the management of imperfect information and of imprecision constitutes a key point. This calls for the framework of fuzzy sets, which exhibits nice features to represent spatial imprecision at different levels, imprecision in knowledge and knowledge representation, and which provides powerful tools for fusion, decision-making and reasoning. In this paper, we review the main fuzzy approaches for defining spatial relationships including topological (set relationships, adjacency) and metrical relations (distances, directional relative position).},
  doi      = {https://doi.org/10.1016/j.imavis.2004.06.013},
  file     = {:PDF/Fuzzy_spatial_review_Bloch2005.pdf:PDF},
  keywords = {Fuzzy spatial relationships, Degree of intersection, Degree of inclusion, Degree of adjacency, Distances, Directional relative position, Structural pattern recognition, Image interpretation, Spatial reasoning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0262885604001490},
}

@Article{Pierrard2021Spatial,
  author        = {Pierrard, R. and Poli, J.-P. and Hudelot, C.},
  journal       = {Artificial Intelligence},
  title         = {Spatial relation learning for explainable image classification and annotation in critical applications},
  year          = {2021},
  note          = {cited By 0},
  volume        = {292},
  art_number    = {103434},
  document_type = {Article},
  doi           = {10.1016/j.artint.2020.103434},
  file          = {:PDF/Spatial-relation-learning-for-explainable-image-classification-and-annotation-in-critical-applications2021Artificial-Intelligence.pdf:PDF},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097535218&doi=10.1016%2fj.artint.2020.103434&partnerID=40&md5=aaa35b76d6ea225b37454c1e99e3aa2d},
}

@Article{Clement2018Learning,
  author        = {Clement, M. and Kurtz, C. and Wendling, L.},
  journal       = {Pattern Recognition},
  title         = {Learning spatial relations and shapes for structural object description and scene recognition},
  year          = {2018},
  note          = {cited By 11},
  pages         = {197-210},
  volume        = {84},
  document_type = {Article},
  doi           = {10.1016/j.patcog.2018.06.017},
  file          = {:PDF/Learning-spatial-relations-and-shapes-for-structural-object-description-and-scene-recognition2018Pattern-Recognition.pdf:PDF},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050152123&doi=10.1016%2fj.patcog.2018.06.017&partnerID=40&md5=0d989b79c5df6e6734909c32381cd7c0},
}

@Conference{Pierrard2018Learning,
  author        = {Pierrard, R. and Poli, J.-P. and Hudelot, C.},
  title         = {Learning fuzzy relations and properties for explainable artificial intelligence},
  year          = {2018},
  note          = {cited By 3},
  volume        = {2018-July},
  art_number    = {8491559},
  document_type = {Conference Paper},
  doi           = {10.1109/Fuzz-Ieee.2018.8491538},
  file          = {:PDF/Learning_fuzzy_relations_Pierrard2018.pdf:PDF},
  journal       = {IEEE International Conference on Fuzzy Systems},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060479646&doi=10.1109%2fFuzz-Ieee.2018.8491538&partnerID=40&md5=71faa5d1ed83794a9cabf7208afbf638},
}

@Article{Ciaramella2006Fuzzy,
  author        = {Ciaramella, A. and Tagliaferri, R. and Pedrycz, W. and Di Nola, A.},
  journal       = {International Journal of Approximate Reasoning},
  title         = {Fuzzy relational neural network},
  year          = {2006},
  note          = {cited By 45},
  number        = {2},
  pages         = {146-163},
  volume        = {41},
  document_type = {Conference Paper},
  doi           = {10.1016/j.ijar.2005.06.016},
  file          = {:PDF/Fuzzy-relational-neural-network2006International-Journal-of-Approximate-Reasoning.pdf:PDF},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844466785&doi=10.1016%2fj.ijar.2005.06.016&partnerID=40&md5=4c5673f9003e0c659bd5d93d0bb44002},
}

@Article{Smeulders2000Content,
  author  = {A. W. M. {Smeulders} and M. {Worring} and S. {Santini} and A. {Gupta} and R. {Jain}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Content-based image retrieval at the end of the early years},
  year    = {2000},
  number  = {12},
  pages   = {1349-1380},
  volume  = {22},
  doi     = {10.1109/34.895972},
  file    = {:PDF/COntent_based_retrieval_PAMI_Smeulders2000.pdf:PDF},
}

@Article{Guesgen2015Fuzzy,
  author        = {Guesgen, H.W.},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {A fuzzy set approach to expressing preferences in spatial reasoning},
  year          = {2015},
  note          = {cited By 1},
  pages         = {173-185},
  volume        = {9060},
  document_type = {Article},
  doi           = {10.1007/978-3-319-14726-0_12},
  file          = {:PDF/A-fuzzy-set-approach-to-expressing-preferences-in-spatial-reasoning2015Lecture-Notes-in-Computer-Science-including-subseries-Lecture-Notes-in-Artificial-Intelligence-and-Lecture-Notes-in-Bioinformatics.pdf:PDF},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921769256&doi=10.1007%2f978-3-319-14726-0_12&partnerID=40&md5=43c3cf414eb068dfeed7c933560d399a},
}

@Conference{Naeem2015Relative,
  author        = {Naeem, M. and Matsakis, P.},
  booktitle     = {ICPRAM},
  title         = {Relative position descriptors a review},
  year          = {2015},
  note          = {cited By 7},
  pages         = {286-295},
  volume        = {1},
  document_type = {Conference Paper},
  doi           = {10.5220/0005211002860295},
  file          = {:PDF/ICPRAM_2015_78.pdf:PDF},
  journal       = {ICPRAM 2015 - 4th International Conference on Pattern Recognition Applications and Methods, Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938834316&doi=10.5220%2f0005211002860295&partnerID=40&md5=863f0a96c17ef2046ee402d9fac6b2a1},
}

@Article{Jaworski2010Use,
  author      = {Jaworski, T. and Kucharski, J.},
  journal     = {Automatyka / Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie},
  title       = {The use of fuzzy logic for description of spatial relations between objects},
  year        = {2010},
  pages       = {563-580},
  volume      = {T. 14, z. 3/1},
  abstract    = {Celem artykułu jest przedstawienie zestawienie przeglądu najważniejszych metod określania relacji przestrzennych między zbiorami ostrymi i rozmytymi w obrazach 2D. Rozpatrywane typy relacji to punkt - punkt, punkt - obiekt, obiekt ostry - obiekt ostry oraz obiekt rozmyty - obiekt rozmyty. Artykuł porusza również kierunek przyszłych badań nad wykorzystaniem rozmytych kierunkowych relacji przestrzennych do modelowania procesów dynamicznych na bazie tomogramów i termogramów.},
  affiliation = {Computer Engineering Department, Technical University of Lodz, Poland},
  file        = {:PDF/Jaworski.pdf:PDF},
  keywords    = {logika rozmyta; relacje przestrzenne; kierunkowe relacje przestrzenne},
  language    = {Polish},
  url         = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-article-AGH1-0025-0088},
}

@InProceedings{Zhang1993Fuzzy,
  author    = {W. {Zhang} and M. {Sugeno}},
  booktitle = {[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems},
  title     = {A fuzzy approach to scene understanding},
  year      = {1993},
  pages     = {564-569 vol.1},
  doi       = {10.1109/FUZZY.1993.327529},
  file      = {:PDF/A_Fuzzy_approach_Zhang_1993.pdf:PDF},
  timestamp = {2021-09-13},
}

@InProceedings{Wang2008Study,
  author    = {Wang, Xin and Matsakis, Pascal and Trick, Lana and Nonnecke, Blair and Veltman, Melanie},
  booktitle = {Headway in Spatial Data Handling},
  title     = {A Study on how Humans Describe Relative Positions of Image Objects},
  year      = {2008},
  address   = {Berlin, Heidelberg},
  editor    = {Ruas, Anne and Gold, Christopher},
  pages     = {1--18},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Information describing the layout of objects in space is commonly conveyed through the use of linguistic terms denoting spatial relations that hold between the objects. Though progress has been made in the understanding and modelling of many individual relations, a better understanding of how human subjects use spatial relations together in natural language to is required. This paper outlines the design and completion of an experiment resulting in the collection of 1920 spoken descriptions from 32 human subjects; they describe the relative positions of a variety of objects within an image space. We investigate the spatial relations that the subjects express in their descriptions, and the terms through which they do so, in an effort to determine variations and commonalities. Analysis of the descriptions determines that common elements of spatial perception do indeed exist between subjects, and that the subjects are quite consistent with each other in the use of spatial relations.},
  file      = {:PDF/Wang2008_Chapter_AStudyOnHowHumansDescribeRelat.pdf:PDF},
  isbn      = {978-3-540-68566-1},
  url       = {https://www.researchgate.net/publication/220885029_A_Study_on_how_Humans_Describe_Relative_Positions_of_Image_Objects},
}

@InProceedings{Egenhofer2015Qualitative,
  author    = {Egenhofer, Max J.},
  booktitle = {Studying Visual and Spatial Reasoning for Design Creativity},
  title     = {Qualitative Spatial-Relation Reasoning for Design},
  year      = {2015},
  address   = {Dordrecht},
  editor    = {Gero, John S.},
  pages     = {153--175},
  publisher = {Springer Netherlands},
  abstract  = {Qualitative spatial relations are symbol abstractions of geometric representations, which allow computational analyses independent of, but consistent with, graphical depictions. This paper compiles some of the most commonly used sets of qualitative spatial relations and their logical inference mechanisms. The abstract representation of the relations' interconnectedness in the form of their conceptual neighborhood graphs offers intriguing insight about the regularity of such complete sets of qualitative relations.},
  file      = {:PDF/Egenhofer2015_Chapter_QualitativeSpatial-RelationRea.pdf:PDF},
  isbn      = {978-94-017-9297-4},
  url       = {https://www.researchgate.net/publication/228570304_Qualitative_Spatial-Relation_Reasoning_for_Design},
}

@InCollection{Bloch2013Fuzziness,
  author    = {Bloch, Isabelle},
  booktitle = {On Fuzziness: A Homage to Lotfi A. Zadeh -- Volume 1},
  publisher = {Springer Berlin Heidelberg},
  title     = {Fuzzy Models of Spatial Relations, Application to Spatial Reasoning},
  year      = {2013},
  address   = {Berlin, Heidelberg},
  editor    = {Seising, Rudolf and Trillas, Enric and Moraga, Claudio and Termini, Settimo},
  isbn      = {978-3-642-35641-4},
  pages     = {51--58},
  abstract  = {Spatial relations are an important component of image content, that proved to be useful for recognition of individual objects and for image understanding. Indeed, spatial relations provide structural information about the scene, which is often more stable that individual object characteristics, can help disambiguating objects of similar appearance, and is often available as prior knowledge. A typical example is anatomy, where relations between anatomical structures are described in anatomical textbooks or dedicated web sites, and can be used to drive the interpretation of medical images. This will be illustrated on magnetic resonance images (MRI) of the brain, for segmenting and recognizing internal brain structures. This is a typical example where shape and appearance information may not be sufficient for recognition, in particular in pathological cases, while using structural knowledge is relevant and helps solving the problem. Similar examples can be found in understanding aerial and satellite images.},
  doi       = {10.1007/978-3-642-35641-4_8},
  file      = {:PDF/Bloch2013_Chapter_FuzzyModelsOfSpatialRelationsA.pdf:PDF},
  url       = {https://doi.org/10.1007/978-3-642-35641-4_8},
}

@InProceedings{Dutta1988Approximate,
  author    = {Dutta, Soumitra},
  booktitle = {Proceedings of the 1st International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 1},
  title     = {Approximate Spatial Reasoning},
  year      = {1988},
  address   = {New York, NY, USA},
  pages     = {126–140},
  publisher = {Association for Computing Machinery},
  series    = {IEA/AIE '88},
  abstract  = {It is a truism that much of human reasoning is approximate in nature. Spatial reasoning is an area where humans consistently reason approximately with demonstrably good results. The unique mental processes behind these actions are not well understood. However, it is important that we try to incorporate such approximate reasoning techniques in our computer systems. Approximate spatial reasoning is very important for intelligent mobile agents (e.g., robots), specially for those operating in uncertain or unknown or dynamic domains. In such situations, besides the hazard in the domain, the real constraints (e.g., limited memory and limited time for observations and/or inferencing) faced by agents makes the use of approximate reasoning techniques imperative. In this paper we present a model for approximate spatial reasoning using fuzzy logic to represent the uncertainty in the environment. We develop algorithms to reason about spatial information expressed in the form of approximate linguistic descriptions, very similar to the kind of spatial information processed by humans. We only deal with static spatial reasoning in this paper.},
  doi       = {10.1145/51909.51925},
  file      = {:PDF/Approximate_spatial_Dutta1988.pdf:PDF},
  isbn      = {0897912713},
  location  = {Tullahoma, Tennessee, USA},
  numpages  = {15},
  url       = {https://doi.org/10.1145/51909.51925},
}

@Article{Levitt1990Qualititative,
  author  = {Tod S. Levitt and Daryl T. Lawton},
  journal = {Artificial Intelligence},
  title   = {Qualitative navigation for mobile robots},
  year    = {1990},
  issn    = {0004-3702},
  number  = {3},
  pages   = {305-360},
  volume  = {44},
  doi     = {https://doi.org/10.1016/0004-3702(90)90027-W},
  url     = {https://www.sciencedirect.com/science/article/pii/000437029090027W},
}

@InProceedings{Sharma1995Inferences,
  author    = {Sharma, Jayant and Flewelling, Douglas M.},
  booktitle = {Advances in Spatial Databases},
  title     = {Inferences from combined knowledge about topology and directions},
  year      = {1995},
  address   = {Berlin, Heidelberg},
  editor    = {Egenhofer, Max J. and Herring, John R.},
  pages     = {279--291},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Separate mechanisms exist for the compositions of binary topological relations and binary direction relations. They are appropriate for homogeneous spatial reasoning, i.e., the inference of new topological relations from a set of topological relations, or the derivation of new direction relations from a set of direction relations; however, these composition mechanisms are insufficient for heterogeneous spatial reasoning, such as the inference of spatial relations from the combination of topological relations and cardinal directions. This paper discusses the shortcomings of current inference methods for heterogeneous spatial reasoning and presents a new method for heterogeneous directiontopology reasoning. The results demonstrate that using a canonical model, in particular Allen's interval relations, leads to a powerful heterogeneous reasoning mechanism. The spatial objects are approximated by their minimum bounding rectangles and the topological and direction relations are mapped onto interval relations. Compositions are performed using the composition table for interval relations. The results of the compositions are then reverse mapped onto directions. This process enables complex three-step inferences over topological and direction relations such as A West of B, B overlap C, and C West of D imply A West of D.},
  file      = {:PDF/Sharma-Flewelling1995_Chapter_InferencesFromCombinedKnowledg.pdf:PDF},
  isbn      = {978-3-540-49536-9},
}

@InProceedings{Tan2014Grounding,
  author    = {J. {Tan} and Z. {Ju} and H. {Liu}},
  booktitle = {2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
  title     = {Grounding spatial relations in natural language by fuzzy representation for human-robot interaction},
  year      = {2014},
  pages     = {1743-1750},
  doi       = {10.1109/FUZZ-IEEE.2014.6891797},
  file      = {:PDF/Grounding_spatial_relations_Tan_2014.pdf:PDF},
}

@InProceedings{Gader1997Fuzzy,
  author    = {P. D. {Gader}},
  booktitle = {Proceedings of 6th International Fuzzy Systems Conference},
  title     = {Fuzzy spatial relations based on fuzzy morphology},
  year      = {1997},
  pages     = {1179-1183 vol.2},
  volume    = {2},
  doi       = {10.1109/FUZZY.1997.622875},
  file      = {:PDF/Fuzzy_spatial_relations_Gader_1997.pdf:PDF},
  test      = {dfddfdfdfdf},
}

@Article{Krishnapuram1993Qualititative,
  author  = {R. {Krishnapuram} and J. M. {Keller} and Y. {Ma}},
  journal = {IEEE Transactions on Fuzzy Systems},
  title   = {Quantitative analysis of properties and spatial relations of fuzzy image regions},
  year    = {1993},
  number  = {3},
  pages   = {222-233},
  volume  = {1},
  doi     = {10.1109/91.236554},
  file    = {:PDF/Quantitative_Analysis_Krishnapuram_1993.pdf:PDF},
}

@Article{Matsakis1999New,
  author  = {P. {Matsakis} and L. {Wendling}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {A new way to represent the relative position between areal objects},
  year    = {1999},
  number  = {7},
  pages   = {634-643},
  volume  = {21},
  doi     = {10.1109/34.777374},
  file    = {:PDF/A_new_way_Matsakis_1999.pdf:PDF},
}

@InProceedings{Keller1999Aspects,
  author    = {J. M. {Keller} and P. {Matsakis}},
  booktitle = {FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315)},
  title     = {Aspects of high level computer vision using fuzzy sets},
  year      = {1999},
  pages     = {847-852 vol.2},
  volume    = {2},
  doi       = {10.1109/FUZZY.1999.793059},
  file      = {:PDF/Axpects_of_high_level_Keller_1999.pdf:PDF},
}

@InProceedings{Abella1999From,
  author    = {A. {Abella} and J. R. {Kender}},
  booktitle = {Proceedings Integration of Speech and Image Understanding},
  title     = {From images to sentences via spatial relations},
  year      = {1999},
  pages     = {117-146},
  doi       = {10.1109/ISIU.1999.824875},
  file      = {:PDF/From_images_to_sentences_Abella_1999.pdf:PDF},
}

@Article{Allen1983Maintaining,
  author     = {Allen, James F.},
  journal    = {Commun. ACM},
  title      = {Maintaining Knowledge about Temporal Intervals},
  year       = {1983},
  issn       = {0001-0782},
  month      = nov,
  number     = {11},
  pages      = {832–843},
  volume     = {26},
  address    = {New York, NY, USA},
  doi        = {10.1145/182.358434},
  file       = {:PDF/Knowledge_temporal_Allen_1983.pdf:PDF},
  issue_date = {Nov. 1983},
  keywords   = {interval reasoning, interval representation, temporal interval},
  numpages   = {12},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/182.358434},
}

@Article{Keller2000Fuzzy,
  author   = {James M. Keller and Xiaomei Wang},
  journal  = {Computer Vision and Image Understanding},
  title    = {A Fuzzy Rule-Based Approach to Scene Description Involving Spatial Relationships},
  year     = {2000},
  issn     = {1077-3142},
  number   = {1},
  pages    = {21-41},
  volume   = {80},
  abstract = {Automated scene description is a very important part of, but also a very hard task in, high-level computer vision. Today, scene description is being applied in real problems such as robotic navigation. In this paper, we present a fuzzy rule-based approach that accomplishes the task of automated linguistic scene description. Membership functions for spatial relations between different components in the scene are evaluated, guided by a spatial relationship matrix that describes typical scene entities for which these relative locations are desirable. A fuzzy logic rule-based system is developed to combine the spatial relationships and other important scene properties to generate the final linguistic interpretation. Excellent results from several image examples of different types show the applicability of this approach.},
  doi      = {https://doi.org/10.1006/cviu.2000.0872},
  file     = {:PDF/Fuzzy_rule_Keller_2000.pdf:PDF},
  url      = {https://www.sciencedirect.com/science/article/pii/S1077314200908725},
}

@InBook{Matsakis2010General,
  author    = {Matsakis, Pascal and Wendling, Laurent and Ni, JingBo},
  editor    = {Jeansoulin, Robert and Papini, Odile and Prade, Henri and Schockaert, Steven},
  pages     = {49--74},
  publisher = {Springer Berlin Heidelberg},
  title     = {A General Approach to the Fuzzy Modeling of Spatial Relationships},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-14755-5},
  abstract  = {How to satisfactorily model spatial relationships between 2D or 3D objects? If the objects are far enough from each other, they can be approximated by their centers. If they are not too far, not too close, they can be approximated by their minimum bounding rectangles or boxes. If they are close, no such simplifying approximation should be made. Two concepts are at the core of the approach described in this paper: the concept of the {\$}{\backslash}mathcal{\{}F{\}}{\$}-histogram and that of the {\$}{\backslash}mathcal{\{}F{\}}{\$}-template. The basis of the former was laid a decade ago; since then, it has naturally evolved and matured. The latter is much newer, and has dual characteristics. Our aim here is to present a snapshot of these concepts and of their applications. It is to highlight (and reflect on) their duality--a duality that calls for a clear distinction between the terms spatial relationship, relationship to a reference object, and relative position. Finally, it is to identify directions for future research.},
  booktitle = {Methods for Handling Imperfect Spatial Information},
  doi       = {10.1007/978-3-642-14755-5_3},
  file      = {:PDF/Matsakis2010_Chapter_AGeneralApproachToTheFuzzyMode.pdf:PDF},
  url       = {https://doi.org/10.1007/978-3-642-14755-5_3},
}

@Article{Li2012Reasoning,
  author   = {Li, Sanjiang and Cohn, Anthony G.},
  journal  = {Computational Intelligence},
  title    = {REASONING WITH TOPOLOGICAL AND DIRECTIONAL SPATIAL INFORMATION},
  year     = {2012},
  number   = {4},
  pages    = {579-616},
  volume   = {28},
  abstract = {Current research on qualitative spatial representation and reasoning mainly focuses on one single aspect of space. In real-world applications, however, multiple spatial aspects are often involved simultaneously. This paper investigates problems arising in reasoning with combined topological and directional information. We use the RCC8 algebra and the rectangle algebra (RA) for expressing topological and directional information, respectively. We give examples to show that the bipath-consistency algorithm Bipath-Consistency is incomplete for solving even basic RCC8 and RA constraints. If topological constraints are taken from some maximal tractable subclasses of RCC8, and directional constraints are taken from a subalgebra, termed DIR49, of RA, then we show that Bipath-Consistency is able to separate topological constraints from directional ones. This means, given a set of hybrid topological and directional constraints from the above subclasses of RCC8 and RA, we can transfer the joint satisfaction problem in polynomial time to two independent satisfaction problems in RCC8 and RA. For general RA constraints, we give a method to compute solutions that satisfy all topological constraints and approximately satisfy each RA constraint to any prescribed precision.},
  doi      = {https://doi.org/10.1111/j.1467-8640.2012.00431.x},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00431.x},
  file     = {:PDF/Reasoning_Li_2012.pdf:PDF},
  keywords = {qualitative spatial reasoning, topology, region connection calculus, rectangle algebra, joint satisfaction problem, approximation},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00431.x},
}

@Article{Egenhofer1991Point,
  author    = {MAX J. EGENHOFER and ROBERT D. FRANZOSA},
  journal   = {International Journal of Geographical Information Systems},
  title     = {Point-set topological spatial relations},
  year      = {1991},
  number    = {2},
  pages     = {161-174},
  volume    = {5},
  abstract  = {Abstract Practical needs in geographic information systems (GIS) have led to the investigation of formal and sound methods of describing spatial relations. After an introduction to the basic ideas and notions of topology, a novel theory of topological spatial relations between sets is developed in which the relations are defined in terms of the intersections of the boundaries and interiors of two sets. By considering empty and non-empty as the values of the intersections, a total of sixteen topological spatial relations is described, each of which can be realized in R 2. This set is reduced to nine relations if the sets are restricted to spatial regions, a fairly broad class of subsets of a connected topological space with an application to GIS. It is shown that these relations correspond to some of the standard set theoretical and topological spatial relations between sets such as equality, disjointness and containment in the interior.},
  doi       = {10.1080/02693799108927841},
  eprint    = {https://doi.org/10.1080/02693799108927841},
  file      = {:PDF/Egenhofer_Franzosa_1991.pdf:PDF},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/02693799108927841},
}

@Article{Cohn1997Qualititative,
  author  = {Anthony G. Cohn and Brandon Bennett and John Gooday and Nicholas Mark Gotts},
  journal = {Geoinformatica},
  title   = {Qualitative Spatial Representation and Reasoning with the Region Connection Calculus},
  year    = {1997},
  pages   = {275-316},
  volume  = {1},
  file    = {:PDF/Cohn1997_Article_QualitativeSpatialRepresentati.pdf:PDF},
  url     = {https://www.researchgate.net/publication/2659953_Qualitative_Spatial_Representation_and_Reasoning_with_the_Region_Connection_Calculus},
}

@InProceedings{Randell1992Spatial,
  author    = {Randell, David A. and Cui, Zhan and Cohn, Anthony G.},
  booktitle = {Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning},
  title     = {A Spatial Logic Based on Regions and Connection},
  year      = {1992},
  address   = {San Francisco, CA, USA},
  pages     = {165–176},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {KR'92},
  file      = {:PDF/A_Spatial_Logic_based_on_Regions_and_Connection.pdf:PDF},
  isbn      = {1558602623},
  location  = {Cambridge, MA},
  numpages  = {12},
  url       = {https://www.researchgate.net/publication/221393453_A_Spatial_Logic_based_on_Regions_and_Connection},
}

@Article{Russakovsky2015Imagenet,
  author   = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  journal  = {International Journal of Computer Vision},
  title    = {ImageNet Large Scale Visual Recognition Challenge},
  year     = {2015},
  issn     = {1573-1405},
  month    = {Dec},
  number   = {3},
  pages    = {211-252},
  volume   = {115},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  day      = {01},
  doi      = {10.1007/s11263-015-0816-y},
  file     = {:PDF/Russakovsky2015_Article_ImageNetLargeScaleVisualRecogn.pdf:PDF},
  url      = {https://doi.org/10.1007/s11263-015-0816-y},
}

@Article{Zadeh1965Fuzzy,
  author   = {L.A. Zadeh},
  journal  = {Information and Control},
  title    = {Fuzzy sets},
  year     = {1965},
  issn     = {0019-9958},
  number   = {3},
  pages    = {338-353},
  volume   = {8},
  abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.},
  doi      = {https://doi.org/10.1016/S0019-9958(65)90241-X},
  file     = {:PDF/Zadeh65.pdf:PDF},
  url      = {https://www.sciencedirect.com/science/article/pii/S001999586590241X},
}

@Article{Mamdani1999eltitExperiment,
  author   = {E.H. MAMDANI and S. ASSILIAN},
  journal  = {International Journal of Human-Computer Studies},
  title    = {An Experiment in Linguistic Synthesis with a Fuzzy Logic Controller},
  year     = {1999},
  issn     = {1071-5819},
  number   = {2},
  pages    = {135-147},
  volume   = {51},
  abstract = {This paper describes an experiment on the “linguistic” synthesis of a controller for a model industrial plant (a steam engine). Fuzzy logic is used to convert heuristic control rules stated by a human operator into an automatic control strategy. The experiment was initiated to investigate the possibility of human interaction with a learning controller. However, the control strategy set up linguistically proved to be far better than expected in its own right, and the basic experiment of linguistic control synthesis in a non-learning controller is reported here.},
  doi      = {https://doi.org/10.1006/ijhc.1973.0303},
  file     = {:PDF/Mamdani1999.pdf:PDF},
  url      = {https://www.sciencedirect.com/science/article/pii/S1071581973603035},
}

@Article{Zadeh1973Outline,
  author  = {L. A. {Zadeh}},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  title   = {Outline of a New Approach to the Analysis of Complex Systems and Decision Processes},
  year    = {1973},
  number  = {1},
  pages   = {28-44},
  volume  = {SMC-3},
  doi     = {10.1109/TSMC.1973.5408575},
  file    = {:PDF/Zadeh1973.pdf:PDF},
}

@Article{Mamdani1974Applications,
  author  = {E. Mamdani},
  journal = {Proceedings of the IEEE},
  title   = {Applications of fuzzy algorithms for control of a simple dynamic plant},
  year    = {1974},
  doi     = {10.1049/piee.1974.0328},
  url     = {https://digital-library.theiet.org/content/journals/10.1049/piee.1974.0328},
}

@Article{Jegou2011Product,
  author  = {H. {Jégou} and M. {Douze} and C. {Schmid}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Product Quantization for Nearest Neighbor Search},
  year    = {2011},
  number  = {1},
  pages   = {117-128},
  volume  = {33},
  doi     = {10.1109/TPAMI.2010.57},
  file    = {:PDF/Product Quantization for Nearest Neighbor Search.pdf:PDF},
}

@Article{Migalska2018Density,
  author    = {Migalska, Agata},
  journal   = {Applied Stochastic Models in Business and Industry},
  title     = {Density-free test for symmetry verification in images},
  year      = {2018},
  number    = {5},
  pages     = {618-632},
  volume    = {34},
  abstract  = {Presence of symmetry is utilized in multiple machine vision systems to help achieve their goals. In numerous scenarios, this goal is to verify that certain symmetry is indeed exhibited by an image. However, we find that there is a shortage of methods for symmetry verification that would be capable of asserting an arbitrary reflectional or rotational symmetry. Using symmetry detectors to merely perform symmetry verification is improvident and not justified. We thus propose a novel statistical test for symmetry verification that fulfills the requirement of versatility. The proposed test is based on the principle that if an image is invariant to some hypothesized transformation, then an averaged image, obtained by averaging pixel intensities of an input image and of its transformed copy, looks exactly the same as an input image. Adopting the viewpoint that images are visual messages that convey some information allows us to expect that the amount of information in both images is the same. On the contrary, an incorrectly chosen transformation shall result in the information content being different. Based on this equality, we construct the test statistic and show that, when samples are large, the test statistic is asymptotically normally distributed. Finally, to verify the validity of the proposed principle and the performance of the method, a meticulous experimental study was performed on a large set of images. The results of this study confirmed the postulated ability of the method to verify an arbitrary symmetry and are demonstrated with several examples.},
  doi       = {https://doi.org/10.1002/asmb.2321},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.2321},
  file      = {:pdf/Migalska2018.pdf:PDF},
  keywords  = {density-free test, information theory, negentropy, statistical inference, symmetry verification},
  owner     = {Marcin},
  timestamp = {2021-05-10},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2321},
}


@Article{Everingham2010,
  author   = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  title    = {The Pascal Visual Object Classes (VOC) Challenge},
  journal  = {International Journal of Computer Vision},
  year     = {2010},
  volume   = {88},
  number   = {2},
  pages    = {303-338},
  month    = {Jun},
  issn     = {1573-1405},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
  day      = {01},
  doi      = {10.1007/s11263-009-0275-4},
  file     = {:PDF/Everingham2010_Article_ThePascalVisualObjectClassesVO.pdf:PDF},
  url      = {https://doi.org/10.1007/s11263-009-0275-4},
}

@InProceedings{Lin2014Microsoft,
  author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Microsoft COCO: Common Objects in Context},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {740--755},
  publisher = {Springer International Publishing},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  file      = {:PDF/Lin2014_Chapter_MicrosoftCOCOCommonObjectsInCo.pdf:PDF},
  isbn      = {978-3-319-10602-1},
  url       = {http://arxiv.org/abs/1405.0312},
}

@InProceedings{Girshick2015Fast,
  author    = {R. {Girshick}},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Fast R-CNN},
  year      = {2015},
  pages     = {1440-1448},
  doi       = {10.1109/ICCV.2015.169},
  file      = {:PDF/FastRCNN_Girshick2015.pdf:PDF},
}

@Article{LeCun2015Deep,
  author   = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal  = {Nature},
  title    = {Deep learning},
  year     = {2015},
  issn     = {1476-4687},
  month    = {May},
  number   = {7553},
  pages    = {436-444},
  volume   = {521},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  day      = {01},
  doi      = {10.1038/nature14539},
  file     = {:PDF/nature14539.pdf:PDF},
  url      = {https://doi.org/10.1038/nature14539},
}

@Misc{Tadeusiewicz2015Leksykon,
  author    = {Tadeusiewicz, Ryszard and Sokołowski Ośrodek Kultury},
  title     = {Leksykon sieci neuronowych},
  year      = {2015},
  address   = {Wrocław},
  booktitle = {Leksykon sieci neuronowych},
  file      = {:PDF/Leksykon_sieci_neuronowych-1.pdf:PDF},
  isbn      = {9788363270100},
  keywords  = {Sieci neuronowe (informatyka)},
  language  = {pol},
  publisher = {Wydawnictwo Fundacji "Projekt Nauka"},
}


@InProceedings{Redmon2016You,
  author    = {J. {Redmon} and S. {Divvala} and R. {Girshick} and A. {Farhadi}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  year      = {2016},
  pages     = {779-788},
  doi       = {10.1109/CVPR.2016.91},
  file      = {:PDF/Redmon2016.pdf:PDF},
}

@InProceedings{Liu2016SSD,
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  booktitle = {Computer Vision -- ECCV 2016},
  title     = {SSD: Single Shot MultiBox Detector},
  year      = {2016},
  address   = {Cham},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {21--37},
  publisher = {Springer International Publishing},
  abstract  = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3 {\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9 {\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  file      = {:PDF/Liu2016_Chapter_SSDSingleShotMultiBoxDetector.pdf:PDF},
  isbn      = {978-3-319-46448-0},
  url       = {https://arxiv.org/pdf/1512.02325.pdf},
}

@Article{Landola2016SqueezeNet,
  author       = {Forrest N. Iandola and Matthew W. Moskewicz and Khalid Ashraf and Song Han and William J. Dally and Kurt Keutzer},
  journal      = {CoRR},
  title        = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}1MB model size},
  year         = {2016},
  volume       = {abs/1602.07360},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/IandolaMAHDK16.bib},
  eprint       = {1602.07360},
  eprinttype   = {arXiv},
  file         = {:PDF/Iandola2016.pdf:PDF},
  primaryclass = {cs.CV},
  timestamp    = {Fri, 20 Nov 2020 16:16:06 +0100},
  url          = {http://arxiv.org/abs/1602.07360},
}

@Article{Ren2017Faster,
  author  = {S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  year    = {2017},
  number  = {6},
  pages   = {1137-1149},
  volume  = {39},
  doi     = {10.1109/TPAMI.2016.2577031},
  file    = {:PDF/Faster_R-CNN_Ren2017.pdf:PDF},
}

@InProceedings{He2017Mask,
  author    = {K. {He} and G. {Gkioxari} and P. {Dollár} and R. {Girshick}},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Mask R-CNN},
  year      = {2017},
  pages     = {2980-2988},
  doi       = {10.1109/ICCV.2017.322},
  file      = {:PDF/Mask_R_CNN_He_2017.pdf:PDF},
}

@Article{Howard2017Mobilenets,
  author     = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
  journal    = {CoRR},
  title      = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  year       = {2017},
  volume     = {abs/1704.04861},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/HowardZCKWWAA17.bib},
  eprint     = {1704.04861},
  eprinttype = {arXiv},
  file       = {:PDF/Mobile_net_Howard2017.pdf:PDF},
  timestamp  = {Thu, 27 May 2021 16:20:51 +0200},
  url        = {http://arxiv.org/abs/1704.04861},
}

@InProceedings{Redmon2017YOLO9000,
  author    = {J. {Redmon} and A. {Farhadi}},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {YOLO9000: Better, Faster, Stronger},
  year      = {2017},
  pages     = {6517-6525},
  doi       = {10.1109/CVPR.2017.690},
  file      = {:PDF/YOLO9000Redmon.pdf:PDF},
}

@Article{Agarwal2018Recent,
  author        = {Shivang Agarwal and Jean Ogier du Terrail and Frederic Jurie},
  journal       = {CoRR},
  title         = {Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks},
  year          = {2018},
  volume        = {abs/1809.03193},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1809-03193.bib},
  eprint        = {1809.03193},
  file          = {:PDF/Recent_Advances_Agarval_2018.pdf:PDF},
  timestamp     = {Wed, 18 Sep 2019 08:24:15 +0200},
  url           = {http://arxiv.org/abs/1809.03193},
}

@InProceedings{Sandler2018MobilenetV2,
  author    = {M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  year      = {2018},
  pages     = {4510-4520},
  doi       = {10.1109/CVPR.2018.00474},
  file      = {:PDF/MobleNetv2_Sandler2018.pdf:PDF},
}

@Misc{Redmon2018YOLOv3,
  author       = {Joseph Redmon and Ali Farhadi},
  title        = {YOLOv3: An Incremental Improvement},
  year         = {2018},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1804-02767.bib},
  eprint       = {1804.02767},
  eprinttype   = {arXiv},
  file         = {:PDF/Redmon2018.pdf:PDF},
  journal      = {CoRR},
  primaryclass = {cs.CV},
  timestamp    = {Mon, 13 Aug 2018 16:48:24 +0200},
  url          = {http://arxiv.org/abs/1804.02767},
  volume       = {abs/1804.02767},
}

@Misc{Lin2018Focal,
  author       = {Tsung{-}Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll{\'{a}}r},
  title        = {Focal Loss for Dense Object Detection},
  year         = {2017},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1708-02002.bib},
  eprint       = {1708.02002},
  eprinttype   = {arXiv},
  file         = {:PDF/RetinaNet_Lin2018.pdf:PDF},
  journal      = {CoRR},
  primaryclass = {cs.CV},
  timestamp    = {Mon, 13 Aug 2018 16:46:12 +0200},
  url          = {http://arxiv.org/abs/1708.02002},
  volume       = {abs/1708.02002},
}

@Article{Jiao2019Survey,
  author  = {L. {Jiao} and F. {Zhang} and F. {Liu} and S. {Yang} and L. {Li} and Z. {Feng} and R. {Qu}},
  journal = {IEEE Access},
  title   = {A Survey of Deep Learning-Based Object Detection},
  year    = {2019},
  pages   = {128837-128868},
  volume  = {7},
  doi     = {10.1109/ACCESS.2019.2939201},
  file    = {:PDF/Jiao2019.pdf:PDF},
}

@Article{Zhao2019Object,
  author  = {Z. {Zhao} and P. {Zheng} and S. {Xu} and X. {Wu}},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  title   = {Object Detection With Deep Learning: A Review},
  year    = {2019},
  number  = {11},
  pages   = {3212-3232},
  volume  = {30},
  doi     = {10.1109/TNNLS.2018.2876865},
  file    = {:PDF/Object_detection_Zhong_2019.pdf:PDF},
}

@Article{Liu2019Deep,
  author   = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
  journal  = {International Journal of Computer Vision},
  title    = {Deep Learning for Generic Object Detection: A Survey},
  year     = {2020},
  issn     = {1573-1405},
  month    = {Feb},
  number   = {2},
  pages    = {261-318},
  volume   = {128},
  abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.},
  day      = {01},
  doi      = {10.1007/s11263-019-01247-4},
  file     = {:PDF/Liu2018.pdf:PDF},
  url      = {https://doi.org/10.1007/s11263-019-01247-4},
}

@Article{Khan2020Survey,
  author    = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
  journal   = {Artificial Intelligence Review},
  title     = {A survey of the recent architectures of deep convolutional neural networks},
  year      = {2020},
  issn      = {1573-7462},
  month     = {Apr},
  number    = {8},
  pages     = {5455–5516},
  volume    = {53},
  doi       = {10.1007/s10462-020-09825-6},
  file      = {:PDF/Survey_CNN_Khan2019.pdf:PDF},
  publisher = {Springer Science and Business Media LLC},
  url       = {http://dx.doi.org/10.1007/s10462-020-09825-6},
}

@InProceedings{Girshick2014Rich,
  author    = {R. Girshick and J. Donahue and T. Darrell and J. Malik},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  year      = {2014},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {580-587},
  publisher = {IEEE Computer Society},
  abstract  = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  doi       = {10.1109/CVPR.2014.81},
  file      = {:PDF/Girshick2014.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {proposals;feature extraction;training;visualization;object detection;vectors;support vector machines},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2014.81},
}

@InProceedings{Szegedy2014Going,
  author    = {C. Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Going deeper with convolutions},
  year      = {2015},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {1-9},
  publisher = {IEEE Computer Society},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  doi       = {10.1109/CVPR.2015.7298594},
  file      = {:PDF/Going_deeper_with_convolutions.pdf:PDF},
  issn      = {1063-6919},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298594},
}

@InProceedings{Hoiem2012Diagnosing,
  author    = {Hoiem, Derek and Chodpathumwan, Yodsawalai and Dai, Qieyun},
  booktitle = {Computer Vision -- ECCV 2012},
  title     = {Diagnosing Error in Object Detectors},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  editor    = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  pages     = {340--353},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives. In particular, we examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects, other labeled objects, and background. We analyze two classes of detectors: the Vedaldi et al. multiple kernel learning detector and different versions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error. Our analysis also reveals that many different kinds of improvement are necessary to achieve large gains, making more detailed analysis essential for the progress of recognition research. By making our software and annotations available, we make it effortless for future researchers to perform similar analysis.},
  doi       = {10.1007/978-3-642-33712-3_25},
  file      = {:pdf/eccv2012_detanalysis_derek.pdf:PDF},
  isbn      = {978-3-642-33712-3},
  owner     = {Marcin},
  timestamp = {2021-05-21},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-33712-3_25},
}

@Article{Balakrishnan2014Stemming,
  author  = {Balakrishnan, Vimala and Ethel, Lloyd-Yemoh},
  journal = {Lecture Notes on Software Engineering},
  title   = {Stemming and Lemmatization: A Comparison of Retrieval Performances},
  year    = {2014},
  month   = {01},
  pages   = {262-267},
  volume  = {2},
  doi     = {10.7763/LNSE.2014.V2.134},
  file    = {:PDF/Stemming and Lemmatization- A Comparison of Retrieval Performances.pdf:PDF},
}

@InProceedings{Callison2006Re-evaluating,
  author    = {Callison-Burch, Chris and Osborne, Miles and Koehn, Philipp},
  booktitle = {11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics},
  title     = {Re-evaluating the Role of {B}leu in Machine Translation Research},
  year      = {2006},
  address   = {Trento, Italy},
  month     = apr,
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/Re-evaluating the Role of {B}leu in Machine Translation Research.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/E06-1032},
}

@Article{Chen2015microsoftCOCO,
  author     = {Xinlei Chen and Hao Fang and Tsung{-}Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Doll{\'{a}}r and C. Lawrence Zitnick},
  journal    = {CoRR},
  title      = {Microsoft {COCO} Captions: Data Collection and Evaluation Server},
  year       = {2015},
  volume     = {abs/1504.00325},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/ChenFLVGDZ15.bib},
  eprint     = {1504.00325},
  eprinttype = {arXiv},
  timestamp  = {Wed, 16 Oct 2019 16:31:45 +0200},
  url        = {http://arxiv.org/abs/1504.00325},
}

@InProceedings{Karpathy2015Deep,
  author    = {A. Karpathy and L. Fei-Fei},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep visual-semantic alignments for generating image descriptions},
  year      = {2015},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {3128-3137},
  publisher = {IEEE Computer Society},
  abstract  = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
  doi       = {10.1109/CVPR.2015.7298932},
  file      = {:PDF/Deep visual-semantic alignments for generating image descriptions.pdf:PDF},
  groups    = {multimodal, global CNN features},
  issn      = {1063-6919},
  printed   = {yes},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298932},
}

@Article{Youngetal2014Image,
  author   = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal  = {Transactions of the Association for Computational Linguistics},
  title    = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  year     = {2014},
  pages    = {67--78},
  volume   = {2},
  abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.},
  doi      = {10.1162/tacl_a_00166},
  file     = {:PDF/From image descriptions to visual denotations- New similarity metrics for semantic inference over event descriptions.pdf:PDF},
  url      = {https://aclanthology.org/Q14-1006},
}

@InProceedings{Wroblewska2018PolishCorpus,
  author    = {Wr{\'o}blewska, Alina},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  title     = {{P}olish Corpus of Annotated Descriptions of Images},
  year      = {2018},
  address   = {Miyazaki, Japan},
  month     = may,
  publisher = {European Language Resources Association (ELRA)},
  file      = {:PDF/polish corpus of annotated descriptions for images.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/L18-1337},
}

@InProceedings{Ward2002Corpus,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Henderson, John and Reeder, Florence},
  booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},
  title     = {Corpus-Based Comprehensive and Diagnostic MT Evaluation: Initial Arabic, Chinese, French, and Spanish Results},
  year      = {2002},
  address   = {San Francisco, CA, USA},
  pages     = {132–137},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {HLT '02},
  file      = {:PDF/Corpus-based Comprehensive and Diagnostic MT Evaluation- Initial Arabic, Chinese, French, and Spanish Result.pdf:PDF},
  location  = {San Diego, California},
  numpages  = {6},
  url       = {https://dl.acm.org/doi/pdf/10.5555/1289189.1289272},
}

@InProceedings{Vedantam2015Cider,
  author    = {R. Vedantam and C. Zitnick and D. Parikh},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {CIDEr: Consensus-based image description evaluation},
  year      = {2015},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {4566-4575},
  publisher = {IEEE Computer Society},
  abstract  = {Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.},
  doi       = {10.1109/CVPR.2015.7299087},
  file      = {:PDF/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf:PDF},
  groups    = {metryki},
  issn      = {1063-6919},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7299087},
}

@InProceedings{Ushiku2012Efficient,
  author    = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
  booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
  title     = {Efficient Image Annotation for Automatic Sentence Generation},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {549–558},
  publisher = {Association for Computing Machinery},
  series    = {MM '12},
  doi       = {10.1145/2393347.2393424},
  file      = {:PDF/Efficient Image Annotation for Automatic Sentence Generation.pdf:PDF},
  isbn      = {9781450310895},
  keywords  = {online learning, passive-aggressive, multi-stack decoding},
  location  = {Nara, Japan},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2393347.2393424},
}

@Article{Spratling2004Feedback,
  author    = {Spratling, Mike W and Johnson, Mark H},
  journal   = {Journal of cognitive neuroscience},
  title     = {A feedback model of visual attention},
  year      = {2004},
  number    = {2},
  pages     = {219--237},
  volume    = {16},
  file      = {:PDF/A feedback model of visual attention.pdf:PDF},
  publisher = {MIT Press},
  url       = {https://core.ac.uk/reader/29880237?utm_source=linkout},
}

@InProceedings{Shuying2015VeryDeep,
  author    = {Liu, Shuying and Deng, Weihong},
  booktitle = {2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)},
  title     = {Very deep convolutional neural network based image classification using small training sample size},
  year      = {2015},
  pages     = {730-734},
  doi       = {10.1109/ACPR.2015.7486599},
  file      = {:PDF/Very_deep_convolutional_neural_network_based_image_classification_using_small_training_sample_size.pdf:PDF},
}

@Article{Rensink2000Dynamic,
  author  = {Rensink, Ronald},
  journal = {Visual Cognition},
  title   = {The Dynamic Representation of Scenes},
  year    = {2000},
  month   = {01},
  pages   = {17-42},
  volume  = {7},
  doi     = {10.1080/135062800394667},
  file    = {:PDF/The Dynamic Representation of Scenes .pdf:PDF},
}

@InProceedings{RashtchianEtal2010Collecting,
  author    = {Rashtchian, Cyrus and Young, Peter and Hodosh, Micah and Hockenmaier, Julia},
  booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk},
  title     = {Collecting Image Annotations Using {A}mazon{'}s {M}echanical {T}urk},
  year      = {2010},
  address   = {Los Angeles},
  month     = jun,
  pages     = {139--147},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/Collecting Image Annotations Using Amazon’s Mechanical Turk.pdf:PDF;:PDF/The Role of Syntactic Planning in Compositional Image Captioning.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/W10-0721},
}

@InProceedings{Papineni2002Bleu,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  title     = {BLEU: A Method for Automatic Evaluation of Machine Translation},
  year      = {2002},
  address   = {USA},
  pages     = {311–318},
  publisher = {Association for Computational Linguistics},
  series    = {ACL '02},
  abstract  = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  doi       = {10.3115/1073083.1073135},
  file      = {:PDF/BLEU- A Method for Automatic Evaluation of Machine Translation.pdf:PDF},
  groups    = {metryki},
  location  = {Philadelphia, Pennsylvania},
  numpages  = {8},
  url       = {https://doi.org/10.3115/1073083.1073135},
}

@Misc{polish-nlp-resources,
    author =       {S{\l}awomir Dadas},
    title =        {A repository of Polish {NLP} resources},
    howpublished = {Github},
    year =         {2019},
    url =          {https://github.com/sdadas/polish-nlp-resources/}
}

@InProceedings{Wolinski2014Morfeusz,
  author    = {Woli{\'n}ski, Marcin},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
  title     = {Morfeusz Reloaded},
  year      = {2014},
  address   = {Reykjavik, Iceland},
  month     = may,
  publisher = {European Language Resources Association (ELRA)},
  file      = {:PDF/morfeusz reloaded.pdf:PDF},
  url       = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/768_Paper.pdf},
}

@Proceedings{Rashtchian2010Collecting,
  title     = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk},
  year      = {2010},
  address   = {Los Angeles},
  editor    = {Callison-Burch, Chris and Dredze, Mark},
  month     = jun,
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/rashtchian2010Collecting.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/W10-0700},
}

@Article{Miller1995Wordnet,
  author     = {Miller, George A.},
  journal    = {Commun. ACM},
  title      = {WordNet: A Lexical Database for English},
  year       = {1995},
  issn       = {0001-0782},
  month      = nov,
  number     = {11},
  pages      = {39–41},
  volume     = {38},
  abstract   = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  address    = {New York, NY, USA},
  doi        = {10.1145/219717.219748},
  file       = {:PDF/wordnet a lexical.pdf:PDF},
  issue_date = {Nov. 1995},
  numpages   = {3},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/219717.219748},
}

@Article{Barnard2003Matching,
  author  = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and Blei, David and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
  journal = {Journal of Machine Learning Research},
  title   = {Matching Words and Pictures},
  year    = {2003},
  month   = {05},
  volume  = {3},
  doi     = {10.1162/153244303322533214},
  file    = {:PDF/matching.pdf:PDF},
}

@InProceedings{Li2020Oscar,
  author       = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle    = {European Conference on Computer Vision},
  title        = {Oscar: Object-semantics aligned pre-training for vision-language tasks},
  year         = {2020},
  organization = {Springer},
  pages        = {121--137},
  file         = {:PDF/Oscar- Object-semantics aligned pre-training for vision-language tasks.pdf:PDF},
  groups       = {Early fusion and vision-and-language pre-training., BERT},
  url          = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750120.pdf},
}

@InProceedings{Pan2020XLinear,
  author    = {Y and Pan and T and Yao and Y. Li and T. Mei},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {X-Linear Attention Networks for Image Captioning},
  year      = {2020},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {10968-10977},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR42600.2020.01098},
  file      = {:PDF/X-linear attention networks for image captioning.pdf:PDF},
  groups    = {Self-Attention Encoding, Boosting LSTM with Self-Attention, attention taxonomy},
  keywords  = {visualization;decoding;cognition;knowledge discovery;task analysis;aggregates;weight measurement},
  printed   = {yes},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01098},
}

@InProceedings{Li2020Unicoder,
  author    = {Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training},
  year      = {2020},
  number    = {07},
  pages     = {11336--11344},
  volume    = {34},
  file      = {:PDF/Unicoder-vl- A universal encoder for vision and language by cross-modal pre-training.pdf:PDF},
  url       = {https://arxiv.org/pdf/1908.06066.pdf},
}

@InProceedings{Cornia2020Meshed,
  author    = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Meshed-memory transformer for image captioning},
  year      = {2020},
  pages     = {10578--10587},
  file      = {:PDF/Meshed-memory transformer for image captioning.pdf:PDF},
  groups    = {Self-Attention Encoding, Gating mechanisms., attention taxonomy},
  printed   = {yes},
  url       = {https://arxiv.org/pdf/1912.08226.pdf},
}

@InProceedings{Rebuffi2020There,
  author    = {Rebuffi, Sylvestre-Alvise and Fong, Ruth and Ji, Xu and Vedaldi, Andrea},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {There and back again: Revisiting backpropagation saliency methods},
  year      = {2020},
  pages     = {8839--8848},
  file      = {:PDF/There and back again- Revisiting backpropagation saliency methods.pdf:PDF},
  url       = {https://arxiv.org/pdf/2004.02866.pdf},
}

@Article{Jia2021Scaling,
  author     = {Chao Jia and Yinfei Yang and Ye Xia and Yi{-}Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yun{-}Hsuan Sung and Zhen Li and Tom Duerig},
  journal    = {CoRR},
  title      = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
  year       = {2021},
  volume     = {abs/2102.05918},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2102-05918.bib},
  eprint     = {2102.05918},
  eprinttype = {arXiv},
  file       = {:PDF/Scaling up visual and vision-language representation learning with noisy text supervision.pdf:PDF},
  timestamp  = {Wed, 05 May 2021 14:06:23 +0200},
  url        = {https://arxiv.org/abs/2102.05918},
}

@Article{Yang2020Deconfounded,
  author     = {Xu Yang and Hanwang Zhang and Jianfei Cai},
  journal    = {CoRR},
  title      = {Deconfounded Image Captioning: {A} Causal Retrospect},
  year       = {2020},
  volume     = {abs/2003.03923},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-03923.bib},
  eprint     = {2003.03923},
  eprinttype = {arXiv},
  file       = {:PDF/Deconfounded image captioning- A causal retrospect.pdf:PDF},
  timestamp  = {Fri, 12 Jun 2020 16:01:21 +0200},
  url        = {https://arxiv.org/abs/2003.03923},
}

@Article{An2020Data,
  author   = {Qinglong An and Zhengrui Tao and Xingwei Xu and Mohamed {El Mansori} and Ming Chen},
  journal  = {Measurement},
  title    = {A data-driven model for milling tool remaining useful life prediction with convolutional and stacked LSTM network},
  year     = {2020},
  issn     = {0263-2241},
  pages    = {107461},
  volume   = {154},
  abstract = {This paper introduces a hybrid model that incorporates a convolutional neural network (CNN) with a stacked bi-directional and uni-directional LSTM (SBULSTM) network, named CNN-SBULSTM, to address sequence data in the task of tool remaining useful life (RUL) prediction. In the CNN-SBULSTM network, CNN is firstly utilized for local feature extraction and dimension reduction. Then SBULSTM network is designed to denoise and encode the temporal information. Finally, multiple fully connected layers are built on the top of the CNN-SBULSTM network to add non-linearity to the output, and one regression layer is utilized to generate the target RUL. The cyber-physical system (CPS) is used to collect the internal controller signals and the external sensor signals during milling process. The proposed hybrid model and several other published methods are applied to the datasets acquired from milling experiments. The comparison and analysis results indicate that the integrated framework is applicable to track the tool wear evolution and predict its RUL with the average prediction accuracy reaching up to 90%.},
  doi      = {https://doi.org/10.1016/j.measurement.2019.107461},
  file     = {:PDF/A data-driven model for milling tool remaining useful life prediction with convolutional and stacked LSTM network.pdf:PDF},
  keywords = {Tool condition monitoring, Long short-term memory network, Convolutional neural network, Remaining useful life, Cyber-physical system},
  url      = {https://www.sciencedirect.com/science/article/pii/S0263224119313284},
}

@InProceedings{Li2020Context,
  author    = {Li, Zhuowan and Tran, Quan and Mai, Long and Lin, Zhe and Yuille, Alan L.},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Context-Aware Group Captioning via Self-Attention and Contrastive Features},
  year      = {2020},
  pages     = {3437-3447},
  doi       = {10.1109/CVPR42600.2020.00350},
  file      = {:PDF/Context-aware group captioning via self-attention and contrastive features.pdf:PDF},
}

@Article{Cao2020Comprehensiv,
  author  = {Cao, Wenming and Yan, Zhiyue and He, Zhiquan and He, Zhihai},
  journal = {IEEE Access},
  title   = {A Comprehensive Survey on Geometric Deep Learning},
  year    = {2020},
  pages   = {35929-35949},
  volume  = {8},
  doi     = {10.1109/ACCESS.2020.2975067},
  file    = {:PDF/A_Comprehensive_Survey_on_Geometric_Deep_Learning.pdf:PDF},
  printed = {yes},
}

@Article{Huang2020Image,
  author  = {Huang, Yiqing and Chen, Jiansheng and Ouyang, Wanli and Wan, Weitao and Xue, Youze},
  journal = {IEEE Transactions on Image Processing},
  title   = {Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction},
  year    = {2020},
  pages   = {4013-4026},
  volume  = {29},
  comment = {https://github.com/RubickH/Image-Captioning-with-MAD-and-SAP},
  doi     = {10.1109/TIP.2020.2969330},
  file    = {:PDF/Image captioning with end-to-end attribute detection and subsequent attributes prediction.pdf:PDF},
}

@InProceedings{Chen2020Uniter,
  author       = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle    = {European Conference on Computer Vision},
  title        = {Uniter: Universal image-text representation learning},
  year         = {2020},
  organization = {Springer},
  pages        = {104--120},
  doi          = {https://link.springer.com/chapter/10.1007/978-3-030-58577-8_7},
  file         = {:PDF/Uniter Universal image-text representation learning.pdf:PDF},
  url          = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750103.pdf},
}

@Article{Hossain2019Comprehensive,
  author     = {Hossain, MD. Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
  journal    = {ACM Comput. Surv.},
  title      = {A Comprehensive Survey of Deep Learning for Image Captioning},
  year       = {2019},
  issn       = {0360-0300},
  month      = feb,
  number     = {6},
  volume     = {51},
  abstract   = {Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths, and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep-learning-based automatic image captioning.},
  address    = {New York, NY, USA},
  articleno  = {118},
  doi        = {10.1145/3295748},
  file       = {:PDF/A Comprehensive Survey of Deep Learning for Image Captioning.pdf:PDF},
  groups     = {review},
  issue_date = {February 2019},
  keywords   = {natural language processing, computer vision, CNN, LSTM, deep learning, Image captioning},
  numpages   = {36},
  printed    = {yes},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3295748},
}

@Article{Islam2021ExploringVC,
  author  = {Saiful Islam and Aurpan Dash and Ashek Seum and Amir Hossain Raj and Tonmoy Hossain and F. Shah},
  journal = {SN Comput. Sci.},
  title   = {Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods},
  year    = {2021},
  pages   = {120},
  volume  = {2},
  file    = {:PDF/Exploring Video Captioning Techniques A Comprehensive Survey on Deep Learning Methods.pdf:PDF},
  url     = {https://link.springer.com/content/pdf/10.1007/s42979-021-00487-x.pdf},
}

@Article{Sundaramoorthy2021End,
  author     = {Carola Sundaramoorthy and Lin Ziwen Kelvin and Mahak Sarin and Shubham Gupta},
  journal    = {CoRR},
  title      = {End-to-End Attention-based Image Captioning},
  year       = {2021},
  volume     = {abs/2104.14721},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-14721.bib},
  eprint     = {2104.14721},
  eprinttype = {arXiv},
  file       = {:PDF/End-to-End_Attention-based_Image_Captioning.pdf:PDF},
  timestamp  = {Tue, 04 May 2021 15:12:43 +0200},
  url        = {https://arxiv.org/abs/2104.14721},
}

@Article{Khurana2021Video,
  author  = {Khurana, Khushboo and Deshpande, Umesh},
  journal = {IEEE Access},
  title   = {Video Question-Answering Techniques, Benchmark Datasets and Evaluation Metrics Leveraging Video Captioning: A Comprehensive Survey},
  year    = {2021},
  pages   = {43799-43823},
  volume  = {9},
  doi     = {10.1109/ACCESS.2021.3058248},
  file    = {:PDF/Video Question-Answering Techniques,Benchmark Datasets and EvaluationMetrics Leveraging Video Captioning A Comprehensive Survey.pdf:PDF},
}

@InProceedings{Nagchowdhuryetal2021Exploiting,
  author    = {Nag Chowdhury, Sreyasi and Bhowmik, Rajarshi and Ravi, Hareesh and de Melo, Gerard and Razniewski, Simon and Weikum, Gerhard},
  booktitle = {Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)},
  title     = {Exploiting Image Text Synergy for Contextual Image Captioning},
  year      = {2021},
  address   = {Kyiv, Ukraine},
  month     = apr,
  pages     = {30--37},
  publisher = {Association for Computational Linguistics},
  abstract  = {Modern web content - news articles, blog posts, educational resources, marketing brochures - is predominantly multimodal. A notable trait is the inclusion of media such as images placed at meaningful locations within a textual narrative. Most often, such images are accompanied by captions - either factual or stylistic (humorous, metaphorical, etc.) - making the narrative more engaging to the reader. While standalone image captioning has been extensively studied, captioning an image based on external knowledge such as its surrounding text remains under-explored. In this paper, we study this new task: given an image and an associated unstructured knowledge snippet, the goal is to generate a contextual caption for the image.},
  file      = {:PDF/Exploiting Image Text Synergy for Contextual Image Captioning.pdf:PDF},
  url       = {https://www.aclweb.org/anthology/2021.lantern-1.3},
}

@Article{Scaiella2019Large,
  author    = {Scaiella, Antonio and Croce, Danilo and Basili, Roberto},
  journal   = {Italian Journal of Computational Linguistics},
  title     = {Large scale datasets for Image and Video Captioning in Italian},
  year      = {2019},
  number    = {5},
  pages     = {49-60},
  volume    = {2},
  editor    = {Roberto Basili and Simonetta Montemagni},
  file      = {:PDF/Large scale datasets for Image and Video Captioning in Italian.pdf:PDF},
  publisher = {Accademia University Press},
  url       = {http://www.ai-lc.it/IJCoL/v5n2/IJCOL_5_2_3___scaiella_et_al.pdf},
}

@Article{Wu2017AICAIChallenger,
  author     = {Jiahong Wu and He Zheng and Bo Zhao and Yixin Li and Baoming Yan and Rui Liang and Wenjia Wang and Shipei Zhou and Guosen Lin and Yanwei Fu and Yizhou Wang and Yonggang Wang},
  journal    = {CoRR},
  title      = {{AI} Challenger : {A} Large-scale Dataset for Going Deeper in Image Understanding},
  year       = {2017},
  volume     = {abs/1711.06475},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1711-06475.bib},
  eprint     = {1711.06475},
  eprinttype = {arXiv},
  file       = {:PDF/AI Challenger A Large-scale Dataset for Going Deeper in Image Understanding.pdf:PDF},
  timestamp  = {Fri, 13 Aug 2021 14:56:27 +0200},
  url        = {http://arxiv.org/abs/1711.06475},
}

@InProceedings{YanGan2018Image,
  author    = {Yan, Shiyang and Wu, Fangyu and Smith, Jeremy S. and Lu, Wenjin and Zhang, Bailing},
  booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
  title     = {Image Captioning using Adversarial Networks and Reinforcement Learning},
  year      = {2018},
  pages     = {248-253},
  doi       = {10.1109/ICPR.2018.8545049},
  file      = {:PDF/Image Captioning using Adversarial Networks and Reinforcement Learning.pdf:PDF},
}

@Article{TantiGattCamilleri2018Where,
  author    = {TANTI, MARC and GATT, ALBERT and CAMILLERI, KENNETH P.},
  journal   = {Natural Language Engineering},
  title     = {Where to put the image in an image caption generator},
  year      = {2018},
  number    = {3},
  pages     = {467–489},
  volume    = {24},
  doi       = {10.1017/S1351324918000098},
  file      = {:PDF/Where to put the Image in an Image Caption Generator.pdf:PDF;:PDF/1703.09137v2.pdf:PDF},
  publisher = {Cambridge University Press},
}

@InProceedings{Hendricks2016Deep,
  author    = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data},
  year      = {2016},
  pages     = {1-10},
  doi       = {10.1109/CVPR.2016.8},
  file      = {:PDF/Deep Compositional Captioning Describing Novel Object Categories without Paired Training Data.pdf:PDF},
  groups    = {Describing novel objects},
}

@InProceedings{Pennington2014GLOVE,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  year      = {2014},
  address   = {Doha, Qatar},
  editor    = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  month     = oct,
  pages     = {1532--1543},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/D14-1162},
  file      = {:PDF/GloVe Global Vectors for Word Representation.pdf:PDF},
  url       = {https://aclanthology.org/D14-1162},
}

@Article{Wu2015ImageCW,
  author     = {Qi Wu and Chunhua Shen and Anton van den Hengel and Lingqiao Liu and Anthony R. Dick},
  journal    = {CoRR},
  title      = {Image Captioning with an Intermediate Attributes Layer},
  year       = {2015},
  volume     = {abs/1506.01144},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/WuSHLD15.bib},
  eprint     = {1506.01144},
  eprinttype = {arXiv},
  file       = {:PDF/Image Captioning with an Intermediate Attributes Layer.pdf:PDF},
  timestamp  = {Tue, 19 Mar 2019 13:03:53 +0100},
  url        = {http://arxiv.org/abs/1506.01144},
}

@InProceedings{Szegedy2016RethinkingTI,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  year      = {2016},
  month     = {June},
  pages     = {2818-2826},
  abstract  = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.},
  doi       = {10.1109/CVPR.2016.308},
  file      = {:PDF/Rethinking the Inception Architecture for Computer Vision.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Convolution;Computer architecture;Training;Computational efficiency;Computer vision;Benchmark testing;Computational modeling},
}

@InProceedings{Johnson2018Image,
  author    = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  booktitle = {CVPR},
  title     = {Image Generation from Scene Graphs},
  year      = {2018},
  file      = {:PDF/Image generation from scene graph.pdf:PDF},
}

@InProceedings{Zellers2019Vcr,
  author    = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {From Recognition to Cognition: Visual Commonsense Reasoning},
  year      = {2019},
  month     = {June},
  file      = {:PDF/from recognition to cognition.pdf:PDF},
}

@InProceedings{Zellers2018Scenegraphs,
  author    = {Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle = {Conference on Computer Vision and Pattern Recognition},
  title     = {Neural Motifs: Scene Graph Parsing with Global Context},
  year      = {2018},
  file      = {:PDF/Neural Motifs- Scene Graph Parsing with Global Context.pdf:PDF},
}

@InProceedings{Donadello2019Compensating,
  author    = {Ivan Donadello and Luciano Serafini},
  booktitle = {{IJCNN}},
  title     = {Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation},
  year      = {2019},
  pages     = {1--8},
  publisher = {{IEEE}},
  comment   = {https://github.com/ivanDonadello/Visual-Relationship-Detection-LTN},
  file      = {:PDF/Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation.pdf:PDF},
  printed   = {yes},
}

@InProceedings{Lu2016Visual,
  author    = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
  booktitle = {European Conference on Computer Vision},
  title     = {Visual Relationship Detection with Language Priors},
  year      = {2016},
  comment   = {https://cs.stanford.edu/people/ranjaykrishna/vrd/},
}

@Article{Han2021ImageSG,
  author     = {Xiaotian Han and Jianwei Yang and Houdong Hu and Lei Zhang and Jianfeng Gao and Pengchuan Zhang},
  journal    = {CoRR},
  title      = {Image Scene Graph Generation {(SGG)} Benchmark},
  year       = {2021},
  volume     = {abs/2107.12604},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2107-12604.bib},
  eprint     = {2107.12604},
  eprinttype = {arXiv},
  file       = {:PDF/Image Scene Graph Generation (SGG) Benchmark.pdf:PDF},
  timestamp  = {Fri, 30 Jul 2021 13:03:06 +0200},
  url        = {https://arxiv.org/abs/2107.12604},
}

@Article{Krishna2016Visualgenome,
  author     = {Ranjay Krishna and Yuke Zhu and Oliver Groth and Justin Johnson and Kenji Hata and Joshua Kravitz and Stephanie Chen and Yannis Kalantidis and Li{-}Jia Li and David A. Shamma and Michael S. Bernstein and Li Fei{-}Fei},
  journal    = {CoRR},
  title      = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  year       = {2016},
  volume     = {abs/1602.07332},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/KrishnaZGJHKCKL16.bib},
  eprint     = {1602.07332},
  eprinttype = {arXiv},
  file       = {:PDF/Visual_Genome.pdf:PDF},
  timestamp  = {Wed, 15 Sep 2021 14:13:01 +0200},
  url        = {http://arxiv.org/abs/1602.07332},
}

@InProceedings{Krause2016Paragraphs,
  author    = {Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},
  booktitle = {Computer Vision and Patterm Recognition (CVPR)},
  title     = {A Hierarchical Approach for Generating Descriptive Image Paragraphs},
  year      = {2017},
  file      = {:PDF/A Hierarchical Approach for Generating Descriptive Image Paragraphs.pdf:PDF},
  url       = {https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html},
}

@InProceedings{Anderson2016Spice,
  author       = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle    = {European conference on computer vision},
  title        = {Spice: Semantic propositional image caption evaluation},
  year         = {2016},
  organization = {Springer},
  pages        = {382--398},
  file         = {:PDF/SPICE- Semantic Propositional Image Caption Evaluation.pdf:PDF},
  groups       = {metryki},
}

@InProceedings{Johnson2015Image,
  author    = {Johnson, Justin and Krishna, Ranjay and Stark, Michael and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Image retrieval using scene graphs},
  year      = {2015},
  pages     = {3668-3678},
  doi       = {10.1109/CVPR.2015.7298990},
  file      = {:PDF/Image Retrieval using Scene Graphs.pdf:PDF},
}

@Article{Luo2018Discriminability,
  author     = {Ruotian Luo and Brian L. Price and Scott Cohen and Gregory Shakhnarovich},
  journal    = {CoRR},
  title      = {Discriminability objective for training descriptive captions},
  year       = {2018},
  volume     = {abs/1803.04376},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1803-04376.bib},
  comment    = {nacisk na rónorodność
-standardowe modele tworza podobne podpisy  (MLE-maximum likelihood estimation )
-transformers
architektura koder-dekoder (podobnie do RNN), ale moga rownolegle otrzymywac na wejsciu całe zdanie( rownolegle otrzymywanie kazdego slowa w zdaniu) np. całe zdanie rownolegle slowo po slowie jest wstrzykiwane do pierwszej warstwy embeddings
transformers nie uzywa  konwolucji(wiedzy o poprzdnich slowach)  - uzwane sa zafiksownae wagi slow w zdaniach
cechy obrazu tecetron features
blok dekodera ma 2 warstwy z mechanizmem self atencji,  a druga to prosta, pozycyjna, w pełni połączona sieć typu feed-forward.  Dla kazdego slowa otrzymujemy wektor atencji, ktory przechowuje kontekst słów w zdaniu. Atencja wieloglowicowa w dekoderze eykonuje self-attention.. Pozwala to laczyc slowa na wejsciu  z innymi slowami.
Dekoder uzywa 3 warstwy ktora wykonuje atencje wieloglowicowa na wyjsciu encodera. Podobnie do dekodera, podwarstwy sa laczone przez normalizacje. Wekotry atencji wejscia i wyjscia sa przekazywane do mechanizmu atencji wieloglowicowej. Standardowo dekoder zachwouje sie jak klasyfikator (softmax)

podpisy:beam_search
własna miara roznorodnosci

zbiór danych  COCO
 113,287 tren
 5,000 validacja  5,000  test.
5 podpisow do obrazu

model obraz-jezyk to Vilibert wytrenowany na conceptual captions. Wytrenowany na wielu zadanich jezyk obraz(odp. na pytani, rozumienie obrazu, wyszukiwanie obrazu, w kazdym zadaniu osiagnieto state of the art poziom). Skupiono sie na  tym  by stworzyc  model wiązacy obraz z tekstem, ktory jest latwy do przeniesienia. Porzucono podejscie gdzie znajdowanie powiazan obraz tekst to czesc treningu/},
  eprint     = {1803.04376},
  eprinttype = {arXiv},
  file       = {:PDF/Discriminability objective for training descriptive captions.pdf:PDF},
  timestamp  = {Mon, 13 Aug 2018 16:47:55 +0200},
  url        = {http://arxiv.org/abs/1803.04376},
}

@Misc{Amirian2019ASR,
  author   = {Amirian,Soheyla and Rasheed,Khaled and Taha,Thiab R. and Arabnia,Hamid R.},
  note     = {Name - Google Inc; Copyright - Copyright The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp) 2019; Last updated - 2020-01-27},
  title    = {A Short Review on Image Caption Generation with Deep Learning},
  year     = {2019},
  abstract = {Methodologies that utilize Deep Learning offer great potential for applications that automatically attempt to generate captions or descriptions about images. Image captioning is considered to be one of the intellectually challenging problems in imaging science. The application domains include: automatic caption (or description) generation for images for people who suffer from various degrees of visual impairment; the automatic creation of metadata for images (indexing) for use by search engines; general purpose robot vision systems; and many others. Each of these application domains can positively and significantly impact many other task-specific applications. This paper is not meant to be a comprehensive review of image captioning; rather, it is a concise review of image captioning methodologies based on deep learning, strengths and limitations, the datasets and the evaluation metrics used in automatic image captioning. Finally, a quick discussion about the software and hardware requirements for implementing an image captioning method is presented.},
  file     = {:PDF/A Short Review on Image Caption Generation with Deep Learning.pdf:PDF},
  journal  = {Proceedings of the International Conference on Image Processing, Computer Vision, and Pattern Recognition (IPCV)},
  keywords = {Computers; Research; Language; International conferences; Software; Search engines; Datasets; Deep learning; Science; Researchers; Machine translation; Machine learning; Domains; Pattern recognition; Natural language; Visual impairment; Big Data; Neural networks; Images; Methods; Linguistics; Vision systems; Semantics; Disabled people},
  language = {English},
  pages    = {10-18},
  url      = {https://www.proquest.com/conference-papers-proceedings/short-review-on-image-caption-generation-with/docview/2277982436/se-2?accountid=27375},
}

@Article{Wang2021Integrative,
  author  = {Wang, Chaoyang and Zhou, Ziwei and Xu, Liang},
  journal = {Journal of Physics: Conference Series},
  title   = {An Integrative Review of Image Captioning Research},
  year    = {2021},
  month   = {01},
  pages   = {042060},
  volume  = {1748},
  doi     = {10.1088/1742-6596/1748/4/042060},
  file    = {:PDF/An_Integrative_Review_of_Image_Captioning_Research.pdf:PDF},
}

@Article{Wang2020AnOO,
  author  = {Haoran Wang and Yanjing Zhang and Xiaosheng Yu},
  journal = {Computational Intelligence and Neuroscience},
  title   = {An Overview of Image Caption Generation Methods},
  year    = {2020},
  volume  = {2020},
  doi     = {https://doi.org/10.1155/2020/3062706},
  file    = {:PDF/An Overview of Image Caption Generation Methods.pdf:PDF},
  groups  = {review},
}

@Article{Deorukhkar2021Detailed,
  author    = {Deorukhkar, Kalpana and Ket, Satish},
  journal   = {Multimedia Tools and Applications},
  title     = {A detailed review of prevailing image captioning methods using deep learning techniques},
  year      = {2021},
  pages     = {1--24},
  doi       = {https://doi.org/10.1007/s11042-021-11293-1},
  file      = {:PDF/A detailed review of prevailing image captioning methods using deep learning techniques.pdf:PDF},
  publisher = {Springer},
  url       = {https://link.springer.com/article/10.1007/s11042-021-11293-1},
}

@Article{Cornia2019ShowCA,
  author  = {Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
  year    = {2019},
  pages   = {8299-8308},
  comment = {https://github.com/aimagelab/show-control-and-tell},
  file    = {:PDF/Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_Grounded_Captions.pdf:PDF},
  groups  = {Visual sentinel},
}

@InProceedings{Chen2021Human,
  author    = {Chen, Long and Jiang, Zhihong and Xiao, Jun and Liu, Wei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles},
  year      = {2021},
  month     = {June},
  pages     = {16846-16856},
  comment   = {)https://github.com/mad-red/VSR-guided-CIC
Zalety:
zróżnicowane opisy oraz balans miedzy jakoscia i zroznicowaniem
-wiele rozwiązań koder-dekoder osiągneło super wyniki, ale produkuja powtarzalne generyczne zdania
-nie ma mozliwosci skupienia sie na obszarze zainteresowania (tak jak to robi czlowiek)
-dzieki modelom CIC( Controllable Image Captioning) mozemy produkowac zróżnicowane opisy poprzez podanie różnych sygnałów sterujących
-standardowe modle CIC skupiaja sie na sentymencie, emocjach, osobie w zdaniu np. styl zdania(strona bierna). Jednak wciaz trudne jest scisle kontrolowanie
Nowe rozwiązanie:
Verb-specific Semantic Roles (VSR) - zorientowany na zdarzenia  obiektowy sygnal . Pozwala dostsowac sie do zdarzenia oraz specyficznego przykładu
Etapy:
1. trenowanie modelu GSRL(grounded semantic role labeling) - identyfikacja oraz oznaczenie wszystkich encji dla rół
2. SSP(semantic structure planner) -  uszeregowanie czasownikow i roli semantycznych w celu otrzymania  struktur semantycznych bliskich opisowi ludzkiemu
3. połączenie oznaczonych encji oraz struktur semantycznych oraz uzycie tego rezultatu w sieci RNN. Sieć RNNzamienia role poprzez sekwenycjne skupianie sie na roznych rolach
czasownik jedzenie
role semantyczne dla powyzszego czasownika - jedzenie(naleśnik), pojemnik(talerz), narzedzie(widelec)
encje - czlowiek, nalesnik,
Metryki:
sprawdzanie wygenerowanego podpisu i porownanie z referencyjnym
bleu-4 meteor, rouge, CIDEr-D, SPICE},
  file      = {:PDF/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf:PDF},
  timestamp = {2021-10-28},
}

@InProceedings{Xia2021XGPTCG,
  author    = {Qiaolin Xia and Haoyang Huang and Nan Duan and Dongdong Zhang and Lei Ji and Zhifang Sui and Edward Cui and Taroon Bharti and Xin Liu and Ming Zhou},
  booktitle = {NLPCC},
  title     = {XGPT: Cross-modal Generative Pre-Training for Image Captioning},
  year      = {2021},
  file      = {:PDF/XGPT- Cross-modal Generative Pre-Training for Image Captioning.pdf:PDF},
}

@InProceedings{Zhou2020Unified,
  author    = {Luowei Zhou and Hamid Palangi and Lei Zhang and Houdong Hu and Jason J. Corso and Jianfeng Gao},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
  title     = {Unified Vision-Language Pre-Training for Image Captioning and {VQA}},
  year      = {2020},
  pages     = {13041--13049},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/ZhouPZHCG20.bib},
  comment   = {https://github.com/LuoweiZhou/VLP},
  doi       = {10.1609/AAAI.V34I07.7005},
  file      = {:PDF/Unified Vision-Language Pre-Training for Image Captioning and VQA.pdf:PDF},
  groups    = {Early fusion and vision-and-language pre-training., BERT},
  timestamp = {Thu, 11 Apr 2024 13:33:56 +0200},
  url       = {https://doi.org/10.1609/aaai.v34i07.7005},
}

@Article{Hu2020Vivo,
  author     = {Xiaowei Hu and Xi Yin and Kevin Lin and Lijuan Wang and Lei Zhang and Jianfeng Gao and Zicheng Liu},
  journal    = {CoRR},
  title      = {{VIVO:} Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training},
  year       = {2020},
  volume     = {abs/2009.13682},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2009-13682.bib},
  eprint     = {2009.13682},
  eprinttype = {arXiv},
  file       = {:PDF/VIVO- Visual Vocabulary Pre-Training for Novel Object Captioning.pdf:PDF},
  timestamp  = {Fri, 21 May 2021 15:45:48 +0200},
  url        = {https://arxiv.org/abs/2009.13682},
}

@Article{Herdade2019Image,
  author     = {Simao Herdade and Armin Kappeler and Kofi Boakye and Joao Soares},
  journal    = {CoRR},
  title      = {Image Captioning: Transforming Objects into Words},
  year       = {2019},
  volume     = {abs/1906.05963},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1906-05963.bib},
  eprint     = {1906.05963},
  eprinttype = {arXiv},
  file       = {:PDF/Image Captioning- Transforming Objects into Words.pdf:PDF},
  groups     = {Self-Attention Encoding, Transformer},
  printed    = {yes},
  timestamp  = {Mon, 24 Jun 2019 17:28:45 +0200},
  url        = {http://arxiv.org/abs/1906.05963},
}

@InProceedings{Johnson2016Densecap,
  author    = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title     = {Densecap: Fully convolutional localization networks for dense captioning},
  year      = {2016},
  pages     = {4565--4574},
  file      = {:PDF/DenseCap- Fully Convolutional Localization Networks for Dense Captioning.pdf:PDF},
}

@Article{Stefanini2021Show,
  author     = {Matteo Stefanini and Marcella Cornia and Lorenzo Baraldi and Silvia Cascianelli and Giuseppe Fiameni and Rita Cucchiara},
  journal    = {CoRR},
  title      = {From Show to Tell: {A} Survey on Image Captioning},
  year       = {2021},
  volume     = {abs/2107.06912},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2107-06912.bib},

  eprint     = {2107.06912},
  eprinttype = {arXiv},
  file       = {:PDF/from show to tell a survey on image captioning.pdf:PDF},
  groups     = {review},
  timestamp  = {Wed, 21 Jul 2021 15:55:35 +0200},
  url        = {https://arxiv.org/abs/2107.06912},
}

@InProceedings{Lee2020Vilbert,
  author    = {Lee, Hwanhee and Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Jung, Kyomin},
  booktitle = {Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems},
  title     = {{V}i{LBERTS}core: Evaluating Image Caption Using Vision-and-Language {BERT}},
  year      = {2020},
  address   = {Online},
  month     = nov,
  pages     = {34--39},
  publisher = {Association for Computational Linguistics},
  abstract  = {In this paper, we propose an evaluation metric for image captioning systems using both image and text information. Unlike the previous methods that rely on textual representations in evaluating the caption, our approach uses visiolinguistic representations. The proposed method generates image-conditioned embeddings for each token using ViLBERT from both generated and reference texts. Then, these contextual embeddings from each of the two sentence-pair are compared to compute the similarity score. Experimental results on three benchmark datasets show that our method correlates significantly better with human judgments than all existing metrics.},
  doi       = {10.18653/v1/2020.eval4nlp-1.4},
  file      = {:PDF/ViLBERTScore_Evaluating_Image_Caption_Using_Vision.pdf:PDF},
  url       = {https://aclanthology.org/2020.eval4nlp-1.4},
}

@InProceedings{Vaswani2017AttentionIA,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention is All you Need},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  comment   = {http://nlp.seas.harvard.edu/2018/04/03/attention.html},
  file      = {:PDF/attention is all you need.pdf:PDF},
  groups    = {Self-Attention Encoding, attention taxonomy},
  printed   = {yes},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@InProceedings{Devlin2019BERTPO,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle = {NAACL},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2019},
  file      = {:PDF/ BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
}

@InProceedings{Banerjee2005METEOR,
  author    = {Satanjeev Banerjee and Alon Lavie},
  booktitle = {IEEvaluation@ACL},
  title     = {METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments},
  year      = {2005},
  file      = {:PDF/METEOR- An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.pdf:PDF},
}

@InProceedings{Kusner2015FromWE,
  author    = {Matt J. Kusner and Yu Sun and Nicholas I. Kolkin and Kilian Q. Weinberger},
  booktitle = {ICML},
  title     = {From Word Embeddings To Document Distances},
  year      = {2015},
  file      = {:PDF/From Word Embeddings To Document Distances.pdf:PDF},
  groups    = {metryki},
}

@InProceedings{Jiang2019TIGErTG,
  author    = {Ming Jiang and Qiuyuan Huang and Lei Zhang and Xin Wang and Pengchuan Zhang and Zhe Gan and Jana Diesner and Jianfeng Gao},
  booktitle = {EMNLP},
  title     = {TIGEr: Text-to-Image Grounding for Image Caption Evaluation},
  year      = {2019},
  file      = {:PDF/TIGEr- Text-to-Image Grounding for Image Caption Evaluation.pdf:PDF},
}

@Article{Ramisa2018BreakingNewsAA,
  author  = {Arnau Ramisa and Fei Yan and Francesc Moreno-Noguer and Krystian Mikolajczyk},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {BreakingNews: Article Annotation by Image and Text Processing},
  year    = {2018},
  pages   = {1072-1085},
  volume  = {40},
  file    = {:/Users/mateuszb/Nextcloud/bib/Mateusz doktorat/PDF/Breaking News- Article Annotatdion by Image and Text processing.pdf:PDF},
}

@Article{Biten2019GoodNews,
  author  = {Ali Furkan Biten and Llu{\'i}s G{\'o}mez and Marçal Rusi{\~n}ol and Dimosthenis Karatzas},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Good News, Everyone! Context Driven Entity-Aware Captioning for News Images},
  year    = {2019},
  pages   = {12458-12467},
  file    = {:PDF/Good News, Everyone! Context driven entity-aware captioning for news images.pdf:PDF},
}

@InProceedings{Ordonez2011Im2Text,
  author    = {Vicente Ordonez and Girish Kulkarni and Tamara L. Berg},
  booktitle = {NIPS},
  title     = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
  year      = {2011},
  file      = {:PDF/NIPS-2011-im2text-describing-images-using-1-million-captioned-photographs-Paper.pdf:PDF},
  groups    = {retrieval},
}

@Article{Ranzato2016SequenceLT,
  author  = {Marc'Aurelio Ranzato and Sumit Chopra and Michael Auli and Wojciech Zaremba},
  journal = {CoRR},
  title   = {Sequence Level Training with Recurrent Neural Networks},
  year    = {2016},
  volume  = {abs/1511.06732},
  file    = {:PDF/Sequence level training with recurrent neural networks.pdf:PDF},
}

@Article{Williams2004SimpleSG,
  author  = {Ronald J. Williams},
  journal = {Machine Learning},
  title   = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  year    = {2004},
  pages   = {229-256},
  volume  = {8},
  file    = {:PDF/SimpleStatisticalGradient-foll.pdf:PDF},
}

@InProceedings{Zhang2020BERTScoreET,
  author    = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  title     = {BERTScore: Evaluating Text Generation with {BERT}},
  year      = {2020},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  file      = {:PDF/BERTScore- Evaluating Text Generation with BERT.pdf:PDF},
  timestamp = {Wed, 03 Jun 2020 10:08:32 +0200},
  url       = {https://openreview.net/forum?id=SkeHuCVFDr},
}

@InProceedings{Vinyals2015Show,
  author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Show and tell: A neural image caption generator},
  year      = {2015},
  month     = {June},
  pages     = {3156-3164},
  abstract  = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  doi       = {10.1109/CVPR.2015.7298935},
  file      = {:PDF/Show and tell- A neural image caption generator.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Logic gates;Measurement;Training;Visualization;Recurrent neural networks;Google},
}

@Article{Sinha2021MaskedLM,
  author     = {Koustuv Sinha and Robin Jia and Dieuwke Hupkes and Joelle Pineau and Adina Williams and Douwe Kiela},
  journal    = {CoRR},
  title      = {Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little},
  year       = {2021},
  volume     = {abs/2104.06644},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-06644.bib},
  eprint     = {2104.06644},
  eprinttype = {arXiv},
  file       = {:PDF/Masked Language Modeling and the Distributional Hypothesis- Order Word Matters Pre-training for Little.pdf:PDF},
  timestamp  = {Mon, 19 Apr 2021 16:45:47 +0200},
  url        = {https://arxiv.org/abs/2104.06644},
}

@InProceedings{Naacl21ZhangInductive,
  author    = {Zhang, Tianyi and Hashimoto, Tatsunori},
  booktitle = {North American Association for Computational Linguistics (NAACL)},
  title     = {On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies},
  year      = {2021},
  file      = {:PDF/On the Inductive Bias of Masked Language Modeling- From Statistical to Syntactic Dependencies.pdf:PDF;:PDF/IRJET-V8I12123.pdf:PDF},
}

@Article{Zaremba2015ReinforcementLN,
  author     = {Wojciech Zaremba and Ilya Sutskever},
  journal    = {CoRR},
  title      = {Reinforcement Learning Neural Turing Machines},
  year       = {2015},
  volume     = {abs/1505.00521},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/ZarembaS15.bib},
  eprint     = {1505.00521},
  eprinttype = {arXiv},
  file       = {:PDF/REINFORCEMENT LEARNING NEURAL TURING MACHINES - REVISED.pdf:PDF},
  timestamp  = {Mon, 13 Aug 2018 16:46:53 +0200},
  url        = {http://arxiv.org/abs/1505.00521},
}

@InProceedings{Lin2004Rouge,
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  year      = {2004},
  address   = {Barcelona, Spain},
  month     = jul,
  pages     = {74--81},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/ROUGE- A Package for Automatic Evaluation of Summaries.pdf:PDF},
  groups    = {metryki},
  url       = {https://aclanthology.org/W04-1013},
}

@Article{Ren2017DeepRL,
  author  = {Zhou Ren and Xiaoyu Wang and Ning Zhang and Xutao Lv and Li-Jia Li},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Deep Reinforcement Learning-Based Image Captioning with Embedding Reward},
  year    = {2017},
  pages   = {1151-1159},
  file    = {:PDF/Deep Reinforcement Learning-Based Image Captioning with Embedding Reward.pdf:PDF},
  groups  = {Other deep learning methods},
}

@InProceedings{Rennie2017SelfCriticalST,
  author    = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Self-Critical Sequence Training for Image Captioning }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = Jul,
  pages     = {1179-1195},
  publisher = {IEEE Computer Society},
  abstract  = {Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.},
  doi       = {10.1109/CVPR.2017.131},
  file      = {:PDF/Self- critical sequence training for image captioning.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Training;Inference algorithms;Measurement;Logic gates;Predictive models;Learning (artificial intelligence)},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.131},
}

@InProceedings{Schumann2021Step,
  author    = {Candice Schumann and Susanna Ricco and Utsav Prabhu and Vittorio Ferrari and Caroline Rebecca Pantofaru},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  title     = {A Step Toward More Inclusive People Annotations for Fairness},
  year      = {2021},
  file      = {:PDF/A Step Toward More Inclusive People Annotations for Fairness.pdf:PDF},
}

@InProceedings{Shao2019Objects365,
  author    = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Objects365: A Large-Scale, High-Quality Dataset for Object Detection},
  year      = {2019},
  month     = {October},
  file      = {:PDF/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf:PDF},
}

@Article{Priya2019Hungarian,
  author  = {Priya, D and Ramesh, G},
  journal = {Journal of Physics: Conference Series},
  title   = {The Hungarian Method for the Assignment Problem, With Generalized Interval Arithmetic and Its Applications},
  year    = {2019},
  month   = {11},
  pages   = {012046},
  volume  = {1377},
  doi     = {10.1088/1742-6596/1377/1/012046},
  file    = {:PDF/The_Hungarian_Method_for_the_Assignment_Problem_Wi.pdf:PDF},
}

@Article{Agrawal2019Nocaps,
  author  = {Harsh Agrawal and Karan Desai and Yufei Wang and Xinlei Chen and Rishabh Jain and Mark Johnson and Dhruv Batra and Devi Parikh and Stefan Lee and Peter Anderson},
  journal = {International Conference on Computer Vision},
  title   = {nocaps: novel object captioning at scale},
  year    = {2019},
  pages   = {8947-8956},
}

@InProceedings{Carion2020End,
  author    = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle = {Computer Vision -- ECCV 2020},
  title     = {End-to-End Object Detection with Transformers},
  year      = {2020},
  address   = {Cham},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  pages     = {213--229},
  publisher = {Springer International Publishing},
  abstract  = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  file      = {:PDF/End-to-End_Object_Detection_with_Transformers.pdf:PDF},
  isbn      = {978-3-030-58452-8},
}

@InProceedings{Stewart2015End,
  author    = {Stewart, Russell and Andriluka, Mykhaylo and Ng, Andrew Y.},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {End-to-End People Detection in Crowded Scenes},
  year      = {2016},
  pages     = {2325-2333},
  doi       = {10.1109/CVPR.2016.255},
  file      = {:PDF/End-to-End_People_Detection_in_Crowded_Scenes.pdf:PDF},
}

@InProceedings{Yang2020FashionCT,
  author    = {Xuewen Yang and Heming Zhang and Di Jin and Yingru Liu and Chi-Hao Wu and Jianchao Tan and Dongliang Xie and Jue Wang and Xin Wang},
  booktitle = {ECCV},
  title     = {Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards},
  year      = {2020},
  file      = {:PDF/Fashion Captioning- Towards Generating Accurate Descriptions with Semantic Rewards.pdf:PDF},
}

@InProceedings{Li2019MetaLF,
  author    = {Nannan Li and Zhenzhong Chen and Shan Liu},
  booktitle = {AAAI},
  title     = {Meta Learning for Image Captioning},
  year      = {2019},
  file      = {:PDF/Meta Learning for Image Captioning .pdf:PDF},
}

@Article{Venugopalan2017CaptioningIW,
  author  = {Subhashini Venugopalan and Lisa Anne Hendricks and Marcus Rohrbach and Raymond J. Mooney and Trevor Darrell and Kate Saenko},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Captioning Images with Diverse Objects},
  year    = {2017},
  pages   = {1170-1178},
  file    = {:PDF/Captioning Images with Diverse Objects.pdf:PDF},
}

@Article{Lu2018NeuralBT,
  author  = {Jiasen Lu and Jianwei Yang and Dhruv Batra and Devi Parikh},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title   = {Neural Baby Talk},
  year    = {2018},
  pages   = {7219-7228},
  file    = {:PDF/neural baby talk.pdf:PDF},
  groups  = {Visual sentinel, Two-layer LSTM},
}

@Article{Wu2018DecoupledNO,
  author  = {Yuehua Wu and Linchao Zhu and Lu Jiang and Yi Yang},
  journal = {Proceedings of the 26th ACM international conference on Multimedia},
  title   = {Decoupled Novel Object Captioner},
  year    = {2018},
  file    = {:PDF/Decoupled Novel Object Captioner.pdf:PDF},
}

@InProceedings{Anderson2017GuidedOV,
  author    = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  booktitle = {EMNLP},
  title     = {Guided Open Vocabulary Image Captioning with Constrained Beam Search},
  year      = {2017},
  file      = {:PDF/Guided Open Vocabulary Image Captioning with Constrained Beam Search.pdf:PDF},
}

@Article{Anderson2018BottomUpAT,
  author  = {Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title   = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  year    = {2018},
  pages   = {6077-6086},
  comment = {https://github.com/MILVLG/bottom-up-attention.pytorch},
  file    = {:PDF/Bottom-Up_and_Top-Down_Attention_for_Image_Captioning_and_Visual_Question_Answering.pdf:PDF},
  groups  = {Attention Over Visual Regions, Two-layers and additive attention, attention taxonomy},
  printed = {yes},
}

@Article{Wu2016GooglesNM,
  author     = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
  journal    = {CoRR},
  title      = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  year       = {2016},
  volume     = {abs/1609.08144},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  eprint     = {1609.08144},
  eprinttype = {arXiv},
  file       = {:PDF/Google's Neural Machine Translation System- Bridging the Gap between Human and Machine Translation.pdf:PDF},
  timestamp  = {Thu, 14 Jan 2021 12:12:19 +0100},
  url        = {http://arxiv.org/abs/1609.08144},
}

@InProceedings{Tian2019Image,
  author    = {Tian, Junjiao and Oh, Jean},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
  title     = {Image Captioning with Compositional Neural Module Networks},
  year      = {2019},
  month     = {7},
  pages     = {3576--3584},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  doi       = {10.24963/ijcai.2019/496},
  file      = {:PDF/Image_Captioning_with_Compositional_Neural_Module_.pdf:PDF},
  url       = {https://doi.org/10.24963/ijcai.2019/496},
}

@Article{Hu2018RelationNF,
  author  = {Han Hu and Jiayuan Gu and Zheng Zhang and Jifeng Dai and Yichen Wei},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title   = {Relation Networks for Object Detection},
  year    = {2018},
  pages   = {3588-3597},
  comment = {https://github.com/msracver/ Relation-Networks-for-Object-Detection},
  file    = {:PDF/Relation Networks for Object Detection.pdf:PDF},
  printed = {yes},
}

@Article{Kumar2017Survey,
  author  = {Kumar, Akshi and Goel, Shivali},
  journal = {International Journal of Hybrid Intelligent Systems},
  title   = {A survey of evolution of image captioning techniques},
  year    = {2017},
  month   = {11},
  pages   = {1-19},
  volume  = {14},
  doi     = {10.3233/HIS-170246},
  file    = {:PDF/A survey of evolution of image captioning techniques.pdf:PDF},
  groups  = {review},
}

@InProceedings{Cho2014Learning,
  author    = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
  year      = {2014},
  address   = {Doha, Qatar},
  month     = oct,
  pages     = {1724--1734},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/D14-1179},
  file      = {:PDF/Learning_Phrase_Representations_using_RNN_Encoder-.pdf:PDF},
  groups    = {lm},
  url       = {https://aclanthology.org/D14-1179},
}

@InProceedings{Chorowski2015AttentionBasedMF,
  author    = {Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and Kyunghyun Cho and Yoshua Bengio},
  booktitle = {NIPS},
  title     = {Attention-Based Models for Speech Recognition},
  year      = {2015},
  file      = {:PDF/Attention-Based Models for Speech Recognition.pdf:PDF},
}

@Article{Shelhamer2017Fully,
  author  = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Fully Convolutional Networks for Semantic Segmentation},
  year    = {2017},
  number  = {4},
  pages   = {640-651},
  volume  = {39},
  doi     = {10.1109/TPAMI.2016.2572683},
  file    = {:PDF/Fully_Convolutional_Networks_for_Semantic_Segmentation.pdf:PDF},
}

@Article{Girshick2013Rich,
  author     = {Ross B. Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  journal    = {CoRR},
  title      = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year       = {2013},
  volume     = {abs/1311.2524},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/GirshickDDM13.bib},
  eprint     = {1311.2524},
  eprinttype = {arXiv},
  file       = {:PDF/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:PDF},
  timestamp  = {Mon, 13 Aug 2018 16:48:09 +0200},
  url        = {http://arxiv.org/abs/1311.2524},
}

@Article{Hochreiter1997LSTM,
  author     = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  journal    = {Neural Comput.},
  title      = {Long Short-Term Memory},
  year       = {1997},
  issn       = {0899-7667},
  month      = {nov},
  number     = {8},
  pages      = {1735–1780},
  volume     = {9},
  abstract   = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  address    = {Cambridge, MA, USA},
  doi        = {10.1162/neco.1997.9.8.1735},
  file       = {:PDF/2604.pdf:PDF},
  issue_date = {November 15, 1997},
  numpages   = {46},
  publisher  = {MIT Press},
  url        = {https://doi.org/10.1162/neco.1997.9.8.1735},
}

@Article{Alassi2013Effectiveness,
  author     = {Alassi, Derar and Alhajj, Reda},
  journal    = {Inf. Sci.},
  title      = {Effectiveness of Template Detection on Noise Reduction and Websites Summarization},
  year       = {2013},
  issn       = {0020-0255},
  month      = {jan},
  pages      = {41–72},
  volume     = {219},
  abstract   = {The World Wide Web is the most rapidly growing and accessible source of information. Its popularity has been largely influenced by the wide availability of the Internet in almost every modern house and even on the go after the wide-spread of the handheld devices. Yet, pages on the Web have an additional template (we call it noisy) information that does not add value to the actual content of the page. Even worse, it can harm the effectiveness of Web mining techniques; these templates could be eliminated by preprocessing. Templates form one popular type of noise on the Internet. In this paper, we introduce Noise Detector (ND) as an effective approach for detecting and removing templates from Web pages. ND segments Web pages into semantically coherent blocks. Then it computes content and structure similarities between these blocks; a presentational noise measure is used as well. ND dynamically calculates a threshold for differentiating noisy blocks. Provided that the investigated website has a single visible template, ND can detect the template with high accuracy using two pages only. However, ND can be expanded to detect multiple templates per website, and the challenge will be to minimize the number of pages to be checked. Further, ND leads to website summarization. The conducted experiments show that ND outperforms existing approaches in space complexity, time complexity (see Section 4.6 for more details on ND's processing time against other algorithms'), minimum requirements to produce acceptable results, and results accuracy.},
  address    = {USA},
  doi        = {10.1016/j.ins.2012.07.022},
  file       = {:PDF/Effectiveness of template detection on noise reductionand websites summarization.pdf:PDF},
  issue_date = {January, 2013},
  keywords   = {Template detection, Noise detection, Website summarization, Web mining},
  numpages   = {32},
  publisher  = {Elsevier Science Inc.},
  url        = {https://doi.org/10.1016/j.ins.2012.07.022},
}

@InProceedings{Aker2010Generating,
  author    = {Aker, Ahmet and Gaizauskas, Robert},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  title     = {Generating Image Descriptions Using Dependency Relational Patterns},
  year      = {2010},
  pages     = {1250–1258},
  series    = {ACL '10},
  abstract  = {This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.},
  file      = {:PDF/Generating image descriptions using dependency relational patterns.pdf:PDF},
  location  = {Uppsala, Sweden},
  numpages  = {9},
}

@Article{Bahdanau2015NeuralMT,
  author  = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal = {CoRR},
  title   = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year    = {2015},
  volume  = {abs/1409.0473},
  file    = {:PDF/NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:PDF},
  groups  = {lm},
}

@InProceedings{Yang2011Corpus,
  author    = {Yang, Yezhou and Teo, Ching and Daum{\'e} III, Hal and Aloimonos, Yiannis},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  title     = {Corpus-Guided Sentence Generation of Natural Images},
  year      = {2011},
  address   = {Edinburgh, Scotland, UK.},
  month     = jul,
  pages     = {444--454},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/Corpus-Guided Sentence Generation of Natural Images.pdf:PDF},
  groups    = {template},
  url       = {https://aclanthology.org/D11-1041},
}

@InProceedings{Lu2018Context,
  author    = {Liu, Daqing and Zha, Zheng-Jun and Zhang, Hanwang and Zhang, Yongdong and Wu, Feng},
  booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
  title     = {Context-Aware Visual Policy Network for Sequence-Level Image Captioning},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {1416–1424},
  publisher = {Association for Computing Machinery},
  series    = {MM '18},
  abstract  = {Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the "exposure bias'' during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (eg, visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (eg, "man riding horse'') and comparisons (eg. "smaller cat"). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at urlhttps://github.com/daqingliu/CAVP},
  comment   = {https://github.com/daqingliu/CAVP},
  doi       = {10.1145/3240508.3240632},
  file      = {:PDF/Context-Aware Visual Policy Network for Sequence-Level Image Captioning.pdf:PDF},
  groups    = {Visual Policy},
  isbn      = {9781450356657},
  keywords  = {image captioning, policy network, reinforcement learning, visual context},
  location  = {Seoul, Republic of Korea},
  numpages  = {9},
  printed   = {yes},
  url       = {https://doi.org/10.1145/3240508.3240632},
}

@InProceedings{Ren2015Faster,
  author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {28},
  file      = {:PDF/Faster_R-CNN_Ren2017.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
}

@InProceedings{Kiros2014Multimodal,
  author    = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Rich},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  title     = {Multimodal Neural Language Models},
  year      = {2014},
  address   = {Bejing, China},
  editor    = {Xing, Eric P. and Jebara, Tony},
  month     = {22--24 Jun},
  number    = {2},
  pages     = {595--603},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  abstract  = {We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.},
  file      = {:PDF/kiros14.pdf:PDF},
  groups    = {encoder-decoder-lit},
  pdf       = {http://proceedings.mlr.press/v32/kiros14.pdf},
  url       = {https://proceedings.mlr.press/v32/kiros14.html},
}

@InProceedings{Bartosiewicz2021Generating,
  author    = {Bartosiewicz, Mateusz and Krupińska, Izabela and Bany, Maciej and Konieczna, Anna and Ostrowski, Mateusz and Zalewski, Maciej and Iwanowski, Marcin},
  booktitle = {2021 14th International Conference on Human System Interaction (HSI)},
  title     = {Generating image captions in Polish – experimental study},
  year      = {2021},
  pages     = {1-6},
  doi       = {10.1109/HSI52170.2021.9538664},
  file      = {:PDF/Generating_image_captions_in_Polish__experimental_study.pdf:PDF},
  groups    = {moje},
}

@InProceedings{Kleczek2020Polbert,
  author    = {Dariusz Kłeczek},
  booktitle = {Proceedings of the PolEval 2020 Workshop},
  title     = {Polbert: Attacking Polish NLP Tasks with Transformers},
  year      = {2020},
  editor    = {Maciej Ogrodniczuk and Łukasz Kobyliński},
  publisher = {Institute of Computer Science, Polish Academy of Sciences},
}

@InProceedings{Masotti2017DeepLF,
  author    = {Caterina Masotti and Danilo Croce and Roberto Basili},
  booktitle = {CLiC-it},
  title     = {Deep Learning for Automatic Image Captioning in Poor Training Conditions},
  year      = {2017},
  file      = {:PDF/Deep learning for automatic image captioning in poor training conditions..pdf:PDF},
}

@Book{Santosh2018Document,
  author        = {Santosh, K.C.},
  title         = {Document image analysis: Current trends and challenges in graphics recognition},
  year          = {2018},
  note          = {cited By 7},
  document_type = {Book},
  doi           = {10.1007/978-981-13-2339-3},
  file          = {:PDF/Document-image-analysis-Current-trends-and-challenges-in-graphics-recognition2018.pdf:PDF},
  journal       = {Document Image Analysis: Current Trends and Challenges in Graphics Recognition},
  pages         = {1-174},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063493512&doi=10.1007%2f978-981-13-2339-3&partnerID=40&md5=d990830b1b7cd29105a5fb13aa520401},
}

@InProceedings{Shietal2020Improving,
  author    = {Shi, Zhan and Zhou, Xu and Qiu, Xipeng and Zhu, Xiaodan},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {Improving Image Captioning with Better Use of Caption},
  year      = {2020},
  address   = {Online},
  month     = jul,
  pages     = {7454--7464},
  publisher = {Association for Computational Linguistics},
  abstract  = {Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available.},
  comment   = {https://github.com/Gitsamshi/WeakVRD-Captioning},
  doi       = {10.18653/v1/2020.acl-main.664},
  file      = {:PDF/Improving Image Captioning with Better Use of Captions.pdf:PDF},
  groups    = {Scene graphs, Two-layer LSTM},
  printed   = {yes},
  url       = {https://aclanthology.org/2020.acl-main.664},
}

@InProceedings{Liu2020InteractiveDG,
  author    = {Junhao Liu and Kai Wang and Chunpu Xu and Zhou Zhao and Ruifeng Xu and Ying Shen and Min Yang},
  booktitle = {AAAI},
  title     = {Interactive Dual Generative Adversarial Networks for Image Captioning},
  year      = {2020},
  file      = {:PDF/Interactive_Dual_Generative_Adversarial_Networks_f.pdf:PDF},
}

@InProceedings{Hessel2021CLIPScoreAR,
  author    = {Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Joseph Le Bras and Yejin Choi},
  booktitle = {EMNLP},
  title     = {CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
  year      = {2021},
  file      = {:PDF/CLIPScore- A Reference-free Evaluation Metric for Image Captioning.pdf:PDF},
}

@Article{Mert2021Cosmic,
  author     = {Mert Inan and Piyush Sharma and Baber Khalid and Radu Soricut and Matthew Stone and Malihe Alikhani},
  journal    = {CoRR},
  title      = {COSMic: {A} Coherence-Aware Generation Metric for Image Descriptions},
  year       = {2021},
  volume     = {abs/2109.05281},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2109-05281.bib},
  eprint     = {2109.05281},
  eprinttype = {arXiv},
  file       = {:PDF/COSMic- A Coherence-Aware Generation Metric for Image.pdf:PDF},
  timestamp  = {Tue, 21 Sep 2021 17:46:04 +0200},
  url        = {https://arxiv.org/abs/2109.05281},
}

@Article{Atliha2022Compresion,
  author         = {Atliha, Viktar and Šešok, Dmitrij},
  journal        = {Applied Sciences},
  title          = {Image-Captioning Model Compression},
  year           = {2022},
  issn           = {2076-3417},
  number         = {3},
  volume         = {12},
  abstract       = {Image captioning is a very important task, which is on the edge between natural language processing (NLP) and computer vision (CV). The current quality of the captioning models allows them to be used for practical tasks, but they require both large computational power and considerable storage space. Despite the practical importance of the image-captioning problem, only a few papers have investigated model size compression in order to prepare them for use on mobile devices. Furthermore, these works usually only investigate decoder compression in a typical encoder&ndash;decoder architecture, while the encoder traditionally occupies most of the space. We applied the most efficient model-compression techniques such as architectural changes, pruning and quantization to several state-of-the-art image-captioning architectures. As a result, all of these models were compressed by no less than 91\% in terms of memory (including encoder), but lost no more than 2% and 4.5% in metrics such as CIDEr and SPICE, respectively. At the same time, the best model showed results of 127.4 CIDEr and 21.4 SPICE, with a size equal to only 34.8 MB, which sets a strong baseline for compression problems for image-captioning models, and could be used for practical applications.},
  article-number = {1638},
  doi            = {10.3390/app12031638},
  file           = {:PDF/Image-Captioning Model Compression.pdf:PDF},
  groups         = {review},
  url            = {https://www.mdpi.com/2076-3417/12/3/1638},
}

@InProceedings{Biten2022Let,
  author    = {Biten, Ali Furkan and G\'omez, Llu{\'\i}s and Karatzas, Dimosthenis},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Let There Be a Clock on the Beach: Reducing Object Hallucination in Image Captioning},
  year      = {2022},
  month     = {January},
  pages     = {1381-1390},
  comment   = {https://github.com/furkanbiten/object-bias},
  file      = {:PDF/Let there be a clock on the beach- Reducing Object Hallucination in Image Captioning .pdf:PDF},
}

@InProceedings{He2021Image,
  author    = {He, Sen and Liao, Wentong and Tavakoli, Hamed R. and Yang, Michael and Rosenhahn, Bodo and Pugeault, Nicolas},
  booktitle = {Computer Vision -- ACCV 2020},
  title     = {Image Captioning Through Image Transformer},
  year      = {2021},
  address   = {Cham},
  editor    = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
  pages     = {153--169},
  publisher = {Springer International Publishing},
  abstract  = {Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect of captioning is the notion of attention: how to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous works have proposed the transformer architecture for image captioning. However, the structure between the semantic units in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt the transformer's internal architecture to images. In this work, we introduce the image transformer, which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widens the original transformer layer's inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks. The code is available at https://github.com/wtliao/ImageTransformer.},
  comment   = {https://github.com/wtliao/ImageTransformer},
  file      = {:PDF/Image Captioning Through Image Transformer.pdf:PDF;:PDF/Image Captioning Through Image Transformer.pdf:PDF},
  isbn      = {978-3-030-69538-5},
  printed   = {yes},
}

@Misc{Zhou2022Compact,
  author        = {Yuanen Zhou and Zhenzhen Hu and Daqing Liu and Huixia Ben and Meng Wang},
  title         = {Compact Bidirectional Transformer for Image Captioning},
  year          = {2022},
  archiveprefix = {arXiv},
  comment       = {https://github.com/YuanEZhou/CBTrans},
  eprint        = {2201.01984},
  file          = {:PDF/Compact Bidirectional Transformer for Image Captioning .pdf:PDF},
  primaryclass  = {cs.CV},
  printed       = {yes},
}

@InProceedings{Rohrbach2018ObjectHI,
  author    = {Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {Object Hallucination in Image Captioning},
  year      = {2018},
  address   = {Brussels, Belgium},
  month     = oct # {-} # nov,
  pages     = {4035--4045},
  publisher = {Association for Computational Linguistics},
  abstract  = {Despite continuously improving performance, contemporary image captioning models are prone to {``}hallucinating{''} objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.},
  doi       = {10.18653/v1/D18-1437},
  file      = {:PDF/Object Hallucination in Image Captioning.pdf:PDF},
  url       = {https://aclanthology.org/D18-1437},
}

@InProceedings{Zhou2021Semi,
  author    = {Zhou, Yuanen and Zhang, Yong and Hu, Zhenzhen and Wang, Meng},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  title     = {Semi-Autoregressive Transformer for Image Captioning},
  year      = {2021},
  pages     = {3132-3136},
  comment   = {https://github.com/YuanEZhou/satic},
  doi       = {10.1109/ICCVW54120.2021.00350},
  file      = {:PDF/Semi-Autoregressive_Transformer_for_Image_Captioning.pdf:PDF},
}

@Article{Dubey2021LabelAttentionTW,
  author  = {Shikha Dubey and Farrukh Olimov and Muhammad Aasim Rafique and Joonmo Kim and Moongu Jeon},
  journal = {ArXiv},
  title   = {Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning},
  year    = {2021},
  volume  = {abs/2109.07799},
  comment = {https://github.com/uestc-nnlab/gat},
  file    = {:PDF/geometry attention transformer with position-aware lstms for image captioning.pdf:PDF},
}

@InProceedings{Zhang2019CVPR,
  author    = {Zhang, Lu and Zhang, Jianming and Lin, Zhe and Lu, Huchuan and He, You},
  booktitle = {CVPR},
  title     = {CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection},
  year      = {2019},
  comment   = {https://github.com/zhangludl/code-and-dataset-for-CapSal},
  file      = {:PDF/CapSal_Leveraging_Captioning_to_Boost_Semantics_for_Salient_Object_Detection_CVPR_2019_paper.pdf:PDF},
}

@InProceedings{Shetty2017Speaking,
  author    = {Shetty, Rakshith and Rohrbach, Marcus and Hendricks, Lisa Anne and Fritz, Mario and Schiele, Bernt},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training},
  year      = {2017},
  pages     = {4155-4164},
  doi       = {10.1109/ICCV.2017.445},
  file      = {:PDF/Speaking the Same Language- Matching Machine to Human Captions by Adversarial Training.pdf:PDF},
  groups    = {Other deep learning methods},
}

@Misc{Cheng2021GeometryEntangled,
  author        = {Ling Cheng and Wei Wei and Feida Zhu and Yong Liu and Chunyan Miao},
  title         = {Geometry-Entangled Visual Semantic Transformer for Image Captioning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2109.14137},
  file          = {:PDF/Geometry-Entangled Visual Semantic Transformer for Image Captioning.pdf:PDF},
  primaryclass  = {cs.CV},
  printed       = {yes},
}

@Misc{Fang2021Injecting,
  author        = {Zhiyuan Fang and Jianfeng Wang and Xiaowei Hu and Lin Liang and Zhe Gan and Lijuan Wang and Yezhou Yang and Zicheng Liu},
  title         = {Injecting Semantic Concepts into End-to-End Image Captioning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2112.05230},
  primaryclass  = {cs.CV},
}

@Article{Han2021PreTrainedPAstPresentFuture,
  author   = {Xu Han and Zhengyan Zhang and Ning Ding and Yuxian Gu and Xiao Liu and Yuqi Huo and Jiezhong Qiu and Yuan Yao and Ao Zhang and Liang Zhang and Wentao Han and Minlie Huang and Qin Jin and Yanyan Lan and Yang Liu and Zhiyuan Liu and Zhiwu Lu and Xipeng Qiu and Ruihua Song and Jie Tang and Ji-Rong Wen and Jinhui Yuan and Wayne Xin Zhao and Jun Zhu},
  journal  = {AI Open},
  title    = {Pre-trained models: Past, present and future},
  year     = {2021},
  issn     = {2666-6510},
  pages    = {225-250},
  volume   = {2},
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  doi      = {https://doi.org/10.1016/j.aiopen.2021.08.002},
  file     = {:PDF/Pre-trained models- Past, present and future.pdf:PDF},
  keywords = {Pre-trained models, Language models, Transfer learning, Self-supervised learning, Natural language processing, Multimodal processing, Artificial intelligence},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
}

@Article{Chen2021Visualgpt,
  author  = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  journal = {arXiv preprint arXiv:2102.10407},
  title   = {VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning},
  year    = {2021},
  comment = {https://github.com/Vision-CAIR/VisualGPT},
  file    = {:PDF/VisualGPT- Data-efficient Adaptation of Pretrained Language Models for Image Captioning.pdf:PDF},
}

@Misc{Song2021Exploring,
  author        = {Zeliang Song and Xiaofei Zhou},
  title         = {Exploring Explicit and Implicit Visual Relationships for Image Captioning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2105.02391},
  file          = {:PDF/EXPLORING EXPLICIT AND IMPLICIT VISUAL RELATIONSHIPS FOR IMAGE CAPTIONING.pdf:PDF},
  groups        = {relations},
  primaryclass  = {cs.CV},
}

@Article{Pan2021Mesa,
  author  = {Zizheng Pan and Peng Chen and Haoyu He and Jing Liu and Jianfei Cai and Bohan Zhuang},
  journal = {arXiv preprint arXiv:2111.11124},
  title   = {Mesa: A Memory-saving Training Framework for Transformers},
  year    = {2021},
  comment = {https://github.com/zhuang-group/Mesa},
  file    = {:PDF/Mesa- A Memory-saving Training Framework for Transformers.pdf:PDF},
}

@Misc{Meng2021Objectcentric,
  author        = {Zihang Meng and David Yang and Xuefei Cao and Ashish Shah and Ser-Nam Lim},
  title         = {Object-Centric Unsupervised Image Captioning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2112.00969},
  file          = {:PDF/Object-Centric Unsupervised Image Captioning.pdf:PDF},
  primaryclass  = {cs.CV},
}

@Misc{Yang2021Reformer,
  author        = {Xuewen Yang and Yingru Liu and Xin Wang},
  title         = {ReFormer: The Relational Transformer for Image Captioning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2107.14178},
  file          = {:PDF/ReFormer- The Relational Transformer for Image Captioning.pdf:PDF},
  groups        = {relations},
  primaryclass  = {cs.CV},
}

@InProceedings{Maru2021ComparisonofEncoderArchforImgCapt,
  author    = {Maru, Harsh and Chandana, TSS and Naik, Dinesh},
  booktitle = {2021 5th International Conference on Computing Methodologies and Communication (ICCMC)},
  title     = {Comparison of Image Encoder Architectures for Image Captioning},
  year      = {2021},
  pages     = {740-744},
  doi       = {10.1109/ICCMC51019.2021.9418234},
  file      = {:PDF/Comparison_of_Image_Encoder_Architectures_for_Image_Captioning.pdf:PDF},
}

@Misc{ghandi2022deep,
  author        = {Taraneh Ghandi and Hamidreza Pourreza and Hamidreza Mahyar},
  title         = {Deep Learning Approaches on Image Captioning: A Review},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2201.12944},
  file          = {:PDF/Deep Learning Approaches on Image Captioning- A Review.pdf:PDF},
  groups        = {review},
  primaryclass  = {cs.CV},
}

@Article{Haque2021ImgCapt,
  author  = {Haque, Anwar Ul and Ghani, Sayeed and Saeed, Muhammad},
  journal = {IEEE Access},
  title   = {Image Captioning With Positional and Geometrical Semantics},
  year    = {2021},
  pages   = {160917-160925},
  volume  = {9},
  doi     = {10.1109/ACCESS.2021.3131343},
  file    = {:PDF/Image_Captioning_With_Positional_and_Geometrical_Semantics.pdf:PDF},
  printed = {yes},
}

@InProceedings{Guo2019Aligning,
  author    = {Guo, Longteng and Liu, Jing and Tang, Jinhui and Li, Jiangwei and Luo, Wei and Lu, Hanqing},
  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
  title     = {Aligning Linguistic Words and Visual Semantic Units for Image Captioning},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {765–773},
  publisher = {Association for Computing Machinery},
  series    = {MM '19},
  abstract  = {Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.},
  comment   = {https://github.com/ltguo19/VSUA-Captioning},
  doi       = {10.1145/3343031.3350943},
  file      = {:PDF/Aligning Linguistic Words and Visual Semantic Units for Image Captioning.pdf:PDF},
  groups    = {Spatial and semantic graphs.},
  isbn      = {9781450368896},
  keywords  = {image captioning, visual relationship, vision-language, graph convolutional networks},
  location  = {Nice, France},
  numpages  = {9},
  printed   = {yes},
  url       = {https://doi.org/10.1145/3343031.3350943},
}

@Article{DBLP:journals/corr/abs-2111-15015,
  author     = {Zanyar Zohourianshahzadi and Jugal K. Kalita},
  journal    = {CoRR},
  title      = {Neural Attention for Image Captioning: Review of Outstanding Methods},
  year       = {2021},
  volume     = {abs/2111.15015},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2111-15015.bib},
  eprint     = {2111.15015},
  eprinttype = {arXiv},
  file       = {:PDF/NeuralAttentionForImageCaption.pdf:PDF},
  groups     = {review},
  timestamp  = {Wed, 01 Dec 2021 15:16:43 +0100},
  url        = {https://arxiv.org/abs/2111.15015},
}

@Article{Santos2022PortugeseDataset,
  author         = {dos Santos, Gabriel Oliveira and Colombini, Esther Luna and Avila, Sandra},
  journal        = {Data},
  title          = {#PraCegoVer: A Large Dataset for Image Captioning in Portuguese},
  year           = {2022},
  issn           = {2306-5729},
  number         = {2},
  volume         = {7},
  article-number = {13},
  doi            = {10.3390/data7020013},
  file           = {:PDF/#PraCegoVer- A Large Dataset for Image Captioning in Portuguese.pdf:PDF},
  url            = {https://www.mdpi.com/2306-5729/7/2/13},
}

@Article{Oluwasammi2021Survey,
  author    = {Oluwasammi, Ariyo and Aftab, Muhammad Umar and Qin, Zhiguang and Ngo, Son Tung and Doan, Thang Van and Nguyen, Son Ba and Nguyen, Son Hoang and Nguyen, Giang Hoang},
  journal   = {Complexity},
  title     = {Features to Text: A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning},
  year      = {2021},
  issn      = {1076-2787},
  month     = {Mar},
  pages     = {5538927},
  volume    = {2021},
  abstract  = {With the emergence of deep learning, computer vision has witnessed extensive advancement and has seen immense applications in multiple domains. Specifically, image captioning has become an attractive focal direction for most machine learning experts, which includes the prerequisite of object identification, location, and semantic understanding. In this paper, semantic segmentation and image captioning are comprehensively investigated based on traditional and state-of-the-art methodologies. In this survey, we deliberate on the use of deep learning techniques on the segmentation analysis of both 2D and 3D images using a fully convolutional network and other high-level hierarchical feature extraction methods. First, each domain preliminaries and concept are described, and then semantic segmentation is discussed alongside its relevant features, available datasets, and evaluation criteria. Also, the semantic information capturing of objects and their attributes is presented in relation to their annotation generation. Finally, analysis of the existing methods, their contributions, and relevance are highlighted, informing the importance of these methods and illuminating a possible research continuation for the application of semantic image segmentation and image captioning approaches.},
  day       = {23},
  doi       = {10.1155/2021/5538927},
  file      = {:PDF/Features to Text- A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning.pdf:PDF},
  publisher = {Hindawi},
  url       = {https://doi.org/10.1155/2021/5538927},
}

@InProceedings{sharma2020Survey,
  author    = {Sharma, Himanshu and Agrahari, Manmohan and Singh, Sujeet Kumar and Firoj, Mohd and Mishra, Ravi Kumar},
  booktitle = {2020 International Conference on Power Electronics IoT Applications in Renewable Energy and its Control (PARC)},
  title     = {Image Captioning: A Comprehensive Survey},
  year      = {2020},
  pages     = {325-328},
  doi       = {10.1109/PARC49193.2020.236619},
  file      = {:PDF/Image_Captioning_A_Comprehensive_Survey.pdf:PDF},
  groups    = {review},
}

@Article{Wang2022Diversity,
  author  = {Wang, Qingzhong and Wan, Jia and Chan, Antoni B.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {On Diversity in Image Captioning: Metrics and Methods},
  year    = {2022},
  number  = {2},
  pages   = {1035-1049},
  volume  = {44},
  doi     = {10.1109/TPAMI.2020.3013834},
  file    = {:PDF/On_Diversity_in_Image_Captioning_Metrics_and_Methods.pdf:PDF},
}

@Article{Chen_2021,
  author    = {Feng Chen and Xinyi Li and Jintao Tang and Shasha Li and Ting Wang},
  journal   = {Journal of Physics: Conference Series},
  title     = {A Survey on Recent Advances in Image Captioning},
  year      = {2021},
  month     = {may},
  number    = {1},
  pages     = {012053},
  volume    = {1914},
  abstract  = {Image captioning, an interdisciplinary research field of computer vision and natural language processing, has attracted extensive attention. Image captioning aims to produce reasonable and accurate natural language sentences to describe images. It requires the captioning model to recognize objects and describe their relationships accurately. Intuitively, it is difficult for a machine to have the general image understanding ability like human beings. However, deep learning provides the basis for intelligent exploration. In this review, we will focus on recent advanced deep methods for image captioning. We classify existing methods into different categories and discuss these categories respectively; meanwhile, we discuss the related datasets and evaluation metrics. We also prospect the future research directions.},
  doi       = {10.1088/1742-6596/1914/1/012053},
  file      = {:PDF/A Survey on Recent Advances in Image Captioning.pdf:PDF},
  publisher = {{IOP} Publishing},
  url       = {https://doi.org/10.1088/1742-6596/1914/1/012053},
}

@Article{Pierre2020,
  author     = {Pierre L. Dognin and Igor Melnyk and Youssef Mroueh and Inkit Padhi and Mattia Rigotti and Jarret Ross and Yair Schiff and Richard A. Young and Brian Belgodere},
  journal    = {CoRR},
  title      = {Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge},
  year       = {2020},
  volume     = {abs/2012.11696},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2012-11696.bib},
  eprint     = {2012.11696},
  eprinttype = {arXiv},
  timestamp  = {Wed, 06 Jan 2021 15:26:52 +0100},
  url        = {https://arxiv.org/abs/2012.11696},
}

@InProceedings{Sulaimi2021,
  author    = {Sulaimi, Mousa Al and Ahmad, Imtiaz and Jeragh, Mohammad},
  booktitle = {2021 29th Conference of Open Innovations Association (FRUCT)},
  title     = {Deep Image Captioning Survey: A Resource Availability Perspective},
  year      = {2021},
  pages     = {3-13},
  doi       = {10.23919/FRUCT52173.2021.9435534},
  file      = {:PDF/Deep_Image_Captioning_Survey_A_Resource_Availability_Perspective.pdf:PDF},
}

@Article{Luo2022ATR,
  author  = {Gaifang Luo and Lijun Cheng and Chao Jing and Can Zhao and Guozhu Song},
  journal = {IET Image Process.},
  title   = {A thorough review of models, evaluation metrics, and datasets on image captioning},
  year    = {2022},
  pages   = {311-332},
  volume  = {16},
  file    = {:PDF/A thorough review of models  evaluation metrics  and datasets on image captioning.pdf:PDF},
  groups  = {review},
}

@Article{sidorov2019textcaps,
  author  = {Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  journal = {arXiv preprint arXiv:2003.12462},
  title   = {TextCaps: a Dataset for Image Captioningwith Reading Comprehension},
  year    = {2020},
  comment = {https://github.com/guanghuixu/AnchorCaptioner/blob/main/projects/M4C_Captioner/README.md},
}

@Article{DBLP:journals/corr/abs-2007-09580,
  author     = {Chaorui Deng and Ning Ding and Mingkui Tan and Qi Wu},
  journal    = {CoRR},
  title      = {Length-Controllable Image Captioning},
  year       = {2020},
  volume     = {abs/2007.09580},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2007-09580.bib},
  comment    = {https://github.com/bearcatt/LaBERT},
  eprint     = {2007.09580},
  eprinttype = {arXiv},
  timestamp  = {Tue, 28 Jul 2020 14:46:12 +0200},
  url        = {https://arxiv.org/abs/2007.09580},
}

@Article{DBLP:journals/corr/abs-2001-01037,
  author     = {Jiamei Sun and Sebastian Lapuschkin and Wojciech Samek and Alexander Binder},
  journal    = {CoRR},
  title      = {Understanding Image Captioning Models beyond Visualizing Attention},
  year       = {2020},
  volume     = {abs/2001.01037},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2001-01037.bib},
  comment    = {https://github.com/SunJiamei/LRP-imagecaptioning-pytorch},
  eprint     = {2001.01037},
  eprinttype = {arXiv},
  file       = {:PDF/Explain and Improve- LRP-Inference Fine-Tuning for Image Captioning Models.pdf:PDF},
  timestamp  = {Sat, 23 Jan 2021 01:19:29 +0100},
  url        = {http://arxiv.org/abs/2001.01037},
}

@InProceedings{bugliarello-elliott-2021-role,
  author    = {Bugliarello, Emanuele and Elliott, Desmond},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {The Role of Syntactic Planning in Compositional Image Captioning},
  year      = {2021},
  address   = {Online},
  month     = apr,
  pages     = {593--607},
  publisher = {Association for Computational Linguistics},
  comment   = {https://github.com/e-bug/syncap},
  url       = {https://www.aclweb.org/anthology/2021.eacl-main.48},
}

@InProceedings{9428453,
  author    = {Li, Tong and Hu, Yunhui and Wu, Xinxiao},
  booktitle = {2021 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Image Captioning with Inherent Sentiment},
  year      = {2021},
  pages     = {1-6},
  comment   = {https://github.com/ezeli/InSentiCap_model
https://github.com/ezeli/BUTD_model},
  doi       = {10.1109/ICME51207.2021.9428453},
  file      = {:PDF/Image Captioning with Inherent Sentiment.pdf:PDF},
}

@Article{Yang2018Unsupervised,
  author     = {Yang Feng and Lin Ma and Wei Liu and Jiebo Luo},
  journal    = {CoRR},
  title      = {Unsupervised Image Captioning},
  year       = {2018},
  volume     = {abs/1811.10787},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1811-10787.bib},
  comment    = {https://github.com/fengyang0317/unsupervised_captioning},
  eprint     = {1811.10787},
  eprinttype = {arXiv},
  file       = {:PDF/Unsupervised Image Captioning.pdf:PDF},
  groups     = {Other deep learning methods},
  timestamp  = {Thu, 28 Nov 2019 07:55:53 +0100},
  url        = {http://arxiv.org/abs/1811.10787},
}

@InProceedings{honda-etal-2021-removing,
  author    = {Honda, Ukyo and Ushiku, Yoshitaka and Hashimoto, Atsushi and Watanabe, Taro and Matsumoto, Yuji},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning},
  year      = {2021},
  address   = {Online},
  month     = apr,
  pages     = {3692--3702},
  publisher = {Association for Computational Linguistics},
  abstract  = {Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details.},
  comment   = {https://github.com/ukyh/RemovingSpuriousAlignment},
  doi       = {10.18653/v1/2021.eacl-main.323},
  file      = {:PDF/Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning.pdf:PDF},
  url       = {https://aclanthology.org/2021.eacl-main.323},
}

@Article{DBLP:journals/corr/abs-2008-11009,
  author     = {Jian Han Lim and Chee Seng Chan and KamWoh Ng and Lixin Fan and Qiang Yang},
  journal    = {CoRR},
  title      = {Protect, Show, Attend and Tell: Image Captioning Model with Ownership Protection},
  year       = {2020},
  volume     = {abs/2008.11009},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2008-11009.bib},
  comment    = {https://github.com/jianhanlim/ipr-imagecaptioning},
  eprint     = {2008.11009},
  eprinttype = {arXiv},
  file       = {:PDF/Protect, show, attend and tell- Empowering image captioning models with ownership protection.pdf:PDF},
  timestamp  = {Wed, 08 Dec 2021 09:19:14 +0100},
  url        = {https://arxiv.org/abs/2008.11009},
}

@Article{DBLP:journals/corr/abs-2004-14451,
  author     = {Allen Nie and Reuben Cohn{-}Gordon and Christopher Potts},
  journal    = {CoRR},
  title      = {Pragmatic Issue-Sensitive Image Captioning},
  year       = {2020},
  volume     = {abs/2004.14451},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2004-14451.bib},
  comment    = {https://github.com/windweller/Pragmatic-ISIC},
  eprint     = {2004.14451},
  eprinttype = {arXiv},
  file       = {:PDF/Pragmatic Issue-Sensitive Image Captioning.pdf:PDF},
  timestamp  = {Sun, 03 May 2020 17:39:04 +0200},
  url        = {https://arxiv.org/abs/2004.14451},
}

@Article{mokady2021clipcap,
  author  = {Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal = {arXiv preprint arXiv:2111.09734},
  title   = {ClipCap: CLIP Prefix for Image Captioning},
  year    = {2021},
  comment = {https://github.com/rmokady/CLIP_prefix_caption},
  file    = {:PDF/ClipCap- CLIP Prefix for Image Captioning.pdf:PDF},
}

@Article{Subarna2021,
  author     = {Subarna Tripathi and Kien Nguyen and Tanaya Guha and Bang Du and Truong Q. Nguyen},
  journal    = {CoRR},
  title      = {SG2Caps: Revisiting Scene Graphs for Image Captioning},
  year       = {2021},
  volume     = {abs/2102.04990},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2102-04990.bib},
  comment    = {https://github.com/Kien085/SG2Caps},
  eprint     = {2102.04990},
  eprinttype = {arXiv},
  file       = {:PDF/In Defense of Scene Graphs for Image Captioning .pdf:PDF},
  timestamp  = {Thu, 18 Feb 2021 15:26:00 +0100},
  url        = {https://arxiv.org/abs/2102.04990},
}

@Article{DBLP:journals/corr/abs-2110-07831,
  author     = {Wenkai Yang and Yankai Lin and Peng Li and Jie Zhou and Xu Sun},
  journal    = {CoRR},
  title      = {{RAP:} Robustness-Aware Perturbations for Defending against Backdoor Attacks on {NLP} Models},
  year       = {2021},
  volume     = {abs/2110.07831},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2110-07831.bib},
  comment    = {https://github.com/lancopku/RAP},
  eprint     = {2110.07831},
  eprinttype = {arXiv},
  file       = {:PDF/RAP- Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models.pdf:PDF},
  timestamp  = {Wed, 15 Dec 2021 09:30:00 +0100},
  url        = {https://arxiv.org/abs/2110.07831},
}

@InProceedings{xu2021textcap,
  author    = {Guanghui Xu and Mingkui Tan and Shuaicheng Niu and Yucheng Luo and Qing Du and Qi Wu},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Towards Accurate Text-based Image Captioning with Content Diversity Exploration},
  year      = {2021},
  comment   = {https://github.com/guanghuixu/AnchorCaptioner},
  file      = {:PDF/Towards Accurate Text-based Image Captioning with Content Diversity Exploration.pdf:PDF},
}

@InProceedings{huang2019attention,
  author    = {Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle = {International Conference on Computer Vision},
  title     = {Attention on Attention for Image Captioning},
  year      = {2019},
  file      = {:PDF/Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
  groups    = {Self-Attention Encoding, Boosting LSTM with Self-Attention, attention taxonomy},
  printed   = {yes},
}

@Article{gurari2020captioning,
  author  = {Gurari, Danna and Zhao, Yinan and Zhang, Meng and Bhattacharya, Nilavra},
  journal = {arXiv preprint arXiv:2002.08565},
  title   = {Captioning Images Taken by People Who Are Blind},
  year    = {2020},
  comment = {https://github.com/Yinan-Zhao/AoANet_VizWiz},
  file    = {:PDF/Captioning Images Taken by People Who Are Blind.pdf:PDF},
}

@InProceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  file      = {:PDF/Krizhevsky2012.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@InCollection{Hochreiter2001,
  author               = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
  booktitle            = {A Field Guide to Dynamical Recurrent Neural Networks},
  publisher            = {IEEE Press},
  title                = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  year                 = {2001},
  editor               = {Kremer, S. C. and Kolen, J. F.},
  added-at             = {2008-02-26T12:05:08.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/279df6721c014a00bfac62abd7d5a9968/schaul},
  citeulike-article-id = {2374777},
  description          = {idsia},
  file                 = {:PDF/Gradient_Flow_in_Recurrent_Nets_the_Difficulty_of_.pdf:PDF},
  interhash            = {485c1bd6a99186c9414c6b9ddaed42c9},
  intrahash            = {79df6721c014a00bfac62abd7d5a9968},
  keywords             = {daanbib},
  priority             = {2},
  timestamp            = {2008-02-26T12:07:01.000+0100},
}

@Article{goodfellow2017deep,
  author  = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  journal = {Cambridge Massachusetts},
  title   = {Deep learning (adaptive computation and machine learning series)},
  year    = {2017},
  pages   = {321--359},
  file    = {:PDF/deep-learning-adaptive-computation-and-machine-learning-series BY Ian Goodfellow.pdf:PDF},
}

@InProceedings{pascanu2013difficulty,
  author       = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle    = {International conference on machine learning},
  title        = {On the difficulty of training recurrent neural networks},
  year         = {2013},
  organization = {PMLR},
  pages        = {1310--1318},
  file         = {:PDF/On the difficulty of training recurrent neural networks.pdf:PDF},
}

@InProceedings{Liu2019ExploringAD,
  author    = {Fenglin Liu and Xuancheng Ren and Yuanxin Liu and Kai Lei and Xu Sun},
  booktitle = {International Joint Conference on Artificial Intelligence},
  title     = {Exploring and Distilling Cross-Modal Information for Image Captioning},
  year      = {2019},
  file      = {:PDF/Exploring and Distilling Cross-Modal Information for Image Captioning.pdf:PDF},
  groups    = {attention},
}

@InProceedings{Gao2018,
  author    = {Gao, Lizhao and Wang, Bo and Wang, Wenmin},
  booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
  title     = {Image Captioning with Scene-Graph Based Semantic Concepts},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {225–229},
  publisher = {Association for Computing Machinery},
  series    = {ICMLC 2018},
  abstract  = {Different from existing approaches for image captioning, in this paper, we explore the co-occurrence dependency of high-level semantic concepts and propose a novel method with scene-graph based semantic representation for image captioning. To embed scene graph as an intermediate state, we divide the task of image captioning into two phases, called concept cognition and sentence construction respectively. We build a vocabulary of semantic concepts and propose a CNN-RNN-SVM framework to generate the scene-graph-based sequence, which is then transformed into a bit vector, as the input of RNN in the next phase. We evaluate our method on MS COCO dataset. Experimental results show that our approaches obtain a competitive or superior result to the state-of-the-arts.},
  doi       = {10.1145/3195106.3195114},
  file      = {:PDF/Image Captioning with Scene-graph Based Semantic Concepts.pdf:PDF},
  groups    = {graf},
  isbn      = {9781450363532},
  keywords  = {LSTM, Scene graph, Image captioning, CNN, Semantic representation},
  location  = {Macau, China},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3195106.3195114},
}

@InProceedings{gu2019unpaired,
  author    = {Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei and Zhao, Handong and Yang, Xu and Wang, Gang},
  booktitle = {ICCV},
  title     = {Unpaired Image Captioning via Scene Graph Alignments},
  year      = {2019},
  file      = {:PDF/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf:PDF},
  groups    = {graf},
}

@Article{Perone2018,
  author     = {Christian S. Perone and Roberto Pereira Silveira and Thomas S. Paula},
  journal    = {CoRR},
  title      = {Evaluation of sentence embeddings in downstream and linguistic probing tasks},
  year       = {2018},
  volume     = {abs/1806.06259},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1806-06259.bib},
  eprint     = {1806.06259},
  eprinttype = {arXiv},
  groups     = {embeddings},
  timestamp  = {Tue, 20 Jul 2021 13:58:53 +0200},
  url        = {http://arxiv.org/abs/1806.06259},
}

@InProceedings{gupta2019vico,
  author    = {Gupta, Tanmay and Schwing, Alexander and Hoiem, Derek},
  booktitle = {ICCV},
  title     = {ViCo: Word Embeddings from Visual Co-occurrences},
  year      = {2019},
  file      = {:PDF/ViCo- Word Embeddings from Visual Co-occurrences.pdf:PDF},
  groups    = {embeddings},
}

@Article{chu2020automatic,
  author    = {Chu, Yan and Yue, Xiao and Yu, Lei and Sergei, Mikhailov and Wang, Zhengkui},
  journal   = {Wireless Communications and Mobile Computing},
  title     = {Automatic image captioning based on ResNet50 and LSTM with soft attention},
  year      = {2020},
  pages     = {1--7},
  volume    = {2020},
  file      = {:PDF/Automatic Image Captioning Based on ResNet50 and LSTM with Soft Attention.pdf:PDF},
  publisher = {Hindawi Limited},
}

@Article{staniute2019Systematic,
  author         = {Staniūtė, Raimonda and Šešok, Dmitrij},
  journal        = {Applied Sciences},
  title          = {A Systematic Literature Review on Image Captioning},
  year           = {2019},
  issn           = {2076-3417},
  number         = {10},
  volume         = {9},
  abstract       = {Natural language problems have already been investigated for around five years. Recent progress in artificial intelligence (AI) has greatly improved the performance of models. However, the results are still not sufficiently satisfying. Machines cannot imitate human brains and the way they communicate, so it remains an ongoing task. Due to the increasing amount of information on this topic, it is very difficult to keep on track with the newest researches and results achieved in the image captioning field. In this study a comprehensive Systematic Literature Review (SLR) provides a brief overview of improvements in image captioning over the last four years. The main focus of the paper is to explain the most common techniques and the biggest challenges in image captioning and to summarize the results from the newest papers. Inconsistent comparison of results achieved in image captioning was noticed during this study and hence the awareness of incomplete data collection is raised in this paper. Therefore, it is very important to compare results of a newly created model produced with the newest information and not only with the state of the art methods. This SLR is a source of such information for researchers in order for them to be precisely correct on result comparison before publishing new achievements in the image caption generation field.},
  article-number = {2024},
  doi            = {10.3390/app9102024},
  file           = {:PDF/A Systematic Literature Review on Image Captioning .pdf:PDF},
  groups         = {review},
  url            = {https://www.mdpi.com/2076-3417/9/10/2024},
}

@Article{zhang2021DeepRelationEmbedding,
  author  = {Zhang, Yifan and Zhou, Wengang and Wang, Min and Tian, Qi and Li, Houqiang},
  journal = {IEEE Transactions on Image Processing},
  title   = {Deep Relation Embedding for Cross-Modal Retrieval},
  year    = {2021},
  pages   = {617-627},
  volume  = {30},
  doi     = {10.1109/TIP.2020.3038354},
  file    = {:PDF/Deep_Relation_Embedding_for_Cross-Modal_Retrieval.pdf:PDF},
  groups  = {embeddings, relations},
}

@InProceedings{Chen2021BestPooling,
  author    = {Chen, Jiacheng and Hu, Hexiang and Wu, Hao and Jiang, Yuning and Wang, Changhu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning the Best Pooling Strategy for Visual Semantic Embedding},
  year      = {2021},
  month     = {June},
  pages     = {15789-15798},
  file      = {:PDF/Learning the Best Pooling Strategy for Visual Semantic Embedding.pdf:PDF},
}

@InProceedings{Song2019Polysemous,
  author    = {Song, Yale and Soleymani, Mohammad},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval},
  year      = {2019},
  month     = {June},
  file      = {:PDF/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf:PDF},
  groups    = {embeddings},
}

@InProceedings{Ge2021Structured,
  author    = {Ge, Xuri and Chen, Fuhai and Jose, Joemon M. and Ji, Zhilong and Wu, Zhongqin and Liu, Xiao},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  title     = {Structured Multi-Modal Feature Embedding and Alignment for Image-Sentence Retrieval},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {5185–5193},
  publisher = {Association for Computing Machinery},
  series    = {MM '21},
  abstract  = {The current state-of-the-art image-sentence retrieval methods implicitly align the visual-textual fragments, like regions in images and words in sentences, and adopt attention modules to highlight the relevance of cross-modal semantic correspondences. However, the retrieval performance remains unsatisfactory due to a lack of consistent representation in both semantics and structural spaces. In this work, we propose to address the above issue from two aspects: (i) constructing intrinsic structure (along with relations) among the fragments of respective modalities, e.g., "dog → play → ball" in semantic structure for an image, and (ii) seeking explicit inter-modal structural and semantic correspondence between the visual and textual modalities.In this paper, we propose a novel Structured Multi-modal Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In order to jointly and explicitly learn the visual-textual embedding and the cross-modal alignment, SMFEA creates a novel multi-modal structured module with a shared context-aware referral tree. In particular, the relations of the visual and textual fragments are modeled by constructing Visual Context-aware Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree encoder (TCS-Tree) with shared labels, from which visual and textual features can be jointly learned and optimized. We utilize the multi-modal tree structure to explicitly align the heterogeneous image-sentence data by maximizing the semantic and structural similarity between corresponding inter-modal tree nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.},
  doi       = {10.1145/3474085.3475634},
  file      = {:PDF/Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval .pdf:PDF},
  isbn      = {9781450386517},
  keywords  = {context-aware structured trees, image-sentence retrieval, semantics and structural consistency, multimodal retrieval},
  location  = {Virtual Event, China},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3474085.3475634},
}

@Article{Lo2023ImageTextEmbedding,
  author  = {Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Image-Text Embedding Learning via Visual and Textual Semantic Reasoning},
  year    = {2023},
  number  = {1},
  pages   = {641-656},
  volume  = {45},
  doi     = {10.1109/TPAMI.2022.3148470},
  file    = {:PDF/Image-Text_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning.pdf:PDF},
}

@Article{Chen_Luo_2020,
  author       = {Chen, Tianlang and Luo, Jiebo},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Expressing Objects Just Like Words: Recurrent Visual Embedding for Image-Text Matching},
  year         = {2020},
  month        = {Apr.},
  number       = {07},
  pages        = {10583-10590},
  volume       = {34},
  abstractnote = {Existing image-text matching approaches typically infer the similarity of an image-text pair by capturing and aggregating the affinities between the text and each independent object of the image. However, they ignore the connections between the objects that are semantically related. These objects may collectively determine whether the image corresponds to a text or not. To address this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN) which processes images and sentences symmetrically by recurrent neural networks (RNN). In particular, given an input image-text pair, our model reorders the image objects based on the positions of their most related words in the text. In the same way as extracting the hidden features from word embeddings, the model leverages RNN to extract high-level object features from the reordered object inputs. We validate that the high-level object features contain useful joint information of semantically related objects, which benefit the retrieval task. To compute the image-text similarity, we incorporate a Multi-attention Cross Matching Model into DP-RNN. It aggregates the affinity between objects and words with cross-modality guided attention and self-attention. Our model achieves the state-of-the-art performance on Flickr30K dataset and competitive performance on MS-COCO dataset. Extensive experiments demonstrate the effectiveness of our model.},
  doi          = {10.1609/aaai.v34i07.6631},
  file         = {:PDF/Expressing Objects Just Like Words- Recurrent Visual Embedding for Image-Text Matching .pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6631},
}

@Article{Zhou_Niu_Wang_Gao_Zhang_Hua_2020,
  author       = {Zhou, Mo and Niu, Zhenxing and Wang, Le and Gao, Zhanning and Zhang, Qilin and Hua, Gang},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Ladder Loss for Coherent Visual-Semantic Embedding},
  year         = {2020},
  month        = {Apr.},
  number       = {07},
  pages        = {13050-13057},
  volume       = {34},
  abstractnote = {For visual-semantic embedding, the existing methods normally treat the relevance between queries and candidates in a bipolar way – relevant or irrelevant, and all “irrelevant” candidates are uniformly pushed away from the query by an equal margin in the embedding space, regardless of their various proximity to the query. This practice disregards relatively discriminative information and could lead to suboptimal ranking in the retrieval results and poorer user experience, especially in the long-tail query scenario where a matching candidate may not necessarily exist. In this paper, we introduce a continuous variable to model the relevance degree between queries and multiple candidates, and propose to learn a coherent embedding space, where candidates with higher relevance degrees are mapped closer to the query than those with lower relevance degrees. In particular, the new ladder loss is proposed by extending the triplet loss inequality to a more general inequality chain, which implements variable push-away margins according to respective relevance degrees. In addition, a proper Coherent Score metric is proposed to better measure the ranking results including those “irrelevant” candidates. Extensive experiments on multiple datasets validate the efficacy of our proposed method, which achieves significant improvement over existing state-of-the-art methods.},
  doi          = {10.1609/aaai.v34i07.7006},
  file         = {:PDF/Ladder Loss for Coherent Visual-Semantic Embedding .pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/7006},
}

@Article{Xiao2019Dense,
  author   = {Xinyu Xiao and Lingfeng Wang and Kun Ding and Shiming Xiang and Chunhong Pan},
  journal  = {Pattern Recognition},
  title    = {Dense semantic embedding network for image captioning},
  year     = {2019},
  issn     = {0031-3203},
  pages    = {285-296},
  volume   = {90},
  abstract = {Recently, attributes that contain high-level semantic information of image are always used as a complementary knowledge to improve image captioning performance. However, the use of attributes in prior works cannot excavate the latent visual concepts effectively. At each time step, the semantic information which is sensitive to the predicted word could be different. In this paper, we propose a Dense Semantic Embedding Network (DSEN) for this task. The distinct operation of this network is to densely embed the attributes with the multi-modal of image and text at each step of word generation. The discriminative semantic information hidden in these attributes is formatted in form of global likelihood probabilities. As a result, this dense embedding can modulate the feature distributions of the image, text modals and the hidden states to explicit semantic representation. Furthermore, to improve the discrimination of attributes, a Threshold ReLU (TReLU) is proposed. In addition, a bidirectional LSTM structure is incorporated into the DSEN to capture both the previous and future contexts. Extensive experiments on the COCO and Flickr30K datasets achieve superior results when compared with the state-of-the-art models for the tasks of both image captioning and image-text cross modal retrieval. Most remarkably, our method obtains outstanding performance on the retrieval task, compared with the state-of-the-art models.},
  doi      = {https://doi.org/10.1016/j.patcog.2019.01.028},
  file     = {:PDF/Dense semantic embedding network for image captioning.pdf:PDF},
  keywords = {Image captioning, Retrieval, High-level semantic information, Visual concept, Densely embedding, Long short-term memory},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320319300494},
}

@InProceedings{Park2020MHSAN,
  author    = {Park, Geondo and Han, Chihye and Yoon, Wonjun and Kim, Daeshik},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding},
  year      = {2020},
  month     = {March},
  file      = {:PDF/MHSAN- Multi-Head Self-Attention Network for Visual Semantic Embedding.pdf:PDF},
}

@Article{Che2020Visual,
  author  = {Che, Wenbin and Fan, Xiaopeng and Xiong, Ruiqin and Zhao, Debin},
  journal = {IEEE Transactions on Multimedia},
  title   = {Visual Relationship Embedding Network for Image Paragraph Generation},
  year    = {2020},
  number  = {9},
  pages   = {2307-2320},
  volume  = {22},
  doi     = {10.1109/TMM.2019.2954750},
  file    = {:PDF/Visual Relationship Embedding Network for Image Paragraph Generation.pdf:PDF},
}

@Article{Fu2020Rich,
  author  = {Fu, Xin and Zhao, Yao and Wei, Yunchao and Zhao, Yufeng and Wei, Shikui},
  journal = {IEEE Transactions on Multimedia},
  title   = {Rich Features Embedding for Cross-Modal Retrieval: A Simple Baseline},
  year    = {2020},
  number  = {9},
  pages   = {2354-2365},
  volume  = {22},
  doi     = {10.1109/TMM.2019.2957948},
  file    = {:PDF/Rich Features Embedding for Cross-Modal Retrieval- A Simple Baseline .pdf:PDF},
  groups  = {embeddings},
}

@Misc{https://doi.org/10.48550/arxiv.2210.14472,
  author    = {Weeraprameshwara, Gihan and Jayawickrama, Vihanga and de Silva, Nisansa and Wijeratne, Yudhanjaya},
  title     = {Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource Languages},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2210.14472},
  file      = {:PDF/Sinhala Sentence Embedding- A Two-Tiered Structure for Low-Resource Languages .pdf:PDF},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2210.14472},
}

@Article{zhongang2021,
  author   = {Zhongang Qi and Saeed Khorram and Li Fuxin},
  journal  = {Artificial Intelligence},
  title    = {Embedding deep networks into visual explanations},
  year     = {2021},
  issn     = {0004-3702},
  pages    = {103435},
  volume   = {292},
  abstract = {In this paper, we propose a novel Explanation Neural Network (XNN) to explain the predictions made by a deep network. The XNN works by learning a nonlinear embedding of a high-dimensional activation vector of a deep network layer into a low-dimensional explanation space while retaining faithfulness i.e., the original deep learning predictions can be constructed from the few concepts extracted by our explanation network. We then visualize such concepts for human to learn about the high-level concepts that the deep network is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A pull-away term is applied to SRAE to make the bases of the explanation space more orthogonal to each other. A visualization system is then introduced for human understanding of the features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks. We conducted a human study, which shows that the proposed approach outperforms single saliency map baselines, and improves human performance on a difficult classification task. Besides, several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement.},
  doi      = {https://doi.org/10.1016/j.artint.2020.103435},
  file     = {:PDF/Embedding deep networks into visual explanations.pdf:PDF},
  keywords = {Deep neural networks, Embedding, Visual explanations},
  url      = {https://www.sciencedirect.com/science/article/pii/S000437022030182X},
}

@Article{ZHANG2020,
  author   = {Xiaodan Zhang and Shengfeng He and Xinhang Song and Rynson W.H. Lau and Jianbin Jiao and Qixiang Ye},
  journal  = {Neurocomputing},
  title    = {Image captioning via semantic element embedding},
  year     = {2020},
  issn     = {0925-2312},
  pages    = {212-221},
  volume   = {395},
  abstract = {Image caption approaches that use the global Convolutional Neural Network (CNN) features are not able to represent and describe all the important elements in complex scenes. In this paper, we propose to enrich the semantic representations of images and update the language model by proposing semantic element embedding. For the semantic element discovery, an object detection module is used to predict regions of the image, and a captioning model, Long Short-Term Memory (LSTM), is employed to generate local descriptions for these regions. The predicted descriptions and categories are used to generate the semantic feature, which not only contains detailed information but also shares a word space with descriptions, and thus bridges the modality gap between visual images and semantic captions. We further integrate the CNN feature with the semantic feature into the proposed Element Embedding LSTM (EE-LSTM) model to predict a language description. Experiments on MS COCO datasets demonstrate that the proposed approach outperforms conventional caption methods and is flexible to combine with baseline models to achieve superior performance.},
  doi      = {https://doi.org/10.1016/j.neucom.2018.02.112},
  file     = {:PDF/Image captioning via semantic element embedding.pdf:PDF},
  keywords = {Image captioning, Element embedding, CNN, LSTM},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231219309130},
}

@Article{Zhao_Wu_Zhang_2020,
  author       = {Zhao, Wentian and Wu, Xinxiao and Zhang, Xiaoxun},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {MemCap: Memorizing Style Knowledge for Image Captioning},
  year         = {2020},
  month        = {Apr.},
  number       = {07},
  pages        = {12984-12992},
  volume       = {34},
  abstractnote = {Generating stylized captions for images is a challenging task since it requires not only describing the content of the image accurately but also expressing the desired linguistic style appropriately. In this paper, we propose MemCap, a novel stylized image captioning method that explicitly encodes the knowledge about linguistic styles with memory mechanism. Rather than relying heavily on a language model to capture style factors in existing methods, our method resorts to memorizing stylized elements learned from training corpus. Particularly, we design a memory module that comprises a set of embedding vectors for encoding style-related phrases in training corpus. To acquire the style-related phrases, we develop a sentence decomposing algorithm that splits a stylized sentence into a style-related part that reflects the linguistic style and a content-related part that contains the visual content. When generating captions, our MemCap first extracts content-relevant style knowledge from the memory module via an attention mechanism and then incorporates the extracted knowledge into a language model. Extensive experiments on two stylized image captioning datasets (SentiCap and FlickrStyle10K) demonstrate the effectiveness of our method.},
  doi          = {10.1609/aaai.v34i07.6998},
  file         = {:PDF/MemCap- Memorizing Style Knowledge for Image Captioning .pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6998},
}

@InProceedings{Liu2019,
  author    = {Liu, Fenglin and Gao, Meng and Zhang, Tianhao and Zou, Yuexian},
  booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
  title     = {Exploring Semantic Relationships for Image Captioning without Parallel Data},
  year      = {2019},
  pages     = {439-448},
  doi       = {10.1109/ICDM.2019.00054},
  file      = {:PDF/Exploring_Semantic_Relationships_for_Image_Captioning_without_Parallel_Data.pdf:PDF},
  groups    = {relations},
}

@Article{Bai2021,
  author   = {Cong Bai and Anqi Zheng and Yuan Huang and Xiang Pan and Nan Chen},
  journal  = {Displays},
  title    = {Boosting convolutional image captioning with semantic content and visual relationship},
  year     = {2021},
  issn     = {0141-9382},
  pages    = {102069},
  volume   = {70},
  abstract = {Image captioning aims to display automatically the natural language sentence for the image by the computer, which is an important but a challenging task which covers the fields of computer vision and natural language processing. This task is dominated by Long-short term memory (LSTM) based solutions. Although many progresses have been made based on LSTM in recent years, the model based on LSTM relies on serialized generation of descriptions, which cannot be processed in parallel and pay less attentions to the hierarchical structure of the captions. In order to solve this problem, we propose a framework using a CNN-based generation model to generate image captions with the help of conditional generative adversarial training (CGAN). Furthermore, multi-modal graph convolution network(MGCN) is used to exploit visual relationships between objects for generating the captions with semantic meanings, in which the scene graph is used as the bridge to connect objects, attributes and visual relationship information together to generate better captions. Extensive experiments are conducted on MSCOCO database and the results show that our method could achieve better or comparable scores compared with state-of-the-art methods. Ablation experimental results show that CGAN and MGCN can reflect a better visual relationships between objects in image and thus can generate better captions with richer semantic content.},
  doi      = {https://doi.org/10.1016/j.displa.2021.102069},
  file     = {:PDF/ 0141-9382\:© 2021 Published by Elsevier B.V.Boosting convolutional image captioning with semantic content and visual relationship.pdf:PDF},
  keywords = {Image captioning, Generative adversarial network, Graph convolution network},
  url      = {https://www.sciencedirect.com/science/article/pii/S0141938221000792},
}

@Article{fudholi2022study,
  author  = {Fudholi, Dhomas Hatta and Zahra, Annisa and Nayoan, Royan Abida N},
  journal = {Kinetik: Game Technology, Information System, Computer Network, Computing, Electronics, and Control},
  title   = {A Study on Visual Understanding Image Captioning using Different Word Embeddings and CNN-Based Feature Extractions},
  year    = {2022},
  pages   = {91--98},
  file    = {:PDF/1394-Article Text-124126475-1-10-20220329.pdf:PDF},
}

@InProceedings{9431465,
  author    = {Atliha, Viktar and Šešok, Dmitrij},
  booktitle = {2021 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)},
  title     = {Pretrained Word Embeddings for Image Captioning},
  year      = {2021},
  pages     = {1-4},
  doi       = {10.1109/eStream53087.2021.9431465},
  file      = {:PDF/Pretrained_Word_Embeddings_for_Image_Captioning.pdf:PDF},
}

@InProceedings{Osaid2022,
  author    = {Osaid, Muhammad and Memon, Zulfiqar Ali},
  booktitle = {2022 International Conference on Emerging Trends in Smart Technologies (ICETST)},
  title     = {A Survey On Image Captioning},
  year      = {2022},
  pages     = {1-6},
  doi       = {10.1109/ICETST55735.2022.9922935},
  file      = {:PDF/A_Survey_On_Image_Captioning.pdf:PDF},
  groups    = {review},
}

@Article{Uddagiri2022,
  author    = {Uddagiri Sirisha and Bolem Sai Chandana},
  journal   = {Cogent Engineering},
  title     = {Semantic interdisciplinary evaluation of image captioning models},
  year      = {2022},
  number    = {1},
  pages     = {2104333},
  volume    = {9},
  doi       = {10.1080/23311916.2022.2104333},
  eprint    = {https://doi.org/10.1080/23311916.2022.2104333},
  file      = {:PDF/Semantic interdisciplinary evaluation of image captioning models.pdf:PDF},
  groups    = {review},
  publisher = {Cogent OA},
  ranking   = {rank5},
  url       = {https://doi.org/10.1080/23311916.2022.2104333},
}

@Misc{Cao2022,
  author    = {Cao, Min and Li, Shiping and Li, Juntao and Nie, Liqiang and Zhang, Min},
  title     = {Image-text Retrieval: A Survey on Recent Research and Development},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2203.14713},
  file      = {:PDF/Image-text Retrieval- A Survey on Recent Research and Development.pdf:PDF;:PDF/2106.11342v5.pdf:PDF},
  groups    = {review},
  keywords  = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2203.14713},
}

@Article{ZELASZCZYK2023302,
  author   = {Maciej Żelaszczyk and Jacek Mańdziuk},
  journal  = {Information Fusion},
  title    = {Cross-modal text and visual generation: A systematic review. Part 1: Image to text},
  year     = {2023},
  issn     = {1566-2535},
  pages    = {302-329},
  volume   = {93},
  abstract = {We review the existing literature on generating text from visual data under the cross-modal generation umbrella, which affords us to compare and contrast various approaches taking visual data as input and producing text outputs, while not limiting the analysis to narrowly-defined areas, such as image captioning. We provide a breakdown of image-to-text generation methods into generative and non-generative image captioning and visual dialogue, with further distinctions provided for relevant areas. We provide template methods and discuss the existing research in light of such template methods, highlighting both the salient commonalities between different approach and significant departures from the templates. Where it is of interest, we also provide comparisons between templates for distinct areas. To achieve a comprehensive review, we focus on research papers published at 8 leading machine learning conferences in the years 2016–2021 as well as a number of papers, which do not conform to our search criteria, but nonetheless come from leading venues. This is the first review we know of to provide a systematic description of the current state of image-to-text generation and tie distinct research areas together looking at them through the lens of cross-modal generation.},
  doi      = {https://doi.org/10.1016/j.inffus.2023.01.008},
  file     = {:PDF/Cross-modal text and visual generation- A systematic review. Part 1- Image to text.pdf:PDF},
  groups   = {review},
  keywords = {Image-to-text generation, Text-to-image generation, Image captioning, Cross-modal learning, Multimodal learning, Representation learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253523000143},
}

@Misc{Zhou2023,
  author    = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
  title     = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2302.09419},
  file      = {:PDF/A Comprehensive Survey on Pretrained Foundation Models- A History from BERT to ChatGPT.pdf:PDF},
  groups    = {review},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2302.09419},
}

@Misc{akkus2023multimodaldeeplearning,
  author        = {Cem Akkus and Luyang Chu and Vladana Djakovic and Steffen Jauch-Walser and Philipp Koch and Giacomo Loss and Christopher Marquardt and Marco Moldovan and Nadja Sauter and Maximilian Schneider and Rickmer Schulte and Karol Urbanczyk and Jann Goschenhofer and Christian Heumann and Rasmus Hvingelby and Daniel Schalk and Matthias Aßenmacher},
  title         = {Multimodal Deep Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2301.04856},
  file          = {:PDF/2301.04856v1.pdf:PDF},
  groups        = {review, word2vec},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2301.04856},
}

@Misc{Wang2023,
  author    = {Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
  title     = {Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2302.10035},
  file      = {:PDF/large-scale.pdf:PDF},
  groups    = {review},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2302.10035},
}

@InProceedings{Geetha2023,
  author    = {U, Vijetha and V, Geetha},
  booktitle = {2023 13th International Conference on Cloud Computing, Data Science and Engineering (Confluence)},
  title     = {Opportunities and Challenges in Development of Support System for Visually Impaired: A Survey},
  year      = {2023},
  pages     = {684-690},
  doi       = {10.1109/Confluence56041.2023.10048861},
  file      = {:PDF/Opportunities_and_Challenges_in_Development_of_Support_System_for_Visually_Impaired_A_Survey.pdf:PDF},
  groups    = {review},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  year      = {2010},
  address   = {Madison, WI, USA},
  pages     = {807–814},
  publisher = {Omnipress},
  series    = {ICML'10},
  abstract  = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  file      = {:PDF/reluICML.pdf:PDF},
  isbn      = {9781605589077},
  journal   = {Proceedings of ICML},
  location  = {Haifa, Israel},
  numpages  = {8},
}

@InProceedings{Liu2022Deconfound,
  author    = {Liu, Bing and Wang, Dong and Yang, Xu and Zhou, Yong and Yao, Rui and Shao, Zhiwen and Zhao, Jiaqi},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Show, Deconfound and Tell: Image Captioning with Causal Inference},
  year      = {2022},
  pages     = {18020-18029},
  doi       = {10.1109/CVPR52688.2022.01751},
  file      = {:PDF/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf:PDF},
}

@InProceedings{Kulkarni2011SimpleImgDesc,
  author    = {Kulkarni, Girish and Premraj, Visruth and Dhar, Sagnik and Li, Siming and Choi, Yejin and Berg, Alexander C and Berg, Tamara L},
  booktitle = {CVPR 2011},
  title     = {Baby talk: Understanding and generating simple image descriptions},
  year      = {2011},
  month     = {June},
  pages     = {1601-1608},
  abstract  = {We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.},
  doi       = {10.1109/CVPR.2011.5995466},
  file      = {:PDF/Baby_talk_Understanding_and_generating_simple_image_descriptions.pdf:PDF},
  groups    = {template},
  issn      = {1063-6919},
  keywords  = {Labeling;Detectors;Computer vision;Natural languages;Visualization;Object detection;Image recognition},
}

@InProceedings{Li2011,
  author    = {Li, Siming and Kulkarni, Girish and Berg, Tamara L and Berg, Alexander C and Choi, Yejin},
  booktitle = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
  title     = {Composing Simple Image Descriptions using Web-scale N-grams},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  month     = jun,
  pages     = {220--228},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/W11-0326.pdf:PDF},
  groups    = {template},
  url       = {https://aclanthology.org/W11-0326},
}

@InProceedings{Mitchel2012,
  author    = {Mitchell, Margaret and Han, Xufeng and Dodge, Jesse and Mensch, Alyssa and Goyal, Amit and Berg, Alex and Yamaguchi, Kota and Berg, Tamara and Stratos, Karl and Daum\'{e}, Hal},
  booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
  title     = {Midge: Generating Image Descriptions from Computer Vision Detections},
  year      = {2012},
  address   = {USA},
  pages     = {747–756},
  publisher = {Association for Computational Linguistics},
  series    = {EACL '12},
  abstract  = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
  file      = {:PDF/E12-1076.pdf:PDF},
  groups    = {template},
  isbn      = {9781937284190},
  location  = {Avignon, France},
  numpages  = {10},
}



@InProceedings{Xu2017,
  author    = {Xu, Kaisheng and Wang, Hanli and Tang, Pengjie},
  booktitle = {2017 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Image captioning with deep LSTM based on sequential residual},
  year      = {2017},
  pages     = {361-366},
  doi       = {10.1109/ICME.2017.8019408},
  file      = {:PDF/Image_captioning_with_deep_LSTM_based_on_sequential_residual.pdf:PDF},
  groups    = {vgg16, resnet},
}

@Article{Gu2017,
  author  = {Jiuxiang Gu and G. Wang and Jianfei Cai and Tsuhan Chen},
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title   = {An Empirical Study of Language CNN for Image Captioning},
  year    = {2016},
  pages   = {1231-1240},
  file    = {:PDF/An_Empirical_Study_of_Language_CNN_for_Image_Captioning.pdf:PDF},
  groups  = {vgg16, global CNN features},
}


@Article{Liu2018,
  author  = {Liu, Shuang and Bai, Liang and Hu, Yanli and Wang, Haoran},
  journal = {MATEC Web of Conferences},
  title   = {Image Captioning Based on Deep Neural Networks},
  year    = {2018},
  month   = {11},
  pages   = {01052},
  volume  = {232},
  doi     = {10.1051/matecconf/201823201052},
  file    = {:PDF/Image_Captioning_Based_on_Deep_Neural_Networks.pdf:PDF},
  groups  = {vgg16, resnet},
}

@Article{Subash_2019,
  author    = {R. Subash and R. Jebakumar and Yash Kamdar and Nishit Bhatt},
  journal   = {Journal of Physics: Conference Series},
  title     = {Automatic Image Captioning Using Convolution Neural Networks and LSTM},
  year      = {2019},
  month     = {nov},
  number    = {1},
  pages     = {012096},
  volume    = {1362},
  abstract  = {PC vision has turned out to be universal in our general public, with applications in a few fields. Given a lot of pictures, with its inscription, make a prescient model which produces regular, inventive, and intriguing subtitles for the concealed picture. A speedy look at a picture is adequate for a human to call attention to and portray a monstrous measure of insights regarding the visual scene. To rearrange the current issue of producing inscriptions for pictures by making a model which would give exact subtitles to these pictures which can be additionally utilized in other helpful applications and use cases. Be that as it may, this momentous capacity has ended up being a tricky errand for our visual acknowledgment models. Most of the past research in scene acknowledgment has concentrated on naming pictures with a predetermined arrangement of visual classifications and extraordinary advancement has been accom-plished in these undertakings. For a question picture, the past strategies recover pertinent hopeful normal language states by outwardly contrasting the inquiry picture with database pictures. In any case, while shut vocabularies of visual ideas comprise a helpful demonstrating suspicion, they are boundlessly prohibitive when contrasted with the colossal measure of rich depictions that a human can form. These methodologies forced a breaking point on the assortment of inscriptions produced. The model ought to be exempt of suppositions regarding explicit pre decided formats, standards or classes and rather depend on figuring out how to create sentences from the preparation information. The model proposed utilizes Convolution Neural Networks which help to separate highlights of the picture whose subtitle is to be created and afterward by utilizing a probabilistic methodology and Natural Language Processing Techniques reasonable sentences are framed and inscriptions are produced.},
  doi       = {10.1088/1742-6596/1362/1/012096},
  file      = {:PDF/Subash_2019_J._Phys.__Conf._Ser._1362_012096.pdf:PDF},
  groups    = {vgg16},
  publisher = {IOP Publishing},
  url       = {https://dx.doi.org/10.1088/1742-6596/1362/1/012096},
}

@InProceedings{Dong2017,
  author    = {Dong, Hao and Zhang, Jingqing and McIlwraith, Douglas and Guo, Yike},
  booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
  title     = {I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation},
  year      = {2017},
  pages     = {2015–2019},
  publisher = {IEEE Press},
  abstract  = {Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose dataset (MHP) without using sentence annotation.},
  doi       = {10.1109/ICIP.2017.8296635},
  file      = {:PDF/1703.06676.pdf:PDF},
  groups    = {inception},
  location  = {Beijing, China},
  numpages  = {5},
  url       = {https://doi.org/10.1109/ICIP.2017.8296635},
}

@Article{Xian2017,
  author  = {Xian, Yang and Tian, Yingli},
  journal = {IEEE Transactions on Image Processing},
  title   = {Self-Guiding Multimodal LSTM-When We Do Not Have a Perfect Training Dataset for Image Captioning},
  year    = {2017},
  month   = {09},
  volume  = {PP},
  doi     = {10.1109/TIP.2019.2917229},
  file    = {:PDF/Self-Guiding_Multimodal_LSTMWhen_We_Do_Not_Have_a_Perfect_Training_Dataset_for_Image_Captioning.pdf:PDF},
  groups  = {inception},
}

@Article{Mao2014,
  author     = {Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Alan L. Yuille},
  journal    = {CoRR},
  title      = {Explain Images with Multimodal Recurrent Neural Networks},
  year       = {2014},
  volume     = {abs/1410.1090},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/MaoXYWY14.bib},
  eprint     = {1410.1090},
  eprinttype = {arXiv},
  file       = {:PDF/1410.1090.pdf:PDF},
  groups     = {resnet},
  timestamp  = {Tue, 15 Sep 2020 18:57:32 +0200},
  url        = {http://arxiv.org/abs/1410.1090},
}

@InProceedings{Mikolov2013EfficientEO,
  author    = {Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  booktitle = {International Conference on Learning Representations},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  year      = {2013},
  file      = {:PDF/1301.3781.pdf:PDF},
  groups    = {word2vec},
}

@Article{schuster1997,
  author  = {Schuster, Mike and Paliwal, Kuldip},
  journal = {Signal Processing, IEEE Transactions on},
  title   = {Bidirectional recurrent neural networks},
  year    = {1997},
  month   = {12},
  pages   = {2673 - 2681},
  volume  = {45},
  doi     = {10.1109/78.650093},
  file    = {:PDF/Bidirectional recurrent neural networks .pdf:PDF},
}

@Article{Zhuang2020Tranfer,
  author  = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal = {Proceedings of the IEEE},
  title   = {A Comprehensive Survey on Transfer Learning},
  year    = {2020},
  month   = {07},
  pages   = {1-34},
  volume  = {PP},
  doi     = {10.1109/JPROC.2020.3004555},
  file    = {:PDF/A Comprehensive Survey on Transfer Learning.pdf:PDF},
}

@Article{Pan2010TransferLearning,
  author  = {Pan, Sinno Jialin and Yang, Qiang},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {A Survey on Transfer Learning},
  year    = {2010},
  number  = {10},
  pages   = {1345-1359},
  volume  = {22},
  doi     = {10.1109/TKDE.2009.191},
  file    = {:PDF/A_Survey_on_Transfer_Learning.pdf:PDF},
}

@InProceedings{Raina2007,
  author    = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning},
  title     = {Self-Taught Learning: Transfer Learning from Unlabeled Data},
  year      = {2007},
  address   = {New York, NY, USA},
  pages     = {759–766},
  publisher = {Association for Computing Machinery},
  series    = {ICML '07},
  abstract  = {We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.},
  doi       = {10.1145/1273496.1273592},
  file      = {:PDF/1273496.1273592.pdf:PDF},
  isbn      = {9781595937933},
  location  = {Corvalis, Oregon, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1273496.1273592},
}

@Article{KirosSZ2014Unifying,
  author     = {Ryan Kiros and Ruslan Salakhutdinov and Richard S. Zemel},
  journal    = {CoRR},
  title      = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models},
  year       = {2014},
  volume     = {abs/1411.2539},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/KirosSZ14.bib},
  eprint     = {1411.2539},
  eprinttype = {arXiv},
  file       = {:PDF/1411.2539.pdf:PDF},
  groups     = {multimodal},
  timestamp  = {Mon, 13 Aug 2018 16:47:15 +0200},
  url        = {http://arxiv.org/abs/1411.2539},
}

@InProceedings{Srivastava2018ASO,
  author    = {Gargi Srivastava and Rajeev Srivastava},
  booktitle = {International Conference on Mathematics and Computing},
  title     = {A Survey on Automatic Image Captioning},
  year      = {2018},
  file      = {:PDF/.1273496.1273592.pdf.~771c99d8:~771c99d8},
  groups    = {review},
}

@InProceedings{Farhadi2010,
  author    = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
  booktitle = {Computer Vision -- ECCV 2010},
  title     = {Every Picture Tells a Story: Generating Sentences from Images},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  pages     = {15--29},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
  file      = {:PDF/Every picture tells a story- generating sentences from images.pdf:PDF},
  groups    = {retrieval},
  isbn      = {978-3-642-15561-1},
}

@InProceedings{Karpathy2014,
  author    = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li F},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {27},
  file      = {:PDF/NIPS-2014-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping-Paper.pdf:PDF},
  groups    = {Earlier Deep Models},
  url       = {https://proceedings.neurips.cc/paper/2014/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf},
}

@Article{Delbrouck2018,
  author     = {Jean{-}Benoit Delbrouck and St{\'{e}}phane Dupont},
  journal    = {CoRR},
  title      = {Bringing back simplicity and lightliness into neural image captioning},
  year       = {2018},
  volume     = {abs/1810.06245},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1810-06245.bib},
  eprint     = {1810.06245},
  eprinttype = {arXiv},
  file       = {:PDF/1810.06245.pdf:PDF},
  timestamp  = {Tue, 30 Oct 2018 20:39:56 +0100},
  url        = {http://arxiv.org/abs/1810.06245},
}

@Article{Chollet2017Xception,
  author     = {Fran{\c{c}}ois Chollet},
  journal    = {CoRR},
  title      = {Xception: Deep Learning with Depthwise Separable Convolutions},
  year       = {2016},
  volume     = {abs/1610.02357},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/Chollet16a.bib},
  eprint     = {1610.02357},
  eprinttype = {arXiv},
  file       = {:PDF/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf:PDF},
  timestamp  = {Mon, 13 Aug 2018 16:46:20 +0200},
  url        = {http://arxiv.org/abs/1610.02357},
}

@InProceedings{Baldi2013Dropout,
  author    = {Baldi, Pierre and Sadowski, Peter J},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Understanding Dropout},
  year      = {2013},
  editor    = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {26},
  file      = {:PDF/NIPS-2013-understanding-dropout-Paper.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf},
}

@Article{HochReiter1998Vanishing,
  author  = {Hochreiter, Sepp},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  title   = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
  year    = {1998},
  month   = {04},
  pages   = {107-116},
  volume  = {6},
  doi     = {10.1142/S0218488598000094},
  file    = {:PDF/The_Vanishing_Gradient_Problem_During_Learning_Rec.pdf:PDF},
}

@InProceedings{huang2017densely,
  author    = {G. Huang and Z. Liu and L. Van Der Maaten and K. Q. Weinberger},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Densely Connected Convolutional Networks},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = {jul},
  pages     = {2261-2269},
  publisher = {IEEE Computer Society},
  abstract  = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  doi       = {10.1109/CVPR.2017.243},
  file      = {:PDF/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {training;convolution;network architecture;convolutional codes;neural networks;road transportation},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.243},
}

@Article{Xu2019SceneGC,
  author  = {N. Xu and Anan Liu and Jing Liu and Weizhi Nie and Yuting Su},
  journal = {J. Vis. Commun. Image Represent.},
  title   = {Scene graph captioner: Image captioning based on structural visual representation},
  year    = {2019},
  pages   = {477-485},
  volume  = {58},
  file    = {:PDF/1-s2.0-S1047320318303535-main.pdf:PDF},
}

@Article{Sugano2016SeeingWH,
  author  = {Yusuke Sugano and Andreas Bulling},
  journal = {ArXiv},
  title   = {Seeing with Humans: Gaze-Assisted Neural Image Captioning},
  year    = {2016},
  volume  = {abs/1608.05203},
  file    = {:PDF/1608.05203v1.pdf:PDF},
  groups  = {review, Exploiting human attention},
}

@InProceedings{Lebret2015Phrase,
  author    = {Lebret, R\'{e}mi and Pinheiro, Pedro O. and Collobert, Ronan},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  title     = {Phrase-Based Image Captioning},
  year      = {2015},
  pages     = {2085–2094},
  publisher = {JMLR.org},
  series    = {ICML'15},
  abstract  = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
  file      = {:PDF/lebret15.pdf:PDF},
  groups    = {Earlier Deep Models},
  location  = {Lille, France},
  numpages  = {10},
}

@Article{Bernardi2016,
  author     = {Raffaella Bernardi and Ruket {\c{C}}akici and Desmond Elliott and Aykut Erdem and Erkut Erdem and Nazli Ikizler{-}Cinbis and Frank Keller and Adrian Muscat and Barbara Plank},
  journal    = {CoRR},
  title      = {Automatic Description Generation from Images: {A} Survey of Models, Datasets, and Evaluation Measures},
  year       = {2016},
  volume     = {abs/1601.03896},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/BernardiCEEEIKM16.bib},
  eprint     = {1601.03896},
  eprinttype = {arXiv},
  file       = {:PDF/1601.03896.pdf:PDF},
  groups     = {review},
  timestamp  = {Mon, 13 Aug 2018 16:47:08 +0200},
  url        = {http://arxiv.org/abs/1601.03896},
}

@InProceedings{barraco2022camel,
  author    = {Barraco, Manuele and Stefanini, Matteo and Cornia, Marcella and Cascianelli, Silvia and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle = {International Conference on Pattern Recognition},
  title     = {{CaMEL: Mean Teacher Learning for Image Captioning}},
  year      = {2022},
  file      = {:PDF/2202.10492.pdf:PDF},
}

@Article{Holtzman2019,
  author     = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  journal    = {CoRR},
  title      = {The Curious Case of Neural Text Degeneration},
  year       = {2019},
  volume     = {abs/1904.09751},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1904-09751.bib},
  eprint     = {1904.09751},
  eprinttype = {arXiv},
  file       = {:PDF/1904.09751.pdf:PDF},
  timestamp  = {Sat, 29 Apr 2023 10:09:28 +0200},
  url        = {http://arxiv.org/abs/1904.09751},
}

@InProceedings{liu2021Swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  year      = {2021},
  file      = {:PDF/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf:PDF},
}

@Article{Khan2022,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal    = {ACM Comput. Surv.},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issn       = {0360-0300},
  month      = {sep},
  number     = {10s},
  volume     = {54},
  abstract   = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  address    = {New York, NY, USA},
  articleno  = {200},
  comment    = {https://github.com/lucidrains/x-transformers},
  doi        = {10.1145/3505244},
  file       = {:PDF/2101.01169.pdf:PDF},
  issue_date = {January 2022},
  keywords   = {deep neural networks, bidirectional encoders, transformers, self-supervision, convolutional networks, literature survey, Self-attention},
  numpages   = {41},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3505244},
}

@Misc{Liu2021cptr,
  author        = {Wei Liu and Sihan Chen and Longteng Guo and Xinxin Zhu1 and Jing Liu1},
  title         = {CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2101.10804},
  file          = {:PDF/cptr-upload.pdf:PDF},
  groups        = {Vision Transformer.},
  primaryclass  = {cs.CV},
}

@Article{DHassani,
  author     = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},
  journal    = {CoRR},
  title      = {Escaping the Big Data Paradigm with Compact Transformers},
  year       = {2021},
  volume     = {abs/2104.05704},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-05704.bib},
  comment    = {https://github.com/lucidrains/vit-pytorch},
  eprint     = {2104.05704},
  eprinttype = {arXiv},
  file       = {:PDF/2104.05704.pdf:PDF},
  timestamp  = {Mon, 07 Nov 2022 08:34:14 +0100},
  url        = {https://arxiv.org/abs/2104.05704},
}

@Article{Yang2022HumanCentric,
  author     = {Yang, Zuopeng and Wang, Pengbo and Chu, Tianshu and Yang, Jie},
  journal    = {Pattern Recogn.},
  title      = {Human-Centric Image Captioning},
  year       = {2022},
  issn       = {0031-3203},
  month      = {jun},
  number     = {C},
  volume     = {126},
  address    = {USA},
  doi        = {10.1016/j.patcog.2022.108545},
  file       = {:PDF/1-s2.0-S0031320322000267-main.pdf:PDF},
  issue_date = {Jun 2022},
  keywords   = {Feature hierarchization, Image captioning, Human-centric},
  numpages   = {11},
  publisher  = {Elsevier Science Inc.},
  url        = {https://doi.org/10.1016/j.patcog.2022.108545},
}

@Misc{Roy2020efficient,
  author  = {Aurko Roy* and Mohammad Taghi Saffar* and David Grangier and Ashish Vaswani},
  title   = {Efficient Content-Based Sparse Attention with Routing Transformers},
  year    = {2020},
  comment = {https://github.com/lucidrains/routing-transformer},
  file    = {:PDF/2003.05997.pdf:PDF},
  url     = {https://arxiv.org/pdf/2003.05997.pdf},
}

@Article{kossen2021self,
  author  = {Kossen, Jannik and Band, Neil and Gomez, Aidan N. and Lyle, Clare and Rainforth, Tom and Gal, Yarin},
  journal = {arXiv:2106.02584},
  title   = {Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning},
  year    = {2021},
  comment = {https://github.com/OATML/non-parametric-transformers},
  file    = {:PDF/2106.02584.pdf:PDF},
}

@Misc{merullo2023linearly,
  author        = {Jack Merullo and Louis Castricato and Carsten Eickhoff and Ellie Pavlick},
  title         = {Linearly Mapping from Image to Text Space},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {https://github.com/jmerullo/limber},
  eprint        = {2209.15162},
  file          = {:PDF/5799_linearly_mapping_from_image_to.pdf:PDF},
  primaryclass  = {cs.CL},
}

@InProceedings{sung2022vladapter,
  author    = {Yi-Lin Sung, Jaemin Cho, Mohit Bansal},
  booktitle = {CVPR},
  title     = {VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
  year      = {2022},
  comment   = {https://github.com/ylsung/VL_adapter},
  file      = {:PDF/Welcome to Nextcloud Hub.pdf:PDF},
}

@InProceedings{cho2021vlt5,
  author    = {Jaemin Cho and Jie Lei and Hao Tan and Mohit Bansal},
  booktitle = {ICML},
  title     = {Unifying Vision-and-Language Tasks via Text Generation},
  year      = {2021},
  file      = {:PDF/2102.02779.pdf:PDF},
}

@Article{automatedNilesh,
  title = {Automated Image Captioning Using CNN and RNN},
  file  = {:PDF/IRJET-V8I12123.pdf:PDF},
}

@InProceedings{sundermeyer12_interspeech,
  author    = {Martin Sundermeyer and Ralf Schlüter and Hermann Ney},
  booktitle = {Proc. Interspeech 2012},
  title     = {{LSTM neural networks for language modeling}},
  year      = {2012},
  pages     = {194--197},
  doi       = {10.21437/Interspeech.2012-65},
  file      = {:PDF/sundermeyer12_interspeech.pdf:PDF},
}

@InProceedings{Schneider2017Regnet,
  author    = {Schneider, Nick and Piewak, Florian and Stiller, Christoph and Franke, Uwe},
  booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
  title     = {RegNet: Multimodal sensor registration using deep neural networks},
  year      = {2017},
  pages     = {1803-1810},
  doi       = {10.1109/IVS.2017.7995968},
  file      = {:PDF/2101.00590v1.pdf:PDF},
  keywords  = {Calibration;Feature extraction;Laser radar;Cameras;Sensor systems;Neural networks},
}

@Article{Zhang2021Vinvl,
  author  = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  journal = {CVPR 2021},
  title   = {VinVL: Making Visual Representations Matter in Vision-Language Models},
  year    = {2021},
  file    = {:PDF/2101.00529.pdf},
}

@InProceedings{Radford2021LearningTV,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {8748--8763},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  file      = {:PDF/2103.00020v1:00020v1},
  pdf       = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url       = {https://proceedings.mlr.press/v139/radford21a.html},
}

@InProceedings{chen2023pointgpt,
  author    = {Chen, Guangyan and Wang, Meiling and Yang, Yi and Yu, Kai and Yuan, Li and Yue, Yufeng},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  title     = {PointGPT: auto-regressively generative pre-training from point clouds},
  year      = {2024},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '23},
  abstract  = {Large language models (LLMs) based on the generative pre-training transformer (GPT) [46] have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decoder [27], with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. To explore scalability and enhance performance, a larger pre-training dataset is collected. Additionally, a subsequent post-pre-training stage is introduced, incorporating a labeled hybrid dataset. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9\% on the ModelNet40 dataset and 93.4\% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks. Codes are available at https://github.com/CGuangyan-BIT/PointGPT.},
  articleno = {1291},
  file      = {:PDF/2511_pointgpt_auto_regressively_gen.pdf:PDF},
  location  = {New Orleans, LA, USA},
  numpages  = {13},
  url       = {https://openreview.net/forum?id=rqE0fEQDqs},
}

@InProceedings{Xue_2024_CVPR,
  author    = {Xue, Le and Yu, Ning and Zhang, Shu and Panagopoulou, Artemis and Li, Junnan and Mart{\'\i}n-Mart{\'\i}n, Roberto and Wu, Jiajun and Xiong, Caiming and Xu, Ran and Niebles, Juan Carlos and Savarese, Silvio},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding},
  year      = {2024},
  month     = {June},
  pages     = {27091-27101},
  file      = {:PDF/Xue_ULIP-2_Towards_Scalable_Multimodal_Pre-training_for_3D_Understanding_CVPR_2024_paper.pdf:PDF},
}

@Article{Zhang2024PointGT,
  author   = {Zhang, Huang and Wang, Changshuo and Yu, Long and Tian, Shengwei and Ning, Xin and Rodrigues, Joel},
  journal  = {IEEE Transactions on Multimedia},
  title    = {PointGT: A Method for Point-Cloud Classification and Segmentation Based on Local Geometric Transformation},
  year     = {2024},
  pages    = {1-12},
  doi      = {10.1109/TMM.2024.3374580},
  file     = {:PDF/ssrn-4603211.pdf:PDF},
  keywords = {Point cloud compression;Feature extraction;Three-dimensional displays;Kernel;Bidirectional control;Robustness;Optimization;deep learning;3D point cloud classification and segmentation;local neighborhood;geometric transformation;attention mechanism},
}

@Article{Wang2022Discriminative,
  author   = {Wang, Changshuo and Ning, Xin and Sun, Linjun and Zhang, Liping and Li, Weijun and Bai, Xiao},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {Learning Discriminative Features by Covering Local Geometric Space for Point Cloud Analysis},
  year     = {2022},
  pages    = {1-15},
  volume   = {60},
  doi      = {10.1109/TGRS.2022.3170493},
  file     = {:PDF/2022-Learning_Discriminative_Features_by_Covering_Local_Geometric_Space_for_Point_Cloud_Analysis.pdf:PDF},
  keywords = {Point cloud compression;Feature extraction;Three-dimensional displays;Convolution;Shape;Geometry;Task analysis;Edge feature;high-order relationship;point cloud analysis;shape classification;space-cover convolutional neural network (SC-CNN)},
}

@Article{Wang2024PersonReIdentification,
  author   = {Wang, Changshuo and Ning, Xin and Li, Weijun and Bai, Xiao and Gao, Xingyu},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {3D Person Re-Identification Based on Global Semantic Guidance and Local Feature Aggregation},
  year     = {2024},
  number   = {6},
  pages    = {4698-4712},
  volume   = {34},
  doi      = {10.1109/TCSVT.2023.3328712},
  keywords = {Pedestrians;Three-dimensional displays;Point cloud compression;Feature extraction;Shape;Semantics;Geometry;Point cloud;3D shape representation;person re-identification;semantic guidance;local feature extraction},
}

@InProceedings{Radford2018ImprovingLU,
  author = {Alec Radford and Karthik Narasimhan},
  title  = {Improving Language Understanding by Generative Pre-Training},
  year   = {2018},
  file   = {:PDF/GPT-1.pdf:PDF},
  url    = {https://api.semanticscholar.org/CorpusID:49313245},
}

@InProceedings{Wiegreffepinter2019attention,
  author    = {Wiegreffe, Sarah and Pinter, Yuval},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Attention is not not Explanation},
  year      = {2019},
  address   = {Hong Kong, China},
  editor    = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  month     = nov,
  pages     = {11--20},
  publisher = {Association for Computational Linguistics},
  abstract  = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model{'}s prediction, and consequently reach insights regarding the model{'}s decision-making process. A recent paper claims that {`}Attention is not Explanation{'} (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one{'}s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don{'}t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  doi       = {10.18653/v1/D19-1002},
  file      = {:PDF/D19-1002.pdf:PDF},
  url       = {https://aclanthology.org/D19-1002},
}

@InBook{Lu2019VILBERT,
  author    = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  publisher = {Curran Associates Inc.},
  title     = {ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  year      = {2019},
  address   = {Red Hook, NY, USA},
  abstract  = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks – visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval – by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models – achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
  articleno = {2},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  file      = {:PDF/NeurIPS-2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks-Paper.pdf:PDF},
  numpages  = {11},
}

@InProceedings{Tanbansal2019lxmert,
  author    = {Tan, Hao and Bansal, Mohit},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers},
  year      = {2019},
  address   = {Hong Kong, China},
  editor    = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  month     = nov,
  pages     = {5100--5111},
  publisher = {Association for Computational Linguistics},
  abstract  = {Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: \url{https://github.com/airsplay/lxmert}},
  doi       = {10.18653/v1/D19-1514},
  file      = {:PDF/D19-1514.pdf:PDF},
  url       = {https://aclanthology.org/D19-1514},
}

@Article{Kingma2014AdamOptimizer,
  author    = {Kingma, Diederik and Ba, Jimmy},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  booktitle = {International Conference on Learning Representations (ICLR)},
  file      = {:PDF/1412.6980v9.pdf:PDF},
  optmonth  = {12},
}

@Article{Ding2023DeepLearningTechniques,
  author         = {Ding, Zifeng and Sun, Yuxuan and Xu, Sijin and Pan, Yan and Peng, Yanhong and Mao, Zebing},
  journal        = {Robotics},
  title          = {Recent Advances and Perspectives in Deep Learning Techniques for 3D Point Cloud Data Processing},
  year           = {2023},
  issn           = {2218-6581},
  number         = {4},
  volume         = {12},
  abstract       = {In recent years, deep learning techniques for processing 3D point cloud data have seen significant advancements, given their unique ability to extract relevant features and handle unstructured data. These techniques find wide-ranging applications in fields like robotics, autonomous vehicles, and various other computer-vision applications. This paper reviews the recent literature on key tasks, including 3D object classification, tracking, pose estimation, segmentation, and point cloud completion. The review discusses the historical development of these methods, explores different model architectures, learning algorithms, and training datasets, and provides a comprehensive summary of the state-of-the-art in this domain. The paper presents a critical evaluation of the current limitations and challenges in the field, and identifies potential areas for future research. Furthermore, the emergence of transformative methodologies like PoinTr and SnowflakeNet is examined, highlighting their contributions and potential impact on the field. The potential cross-disciplinary applications of these techniques are also discussed, underscoring the broad scope and impact of these developments. This review fills a knowledge gap by offering a focused and comprehensive synthesis of recent research on deep learning techniques for 3D point cloud data processing, thereby serving as a useful resource for both novice and experienced researchers in the field.},
  article-number = {100},
  doi            = {10.3390/robotics12040100},
  file           = {:PDF/robotics-12-00100.pdf:PDF},
  url            = {https://www.mdpi.com/2218-6581/12/4/100},
}

@Article{Wang2022AdaptiveIncremental,
  author     = {Wang, Changzhi and Gu, Xiaodong},
  journal    = {Applied Intelligence},
  title      = {Image captioning with adaptive incremental global context attention},
  year       = {2022},
  issn       = {0924-669X},
  month      = {apr},
  number     = {6},
  pages      = {6575–6597},
  volume     = {52},
  abstract   = {The encoder-decoder framework has proliferated in current image captioning task, where the decoder generates target description word by word based on the preceding captions. However, this framework encounters two main concerns. Firstly, the decoder cannot adequately capture global dependencies between the current predicted target word and all the previously generated words. Secondly, some generated words (e.g., “on”, “the” and “of”) provide insufficient information, which may deviate from the sentence semantics during gradually generating captions. To address above concerns, in this paper we propose a novel adaptive incremental global context attention (IGCA) method to capture the global information between target words, thus enhancing target word predictions in image captioning. Specifically, all of previous historical decoder hidden states are utilized as the global feature to guide the generation of subsequent word. During the generation procedure, the proposed IGCA mechanism is able to dynamically focus on these text features that are most correlated with the currently generated word. To verify the efficiency of our IGCA model, we conducted extensive experiments on the three public benchmark datasets. The experimental results demonstrate that the proposed model brings significant improvement over the conventional attention-based encoder-decoder methods and achieves state-of-the-art performance on Flick 30k and Flick 8k datasets.},
  address    = {USA},
  doi        = {10.1007/s10489-021-02734-3},
  file       = {:PDF/s10489-021-02734-3.pdf:PDF},
  issue_date = {Apr 2022},
  keywords   = {Image captioning, Attention mechanism, Incremental global context attention, LSTM},
  numpages   = {23},
  publisher  = {Kluwer Academic Publishers},
  url        = {https://doi.org/10.1007/s10489-021-02734-3},
}

@Article{Tang2021AttentionGuided,
  author         = {Tang, Ziwei and Yi, Yaohua and Sheng, Hao},
  journal        = {Sensors},
  title          = {Attention-Guided Image Captioning through Word Information},
  year           = {2021},
  issn           = {1424-8220},
  number         = {23},
  volume         = {21},
  abstract       = {Image captioning generates written descriptions of an image. In recent image captioning research, attention regions seldom cover all objects, and generated captions may lack the details of objects and may remain far from reality. In this paper, we propose a word guided attention (WGA) method for image captioning. First, WGA extracts word information using the embedded word and memory cell by applying transformation and multiplication. Then, WGA applies word information to the attention results and obtains the attended feature vectors via elementwise multiplication. Finally, we apply WGA with the words from different time steps to obtain previous word guided attention (PW) and current word attention (CW) in the decoder. Experiments on the MSCOCO dataset show that our proposed WGA can achieve competitive performance against state-of-the-art methods, with PW results of a 39.1 Bilingual Evaluation Understudy score (BLEU-4) and a 127.6 Consensus-Based Image Description Evaluation score (CIDEr-D); and CW results of a 39.1 BLEU-4 score and a 127.2 CIDER-D score on a Karpathy test split.},
  article-number = {7982},
  doi            = {10.3390/s21237982},
  file           = {:PDF/sensors-21-07982-1.pdf:PDF},
  pubmedid       = {34883986},
  url            = {https://www.mdpi.com/1424-8220/21/23/7982},
}

@Article{Deng2020DensenetAdaptive,
  author   = {Zhenrong Deng and Zhouqin Jiang and Rushi Lan and Wenming Huang and Xiaonan Luo},
  journal  = {Signal Processing: Image Communication},
  title    = {Image captioning using DenseNet network and adaptive attention},
  year     = {2020},
  issn     = {0923-5965},
  pages    = {115836},
  volume   = {85},
  abstract = {Considering the image captioning problem, it is difficult to correctly extract the global features of the images. At the same time, most attention methods force each word to correspond to the image region, ignoring the phenomenon that words such as “the” in the description text cannot correspond to the image region. To address these problems, an adaptive attention model with a visual sentinel is proposed in this paper. In the encoding phase, the model introduces DenseNet to extract the global features of the image. At the same time, on each time axis, the sentinel gate is set by the adaptive attention mechanism to decide whether to use the image feature information for word generation. In the decoding phase, the long short-term memory (LSTM) network is applied as a language generation model for image captioning tasks to improve the quality of image caption generation. Experiments on the Flickr30k and COCO datasets indicate that the proposed model exhibits significant improvement in terms ofthe BLEU and METEOR evaluation criteria.},
  doi      = {https://doi.org/10.1016/j.image.2020.115836},
  file     = {:PDF/1-s2.0-S092359652030059X-main.pdf:PDF},
  keywords = {Image captioning, DenseNet, LSTM, Adaptive attention mechanism},
  url      = {https://www.sciencedirect.com/science/article/pii/S092359652030059X},
}

@InProceedings{Huang2019adaptively,
  author    = {Huang, Lun and Wang, Wenmin and Xia, Yaxian and Chen, Jie},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Adaptively Aligned Image Captioning via Adaptive Attention Time},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  comment   = {https://github.com/husthuaan/AAT},
  file      = {:PDF/NeurIPS-2019-adaptively-aligned-image-captioning-via-adaptive-attention-time-Paper.pdf:PDF},
  groups    = {Attention Over Visual Regions, Two-layer LSTM},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/fecc3a370a23d13b1cf91ac3c1e1ca92-Paper.pdf},
}

@InProceedings{Sow2019SequentialGuiding,
  author    = {Sow, Daouda and Qin, Zengchang and Niasse, Mouhamed and Wan, Tao},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {A Sequential Guiding Network with Attention for Image Captioning},
  year      = {2019},
  pages     = {3802-3806},
  doi       = {10.1109/ICASSP.2019.8682505},
  file      = {:PDF/A_Sequential_Guiding_Network_with_Attention_for_Image_Captioning.pdf:PDF},
  keywords  = {Semantics;Decoding;Task analysis;Adaptation models;Computational modeling;Visualization;Recurrent neural networks},
}

@InProceedings{Jiang2018LearningToGuide,
  author    = {Wenhao Jiang and Lin Ma and Xinpeng Chen and Hanwang Zhang and Wei Liu},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018},
  title     = {Learning to Guide Decoding for Image Captioning},
  year      = {2018},
  editor    = {Sheila A. McIlraith and Kilian Q. Weinberger},
  pages     = {6959--6966},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/JiangMCZL18.bib},
  doi       = {10.1609/AAAI.V32I1.12283},
  file      = {:PDF/12283-13-15811-1-2-20201228.pdf:PDF},
  timestamp = {Mon, 04 Sep 2023 16:50:25 +0200},
  url       = {https://doi.org/10.1609/aaai.v32i1.12283},
}

@InProceedings{Luong2015Effective,
  author    = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  year      = {2015},
  address   = {Lisbon, Portugal},
  editor    = {M{\`a}rquez, Llu{\'\i}s and Callison-Burch, Chris and Su, Jian},
  month     = sep,
  pages     = {1412--1421},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D15-1166},
  file      = {:PDF/D15-1166.pdf:PDF},
  groups    = {attention taxonomy},
  url       = {https://aclanthology.org/D15-1166},
}

@Article{Osman2023Survey,
  author  = {Osman, Asmaa and Wahby Shalaby, Mohamed and Soliman, Mona and Elsayed, Khaled},
  journal = {International Journal of Advanced Computer Science and Applications},
  title   = {A Survey on Attention-Based Models for Image Captioning},
  year    = {2023},
  month   = {01},
  volume  = {14},
  doi     = {10.14569/IJACSA.2023.0140249},
  file    = {:PDF/Paper_49-A_Survey_on_Attention_Based_Models_for_Image_Captioning-withNUaff.pdf:PDF},
  groups  = {review},
}

@Article{Al2024Review,
  author  = {Al-Shamayleh, Ahmad and Adwan, Omar and Alsharaiah, Mohammad and Hussein, Abdelrahman and Kharma, Qasem and Eke, Christopher},
  journal = {Multimedia Tools and Applications},
  title   = {A comprehensive literature review on image captioning methods and metrics based on deep learning technique},
  year    = {2024},
  month   = {02},
  pages   = {1-50},
  volume  = {83},
  doi     = {10.1007/s11042-024-18307-8},
  file    = {:PDF/s11042-024-18307-8-1.pdf:PDF},
  groups  = {review},
}

@Article{Sharma2024Survey,
  author  = {Sharma, Himanshu and Padha, Devanand},
  journal = {Artificial Intelligence Review},
  title   = {A comprehensive survey on image captioning: from handcrafted to deep learning-based techniques, a taxonomy and open research issues},
  year    = {2023},
  month   = {04},
  pages   = {1-43},
  volume  = {56},
  doi     = {10.1007/s10462-023-10488-2},
  file    = {:PDF/s10462-023-10488-2.pdf:PDF},
  groups  = {review},
}

@Article{Liu2020ChineseIC,
  author  = {Maofu Liu and Huijun Hu and Lingjun Li and Yan Yu and Weili Guan},
  journal = {IEEE Transactions on Cybernetics},
  title   = {Chinese Image Caption Generation via Visual Attention and Topic Modeling},
  year    = {2020},
  pages   = {1247-1257},
  volume  = {52},
  file    = {:PDF/Chinese_Image_Caption_Generation_via_Visual_Attention_and_Topic_Modeling.pdf:PDF},
  url     = {https://api.semanticscholar.org/CorpusID:219986264},
}

@Article{Chen2021MemorizedKnowledge,
  author    = {Hui Chen and Guiguang Ding and Zijia Lin and Yuchen Guo and Caifeng Shan and Jungong Han},
  journal   = {Cognitive Computation},
  title     = {Image captioning with memorized knowledge},
  year      = {2021},
  issn      = {1866-9956},
  month     = jul,
  number    = {4},
  pages     = {807--820},
  volume    = {13},
  abstract  = {Image captioning, which aims to automatically generate text description of given images, has received much attention from researchers. Most existing approaches adopt a recurrent neural network (RNN) as a decoder to generate captions conditioned on the input image information. However, traditional RNNs deal with the sequence in a recurrent way, squeezing the information of all previous words into hidden cells and updating the context information by fusing the hidden states with the current word information. This may miss the rich knowledge too far in the past. In this paper, we propose a memory-enhanced captioning model for image captioning. We firstly introduce an external memory to store the past knowledge, i.e., all the information of generated words. When predicting the next word, the decoder can retrieve knowledge information about the past by means of a selective reading mechanism. Furthermore, to better explore the knowledge stored in the memory, we introduce several variants that consider different types of past knowledge. To verify the effectiveness of the proposed model, we conduct extensive experiments and comparisons on the well-known image captioning dataset MS COCO. Compared with the state-of-the-art captioning models, the proposed memory-enhanced captioning model shows a significant improvement in terms of the performance (improving 3.5% in terms of CIDEr). The proposed memory-enhanced captioning model, as demonstrated in the experiments, is more effective and superior to the state-of-the-art methods.},
  doi       = {10.1007/s12559-019-09656-w},
  keywords  = {Attention, Encoder-decoder, Image captioning, Memory},
  language  = {English},
  publisher = {Springer},
}

@Article{Li2018GLA,
  author   = {Li, Linghui and Tang, Sheng and Zhang, Yongdong and Deng, Lixi and Tian, Qi},
  journal  = {IEEE Transactions on Multimedia},
  title    = {GLA: Global–Local Attention for Image Description},
  year     = {2018},
  number   = {3},
  pages    = {726-737},
  volume   = {20},
  doi      = {10.1109/TMM.2017.2751140},
  file     = {:PDF/GLA_GlobalLocal_Attention_for_Image_Description.pdf:PDF},
  groups   = {attention taxonomy},
  keywords = {Recurrent neural networks;Decoding;Image recognition;Feature extraction;Natural language processing;Computational modeling;Convolutional neural network;recurrent neural network;image description;natural language processing},
}

@Article{Xia2020Boosting,
  author     = {Xia, Pengfei and He, Jingsong and Yin, Jin},
  journal    = {Multimedia Tools Appl.},
  title      = {Boosting image caption generation with feature fusion module},
  year       = {2020},
  issn       = {1380-7501},
  month      = {sep},
  number     = {33–34},
  pages      = {24225–24239},
  volume     = {79},
  abstract   = {Image caption generation has been considered as a key issue on vision-to-language tasks. Using the classification model, such as AlexNet, VGG and ResNet as the encoder to extract image features is very common in previous work. However, there is an explicit gap in image feature requirements between caption task and classification task, and has not been widely concerned. In this paper, we propose a novel custom structure, named feature fusion module (FFM), to make the features extracted by the encoder more suitable for caption task. We evaluate the proposed module with two typical models, NIC (Neural Image Caption) and SA (Soft Attention), on two popular benchmarks, MS COCO and Flickr30k. It is consistently observed that FFM is able to boost the performance, and outperforms state-of-the-art methods over five metrics.},
  address    = {USA},
  doi        = {10.1007/s11042-020-09110-2},
  file       = {:PDF/s11042-020-09110-2.pdf:PDF},
  issue_date = {Sep 2020},
  keywords   = {Encoder-decoder model, Feature fusion module, Image caption},
  numpages   = {15},
  publisher  = {Kluwer Academic Publishers},
  url        = {https://doi.org/10.1007/s11042-020-09110-2},
}

@InProceedings{Wang2017Skeleton,
  author    = {Wang, Yufei and Lin, Zhe and Shen, Xiaohui and Cohen, Scott and Cottrell, Garrison W.},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition},
  year      = {2017},
  month     = {July},
  pages     = {7378-7387},
  abstract  = {Recently, there has been a lot of interest in automatically generating descriptions for an image. Most existing language-model based approaches for this task learn to generate an image description word by word in its original word order. However, for humans, it is more natural to locate the objects and their relationships first, and then elaborate on each object, describing notable attributes. We present a coarse-to-fine method that decomposes the original image description into a skeleton sentence and its attributes, and generates the skeleton sentence and attribute phrases separately. By this decomposition, our method can generate more accurate and novel descriptions than the previous state-of-the-art. Experimental results on the MS-COCO and a larger scale Stock3M datasets show that our algorithm yields consistent improvements across different evaluation metrics, especially on the SPICE metric, which has much higher correlation with human ratings than the conventional metrics. Furthermore, our algorithm can generate descriptions with varied length, benefiting from the separate control of the skeleton and attributes. This enables image description generation that better accommodates user preferences.},
  doi       = {10.1109/CVPR.2017.780},
  file      = {:PDF/Wang_Skeleton_Key_Image_CVPR_2017_paper.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Skeleton;Measurement;Training;SPICE;Semantics;Recurrent neural networks},
}

@Article{DING2020520,
  author   = {Songtao Ding and Shiru Qu and Yuling Xi and Shaohua Wan},
  journal  = {Neurocomputing},
  title    = {Stimulus-driven and concept-driven analysis for image caption generation},
  year     = {2020},
  issn     = {0925-2312},
  pages    = {520-530},
  volume   = {398},
  abstract = {Recently, image captioning has achieved great progress in computer vision and artificial intelligence. However, language models still failed to achieve the desired results in high-level visual tasks. Generating accurate image captions for a complex scene that contains multiple targets is a challenge. To solve these problems, we introduce the theory of attention in psychology to image caption generation. We propose two types of attention mechanisms: The stimulus-driven and the concept-driven. Our attention model relies on a combination of convolutional neural network (CNN) over images and long-short term memory (LSTM) network over sentences. Comparison of experimental results illustrates that our proposed method achieves good performance on the MSCOCO test server.},
  doi      = {https://doi.org/10.1016/j.neucom.2019.04.095},
  file     = {:PDF/1-s2.0-S0925231219310367-main.pdf:PDF},
  keywords = {Image captioning, Stimulus-driven, Concept-driven, Attention mechanism, LSTM},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231219310367},
}

@InProceedings{You2016Semantic,
  author    = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Image Captioning with Semantic Attention},
  year      = {2016},
  pages     = {4651-4659},
  doi       = {10.1109/CVPR.2016.503},
  file      = {:PDF/You_Image_Captioning_With_CVPR_2016_paper.pdf:PDF},
  groups    = {global CNN features, attention taxonomy},
  keywords  = {Visualization;Semantics;Recurrent neural networks;Feature extraction;Natural languages;Computer vision},
}

@Article{Gao2019Deliberate,
  author       = {Gao, Lianli and Fan, Kaixuan and Song, Jingkuan and Liu, Xianglong and Xu, Xing and Shen, Heng Tao},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Deliberate Attention Networks for Image Captioning},
  year         = {2019},
  month        = {Jul.},
  number       = {01},
  pages        = {8320-8327},
  volume       = {33},
  abstractnote = {In daily life, deliberation is a common behavior for human to improve or refine their work (e.g., writing, reading and drawing). To date, encoder-decoder framework with attention mechanisms has achieved great progress for image captioning. However, such framework is in essential an one-pass forward process while encoding to hidden states and attending to visual features, but lacks of the deliberation action. The learned hidden states and visual attention are directly used to predict the final captions without further polishing. In this paper, we present a novel Deliberate Residual Attention Network, namely DA, for image captioning. The first-pass residual-based attention layer prepares the hidden states and visual attention for generating a preliminary version of the captions, while the second-pass deliberate residual-based attention layer refines them. Since the second-pass is based on the rough global features captured by the hidden layer and visual attention in the first-pass, our DA has the potential to generate better sentences. We further equip our DA with discriminative loss and reinforcement learning to disambiguate image/caption pairs and reduce exposure bias. Our model improves the state-of-the-arts on the MSCOCO dataset and reaches 37.5% BELU-4, 28.5% METEOR and 125.6% CIDEr. It also outperforms the-state-ofthe-arts from 25.1% BLEU-4, 20.4% METEOR and 53.1% CIDEr to 29.4% BLEU-4, 23.0% METEOR and 66.6% on the Flickr30K dataset.},
  doi          = {10.1609/aaai.v33i01.33018320},
  file         = {:PDF/4845-Article Text-7911-1-10-20190709.pdf:PDF},
  groups       = {attention taxonomy},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4845},
}

@InProceedings{Pedersoli2017Areas,
  author    = {Pedersoli, Marco and Lucas, Thomas and Schmid, Cordelia and Verbeek, Jakob},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  title     = {Areas of attention for image captioning},
  year      = {2017},
  pages     = {1242--1250},
  file      = {:PDF/Pedersoli_Areas_of_Attention_ICCV_2017_paper.pdf:PDF},
  groups    = {single layer LSTM, Geometric Transforms, attention taxonomy},
}

@InProceedings{Guo2020Normalized,
  author    = {Guo, Longteng and Liu, Jing and Zhu, Xinxin and Yao, Peng and Lu, Shichen and Lu, Hanqing},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  title     = {Normalized and geometry-aware self-attention network for image captioning},
  year      = {2020},
  pages     = {10327--10336},
  file      = {:PDF/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf:PDF},
  groups    = {Self-Attention Encoding, Transformer, attention taxonomy},
}

@Article{LIU2020102178,
  author   = {Maofu Liu and Lingjun Li and Huijun Hu and Weili Guan and Jing Tian},
  journal  = {Information Processing \& Management},
  title    = {Image caption generation with dual attention mechanism},
  year     = {2020},
  issn     = {0306-4573},
  number   = {2},
  pages    = {102178},
  volume   = {57},
  abstract = {As a crossing domain of computer vision and natural language processing, the image caption generation has been an active research topic in recent years, which contributes to the multimodal social media translation from unstructured image data to structured text data. The conventional research works have proposed a series of image captioning methods, such as template-based, retrieval-based, encode-decode. Among these methods, the one with encode-decode framework is widely used in the image caption generation, in which the encoder extracts the image features by Convolutional Neural Network (CNN), and the decoder adopts Recurrent Neural Network (RNN) to generate the image description. The Neural Image Caption (NIC) model has achieved good performance in image captioning, and however, there still remains some challenges to be addressed. To tackle the challenges of the lack of image information and the deviation from the core content of the image, our proposed model explores visual attention to deepen the understanding of the image, incorporating the image labels generated by Fully Convolutional Network (FCN) into the generation of image caption. Furthermore, our proposed model exploits textual attention to increase the integrity of the information. Finally, the label generation, attached to the textual attention mechanism, and the image caption generation, have been merged to form an end-to-end trainable framework. In this paper, extensive experiments have been carried out on the AIC-ICC image caption benchmark dataset, and the experimental results show that our proposed model is effective and feasible in the image caption generation.},
  doi      = {https://doi.org/10.1016/j.ipm.2019.102178},
  file     = {:PDF/1-s2.0-S0306457319307885-main.pdf:PDF},
  groups   = {attention taxonomy},
  keywords = {Image caption generation, Textual attention, Visual attention, Dual attention, Fully convolutional network},
  url      = {https://www.sciencedirect.com/science/article/pii/S0306457319307885},
}

@Article{WANG2020107075,
  author   = {Junbo Wang and Wei Wang and Liang Wang and Zhiyong Wang and David Dagan Feng and Tieniu Tan},
  journal  = {Pattern Recognition},
  title    = {Learning visual relationship and context-aware attention for image captioning},
  year     = {2020},
  issn     = {0031-3203},
  pages    = {107075},
  volume   = {98},
  abstract = {Image captioning which automatically generates natural language descriptions for images has attracted lots of research attentions and there have been substantial progresses with attention based captioning methods. However, most attention-based image captioning methods focus on extracting visual information in regions of interest for sentence generation and usually ignore the relational reasoning among those regions of interest in an image. Moreover, these methods do not take into account previously attended regions which can be used to guide the subsequent attention selection. In this paper, we propose a novel method to implicitly model the relationship among regions of interest in an image with a graph neural network, as well as a novel context-aware attention mechanism to guide attention selection by fully memorizing previously attended visual content. Compared with the existing attention-based image captioning methods, ours can not only learn relation-aware visual representations for image captioning, but also consider historical context information on previous attention. We perform extensive experiments on two public benchmark datasets: MS COCO and Flickr30K, and the experimental results indicate that our proposed method is able to outperform various state-of-the-art methods in terms of the widely used evaluation metrics.},
  doi      = {https://doi.org/10.1016/j.patcog.2019.107075},
  file     = {:PDF/1-s2.0-S0031320319303760-main.pdf:PDF},
  keywords = {Image captioning, Relational reasoning, Context-aware attention},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320319303760},
}

@InProceedings{Chen2018Regularizing,
  author    = {X. Chen and L. Ma and W. Jiang and J. Yao and W. Liu},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Regularizing RNNs for Caption Generation by Reconstructing the Past with the Present},
  year      = {2018},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {7995-8003},
  publisher = {IEEE Computer Society},
  abstract  = {Recently, caption generation with an encoder-decoder framework has been extensively studied and applied in different domains, such as image captioning, code captioning, and so on. In this paper, we propose a novel architecture, namely Auto-Reconstructor Network (ARNet), which, coupling with the conventional encoder-decoder framework, works in an end-to-end fashion to generate captions. ARNet aims at reconstructing the previous hidden state with the present one, besides behaving as the input-dependent transition operator. Therefore, ARNet encourages the current hidden state to embed more information from the previous one, which can help regularize the transition dynamics of recurrent neural networks (RNNs). Extensive experimental results show that our proposed ARNet boosts the performance over the existing encoder-decoder models on both image captioning and source code captioning tasks. Additionally, ARNet remarkably reduces the discrepancy between training and inference processes for caption generation. Furthermore, the performance on permuted sequential MNIST demonstrates that ARNet can effectively regularize RNN, especially on modeling long-term dependencies. Our code is available at: https://github.com/chenxinpeng/ARNet.},
  comment   = {https://github.com/chenxinpeng/ARNet},
  doi       = {10.1109/CVPR.2018.00834},
  file      = {:PDF/Chen_Regularizing_RNNs_for_CVPR_2018_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features, hidden state reconstruction},
  keywords  = {computer vision;pattern recognition},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00834},
}

@InProceedings{Ke2019Reflective,
  author    = {Ke, Lei and Pei, Wenjie and Li, Ruiyu and Shen, Xiaoyong and Tai, Yu-Wing},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Reflective Decoding Network for Image Captioning},
  year      = {2019},
  pages     = {8887-8896},
  doi       = {10.1109/ICCV.2019.00898},
  file      = {:PDF/Reflective_Decoding_Network_for_Image_Captioning.pdf:PDF},
  groups    = {Attention Over Visual Regions, Reflective attention},
  keywords  = {Decoding;Visualization;Feature extraction;Syntactics;Task analysis;Rivers;Random access memory},
}

@Article{Xiao2019DAA,
  author   = {Fen Xiao and Xue Gong and Yiming Zhang and Yanqing Shen and Jun Li and Xieping Gao},
  journal  = {Neurocomputing},
  title    = {DAA: Dual LSTMs with adaptive attention for image captioning},
  year     = {2019},
  issn     = {0925-2312},
  pages    = {322-329},
  volume   = {364},
  abstract = {Image captioning enables people to better understand images through fine-grained analysis. Recently the encoder-decoder architecture with attention mechanism has achieved great achievements in image captioning and visual question answering. In this paper, we propose a new captioning algorithm that integrates two separate LSTM (Long-short Term Memory) networks through an adaptive semantic attention model. Within our approach, the first LSTM network is followed by an attention model, which serves as a visual sentinel can flexibly make a trade off between the visual semantic region and textual content. Another LSTM is used as a language model, which combines the hidden state representation of the first LSTM and attention context vector, then outputs the word sequence. The proposed model has been extensively evaluated on two large-scale datasets: MSCOCO and Flickr30k. Experimental results show that the proposed method pays more attention to visual salient regions and achieves significant performance of prior state-of-the-art approaches on multiple evaluation metrics.},
  doi      = {https://doi.org/10.1016/j.neucom.2019.06.085},
  file     = {:PDF/main.pdf:PDF},
  keywords = {Image captioning, Adaptive attention, Convolutional neural network, Long-short term memory},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231219309993},
}

@Article{Zhang2019VAA,
  author   = {Zhang, Zhengyuan and Zhang, Wenkai and Diao, Wenhui and Yan, Menglong and Gao, Xin and Sun, Xian},
  journal  = {IEEE Access},
  title    = {VAA: Visual Aligning Attention Model for Remote Sensing Image Captioning},
  year     = {2019},
  pages    = {137355-137364},
  volume   = {7},
  doi      = {10.1109/ACCESS.2019.2942154},
  file     = {:PDF/VAA_Visual_Aligning_Attention_Model_for_Remote_Sensing_Image_Captioning.pdf:PDF;:PDF/VAA_Visual_Aligning_Attention_Model_for_Remote_Sensing_Image_Captioning.pdf:PDF},
  keywords = {Visualization;Remote sensing;Training;Task analysis;Feature extraction;Decoding;Solid modeling;Image captioning;remote sensing image captioning;attention mechanism;visual aligning},
}

@Article{Zhang2019HQ,
  author   = {Zhang, Zongjian and Wu, Qiang and Wang, Yang and Chen, Fang},
  journal  = {IEEE Transactions on Multimedia},
  title    = {High-Quality Image Captioning With Fine-Grained and Semantic-Guided Visual Attention},
  year     = {2019},
  number   = {7},
  pages    = {1681-1693},
  volume   = {21},
  doi      = {10.1109/TMM.2018.2888822},
  file     = {:PDF/High-Quality_Image_Captioning_With_Fine-Grained_and_Semantic-Guided_Visual_Attention.pdf:PDF},
  keywords = {Visualization;Semantics;Feature extraction;Decoding;Task analysis;Object oriented modeling;Image resolution;Image captioning;attention mechanism;fine-grained resolution;semantic guidance;fully convolutional network-long short term memory framework},
}

@Article{ZHU2018TripleAttention,
  author   = {Xinxin Zhu and Lixiang Li and Jing Liu and Ziyi Li and Haipeng Peng and Xinxin Niu},
  journal  = {Neurocomputing},
  title    = {Image captioning with triple-attention and stack parallel LSTM},
  year     = {2018},
  issn     = {0925-2312},
  pages    = {55-65},
  volume   = {319},
  abstract = {Image captioning aims to describe the content of images with a sentence. It is a natural way for people to express their understanding, but a challenging and important task from the view of image understanding. In this paper, we propose two innovations to improve the performance of such a sequence learning problem. First, we give a new attention method named triple attention (TA-LSTM) which can leverage the image context information at every stage of LSTM. Then, we redesign the structure of basic LSTM, in which not only the stacked LSTM but also the paralleled LSTM are adopted, called as PS-LSTM. In this structure, we not only use the stack LSTM but also use the parallel LSTM to achieve the improvement of the performance compared with the normal LSTM. Through this structure, the proposed model can ensemble more parameters on single model and has ensemble ability itself. Through numerical experiments, on the public available MSCOCO dataset, our final TA-PS-LSTM model achieves comparable performance with some state-of-the-art methods.},
  doi      = {https://doi.org/10.1016/j.neucom.2018.08.069},
  file     = {:PDF/1-s2.0-S0925231218310324-main.pdf:PDF;:PDF/1-s2.0-S0925231218310324-main.pdf:PDF},
  keywords = {Image caption, Deep learning, LSTM, CNN, Attention},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231218310324},
}

@Misc{Liu2022Prophet,
  author = {Liu, Fenglin and Ma, Xuewei and Ren, Xuancheng and Wu, Xian and Fan, Wei and Zou, Yuexian and Sun, Xu},
  month  = {10},
  title  = {Prophet Attention: Predicting Attention with Future Attention for Improved Image Captioning},
  year   = {2022},
  doi    = {10.48550/arXiv.2210.10914},
  file   = {:PDF/NeurIPS-2020-prophet-attention-predicting-attention-with-future-attention-Paper.pdf:PDF},
  groups = {Boosting LSTM with Self-Attention},
}

@Misc{zhu2021autocaptionimagecaptioningneural,
  author        = {Xinxin Zhu and Weining Wang and Longteng Guo and Jing Liu},
  title         = {AutoCaption: Image Captioning with Neural Architecture Search},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2012.09742},
  file          = {:PDF/2012.09742v3.pdf:PDF},
  groups        = {Neural Architecture Search for RNN, Boosting LSTM with Self-Attention},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2012.09742},
}

@Article{Wang2020ShowRecall,
  author       = {Wang, Li and Bai, Zechen and Zhang, Yonghua and Lu, Hongtao},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Show, Recall, and Tell: Image Captioning with Recall Mechanism},
  year         = {2020},
  month        = {Apr.},
  number       = {07},
  pages        = {12176-12183},
  volume       = {34},
  abstractnote = {Generating natural and accurate descriptions in image captioning has always been a challenge. In this paper, we propose a novel recall mechanism to imitate the way human conduct captioning. There are three parts in our recall mechanism : recall unit, semantic guide (SG) and recalled-word slot (RWS). Recall unit is a text-retrieval module designed to retrieve recalled words for images. SG and RWS are designed for the best use of recalled words. SG branch can generate a recalled context, which can guide the process of generating caption. RWS branch is responsible for copying recalled words to the caption. Inspired by pointing mechanism in text summarization, we adopt a soft switch to balance the generated-word probabilities between SG and RWS. In the CIDEr optimization step, we also introduce an individual recalled-word reward (WR) to boost training. Our proposed methods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICE scores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 / 129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathy test split, which surpass the results of other state-of-the-art methods.},
  doi          = {10.1609/aaai.v34i07.6898},
  file         = {:PDF/6898-Article Text-10127-1-10-20200525.pdf:PDF},
  groups       = {Attention Over Visual Regions, Two-layer LSTM},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6898},
}

@InProceedings{Wang2021DynamiAttention,
  author    = {Wang, Changzhi and Gu, Xiaodong},
  booktitle = {2021 International Joint Conference on Neural Networks (IJCNN)},
  title     = {An Image Captioning Approach Using Dynamical Attention},
  year      = {2021},
  pages     = {1-8},
  doi       = {10.1109/IJCNN52387.2021.9533994},
  file      = {:PDF/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
  keywords  = {Visualization;Fuses;Computational modeling;Neural networks;Interference;Task analysis;Computational complexity;Image captioning;Attention mechanism;LSTM;CNN},
}

@InProceedings{Yao2019HierarchyParsing,
  author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Hierarchy Parsing for Image Captioning},
  year      = {2019},
  month     = {Oct},
  pages     = {2621-2629},
  abstract  = {It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.},
  doi       = {10.1109/ICCV.2019.00271},
  file      = {:PDF/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
  groups    = {Hierarchical trees, Two-layer LSTM},
  issn      = {2380-7504},
  keywords  = {Hip;Semantics;Task analysis;Decoding;Visualization;Context modeling;Image segmentation},
}

@InProceedings{Quin2019LookBack,
  author    = {Qin, Yu and Du, Jiajun and Zhang, Yonghua and Lu, Hongtao},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Look Back and Predict Forward in Image Captioning},
  year      = {2019},
  pages     = {8359-8367},
  doi       = {10.1109/CVPR.2019.00856},
  file      = {:PDF/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf:PDF},
  groups    = {Attention Over Visual Regions},
  keywords  = {Vision + Language;Deep Learning ; Image and Video Synthesis},
}

@Article{Ye2018Attentive,
  author   = {Ye, Senmao and Han, Junwei and Liu, Nian},
  journal  = {IEEE Transactions on Image Processing},
  title    = {Attentive Linear Transformation for Image Captioning},
  year     = {2018},
  issn     = {1941-0042},
  month    = {Nov},
  number   = {11},
  pages    = {5514-5524},
  volume   = {27},
  abstract = {We propose a novel attention framework called attentive linear transformation (ALT) for automatic generation of image captions. Instead of learning the spatial or channel-wise attention in existing models, ALT learns to attend to the high-dimensional transformation matrix from the image feature space to the context vector space. Thus ALT can learn various relevant feature abstractions, including spatial attention, channel-wise attention, and visual dependence. Besides, we propose a soft threshold regression to predict the spatial attention probabilities. It preserves more relevant local regions than popular softmax regression. Extensive experiments on the MS COCO and the Flickr30k data sets all demonstrate the superiority of our model compared with other state-of-the-art models.},
  doi      = {10.1109/TIP.2018.2855406},
  file     = {:PDF/Attentive_Linear_Transformation_for_Image_Captioning.pdf:PDF},
  keywords = {Visualization;Adaptation models;Semantics;Decoding;Task analysis;Feeds;Feature extraction;Image captioning;attention;linear transformation;CNN;LSTM},
}

@InProceedings{Yang2019Collate,
  author    = {Yang, Xu and Zhang, Hanwang and Cai, Jianfei},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Learning to Collocate Neural Modules for Image Captioning},
  year      = {2019},
  month     = {Oct},
  pages     = {4249-4259},
  abstract  = {We do not speak word by word from scratch; our brain quickly structures a pattern like STH DO STH AT SOMEPLACE and then fills in the detailed descriptions. To render existing encoder-decoder image captioners such humanlike reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the “inner pattern” connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (i.e., question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design - one for function words and three for visual content words (e.g., noun, adjective, and verb), 2) soft module fusion and multistep module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (e.g., adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDErD on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, e.g., by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.},
  doi       = {10.1109/ICCV.2019.00435},
  file      = {:PDF/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf:PDF},
  groups    = {Early self-attention approaches},
  issn      = {2380-7504},
  keywords  = {Visualization;Training;Task analysis;Cognition;Dogs;Decoding;Neural networks},
}

@misc{openai2023dalle3,
    title = {DALL·E 3 System Card},
    author = {{OpenAI}},
    year = {2023},
    url = {https://openai.com/research/dall-e-3-system-card},
    note = {Accessed: 2024-07-12}
}

@misc{openai2024gpt4o,
    title = {Introducing GPT-4o and More Tools to ChatGPT Free Users},
    author = {{OpenAI}},
    year = {2024},
    url = {https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/},
    note = {Accessed: 2024-07-12}
}

@InProceedings{Song2016Multimodal,
  author    = {Song, Mingoo and Yoo, Chang D.},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  title     = {Multimodal representation: Kneser-ney smoothing/skip-gram based neural language model},
  year      = {2016},
  month     = {Sep.},
  pages     = {2281-2285},
  abstract  = {For image retrieval and caption generation, this paper considers a multimodal representation that associates image with its text description (caption) by defining a neural language model as the conditional probability of the next word given both n past words in a caption and the image that the caption describes. To address the data sparsity problem, the use of the Kneser-Ney smoothing and skip-gram models is examined by integrating each into the multimodal neural language model. A language model (LM) known as Kneser-Ney smoothing is based on absolute-discounting interpolation while skip-gram LM is based on n-grams organized by allowing intermediate tokens to be “skipped”. The multimodal representation is evaluated on the IAPR TC-12 dataset. Using perplexity and BLEU-n measures, both Kneser-Ney smoothing and skip-gram models are demonstrated to be more effective as approaches to addressing the data sparsity problem than the generic n-gram model used in previous multimodal representations. The modality-biased log-bilinear (MLBL-B) model is set as the base model in the experiment.},
  doi       = {10.1109/ICIP.2016.7532765},
  issn      = {2381-8549},
  keywords  = {Data models;Smoothing methods;Context;Context modeling;Predictive models;Mathematical model;Numerical models;multimodal representation;neural language model;Kneser-Ney smoothing;skip-gram;data sparsity},
}

@Article{XU2023DeepCaptioning,
  author   = {Liming Xu and Quan Tang and Jiancheng Lv and Bochuan Zheng and Xianhua Zeng and Weisheng Li},
  journal  = {Neurocomputing},
  title    = {Deep image captioning: A review of methods, trends and future challenges},
  year     = {2023},
  issn     = {0925-2312},
  pages    = {126287},
  volume   = {546},
  abstract = {Image captioning, also called report generation in medical field, aims to describe visual content of images in human language, which requires to model semantic relationship between visual and textual elements and generate corresponding descriptions that conform to human language cognition. Image captioning is significant for promoting human–computer interaction in all fields and particularly, for computer-aided diagnosis in medical field. Currently, with the rapid development of deep learning technologies, image caption has attracted increasing attention of many researchers in artificial intelligence-related fields. To this end, this study attempts to provide readers with systematic and comprehensive research about different deep image captioning methods in natural and medical fields. We first introduce workflow of image captioning from perspective of simulating human process of describing images, including seeing, focusing and telling, which is respectively behavioralized into feature representation, visual encoding and language generation. Within it, we present common-used feature representation, visual encoding and language generation models. Then, we review datasets, evaluations and basic losses used in image captioning, and summarize typical caption methods which are generally divided into that with or without using reinforcement learning. Besides, we describe advantages and disadvantages of existing methods, and conclusion and challenges are finally presented.},
  doi      = {https://doi.org/10.1016/j.neucom.2023.126287},
  file     = {:PDF/1-s2.0-S0925231223004101-main.pdf:PDF},
  groups   = {review},
  keywords = {Image caption, Feature representation, Visual encoding, Language generation, Reinforcement learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223004101},
}

@Article{Hadosh2013Framing,
  author     = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
  journal    = {J. Artif. Int. Res.},
  title      = {Framing image description as a ranking task: data, models and evaluation metrics},
  year       = {2013},
  issn       = {1076-9757},
  month      = may,
  number     = {1},
  pages      = {853–899},
  volume     = {47},
  abstract   = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.},
  address    = {El Segundo, CA, USA},
  file       = {:PDF/live-3994-7274-jair.pdf:PDF},
  groups     = {retrieval},
  issue_date = {May 2013},
  numpages   = {47},
  publisher  = {AI Access Foundation},
}

@Book{Jurasky2023,
  author    = {Jurafsky, Daniel and Martin, James H.},
  publisher = {Prentice Hall PTR},
  title     = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  year      = {2000},
  address   = {USA},
  edition   = {1st},
  isbn      = {0130950696},
  abstract  = {From the Publisher:This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.   Methodology   boxes are included in each chapter.  Each chapter is built around one or more worked examples  to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.},
  file      = {:PDF/ed3book.pdf:PDF},
  groups    = {review},
}

@InProceedings{Luo2023SemanticDiffusionNetwork,
  author    = {Luo, Jianjie and Li, Yehao and Pan, Yingwei and Yao, Ting and Feng, Jianlin and Chao, Hongyang and Mei, Tao},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Semantic-Conditional Diffusion Networks for Image Captioning* }},
  year      = {2023},
  address   = {Los Alamitos, CA, USA},
  month     = Jun,
  pages     = {23359-23368},
  publisher = {IEEE Computer Society},
  abstract  = {Recent advances on text-to-image generation have witnessed the rise of diffusion models which act as powerful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the dependency among discrete words and meanwhile pursue complex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image captioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic information. The rich semantics are further regarded as semantic prior to trigger the learning of Diffusion Transformer, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer structures are stacked to progressively strengthen the output sentence with better visional-language alignment and linguistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical sequence training strategy is designed to guide the learning of SCD-Net with the knowledge of a standard autoregressive Transformer model. Extensive experiments on COCO dataset demonstrate the promising potential of using diffusion models in the challenging image captioning task. Source code is available at},
  doi       = {10.1109/CVPR52729.2023.02237},
  file      = {:PDF/Semantic-Conditional_Diffusion_Networks_for_Image_Captioning.pdf:PDF},
  groups    = {transformer},
  keywords  = {Training;Shape;Computational modeling;Source coding;Semantics;Diffusion processes;Coherence},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.02237},
}

@InProceedings{li2007Clasifying,
  author    = {Li, Li-Jia and Li Fei-Fei},
  booktitle = {2007 IEEE 11th International Conference on Computer Vision},
  title     = {What, where and who? Classifying events by scene and object recognition},
  year      = {2007},
  month     = {Oct},
  pages     = {1-8},
  abstract  = {We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.},
  doi       = {10.1109/ICCV.2007.4408872},
  file      = {:PDF/What_where_and_who_Classifying_events_by_scene_and-1.pdf:PDF},
  issn      = {2380-7504},
  keywords  = {Layout;Object recognition;Image recognition;Humans;Snow;Lakes;Boats;Graphical models;Assembly;Image databases},
}

@Article{Liu2019Survey,
  author     = {Liu, Xiaoxiao and Xu, Qingyang and Wang, Ning},
  journal    = {Vis. Comput.},
  title      = {A survey on deep neural network-based image captioning},
  year       = {2019},
  issn       = {0178-2789},
  month      = mar,
  number     = {3},
  pages      = {445–470},
  volume     = {35},
  abstract   = {Image captioning is a hot topic of image understanding, and it is composed of two natural parts ("look" and "language expression") which correspond to the two most important fields of artificial intelligence ("machine vision" and "natural language processing"). With the development of deep neural networks and better labeling database, the image captioning techniques have developed quickly. In this survey, the image captioning approaches and improvements based on deep neural network are introduced, including the characteristics of the specific techniques. The early image captioning approach based on deep neural network is the retrieval-based method. The retrieval method makes use of a searching technique to find an appropriate image description. The template-based method separates the image captioning process into object detection and sentence generation. Recently, end-to-end learning-based image captioning method has been verified effective at image captioning. The end-to-end learning techniques can generate more flexible and fluent sentence. In this survey, the image captioning methods are reviewed in detail. Furthermore, some remaining challenges are discussed.},
  address    = {Berlin, Heidelberg},
  doi        = {10.1007/s00371-018-1566-y},
  file       = {:PDF/s00371-018-1566-y.pdf:PDF},
  groups     = {review},
  issue_date = {March 2019},
  keywords   = {Attention mechanism, Dense captioning, Image captioning, Image understanding, Language model, Object detection},
  numpages   = {26},
  publisher  = {Springer-Verlag},
  url        = {https://doi.org/10.1007/s00371-018-1566-y},
}

@Article{Wang2019Survey,
  author     = {Yiyu Wang and Jungang Xu and Yingfei Sun and Ben He},
  journal    = {CoRR},
  title      = {Image Captioning based on Deep Learning Methods: {A} Survey},
  year       = {2019},
  volume     = {abs/1905.08110},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-08110.bib},
  eprint     = {1905.08110},
  eprinttype = {arXiv},
  file       = {:PDF/1905.08110v1.pdf:PDF},
  groups     = {review},
  timestamp  = {Tue, 28 May 2019 12:48:08 +0200},
  url        = {http://arxiv.org/abs/1905.08110},
}

@InProceedings{Elliott2013Image,
  author    = {Elliott, Desmond and Keller, Frank},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  title     = {Image Description using Visual Dependency Representations},
  year      = {2013},
  address   = {Seattle, Washington, USA},
  editor    = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
  month     = oct,
  pages     = {1292--1302},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/D13-1128.pdf:PDF},
  groups    = {template},
  url       = {https://aclanthology.org/D13-1128},
}

@InProceedings{Elliott2015Describing,
  author    = {Elliott, Desmond and de Vries, Arjen},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  title     = {Describing Images using Inferred Visual Dependency Representations},
  year      = {2015},
  address   = {Beijing, China},
  editor    = {Zong, Chengqing and Strube, Michael},
  month     = jul,
  pages     = {42--52},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/P15-1005},
  file      = {:PDF/P15-1005.pdf:PDF},
  groups    = {template},
  url       = {https://aclanthology.org/P15-1005},
}

@Article{Kuznetsova2014Treetalk,
  author    = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Tamara L. and Choi, Yejin},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {{T}ree{T}alk: Composition and Compression of Trees for Image Descriptions},
  year      = {2014},
  pages     = {351--362},
  volume    = {2},
  abstract  = {We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation, where the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00188},
  editor    = {Lin, Dekang and Collins, Michael and Lee, Lillian},
  file      = {:PDF/tacl_a_00188.pdf:PDF},
  groups    = {retrieval},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q14-1028},
}

@InProceedings{yatskar-etal-2014-see,
  author    = {Yatskar, Mark and Galley, Michel and Vanderwende, Lucy and Zettlemoyer, Luke},
  booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014)},
  title     = {See No Evil, Say No Evil: Description Generation from Densely Labeled Images},
  year      = {2014},
  address   = {Dublin, Ireland},
  editor    = {Bos, Johan and Frank, Anette and Navigli, Roberto},
  month     = aug,
  pages     = {110--120},
  publisher = {Association for Computational Linguistics and Dublin City University},
  doi       = {10.3115/v1/S14-1015},
  file      = {:PDF/S14-1015.pdf:PDF},
  url       = {https://aclanthology.org/S14-1015},
}

@InProceedings{BMVC2015_93,
  author    = {Dahua Lin and Sanja Fidler and Chen Kong and Raquel Urtasun},
  booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
  title     = {Generating Multi-sentence Natural Language Descriptions of Indoor Scenes},
  year      = {2015},
  editor    = {Xianghua Xie, Mark W. Jones, and Gary K. L. Tam},
  month     = {September},
  pages     = {93.1-93.13},
  publisher = {BMVA Press},
  articleno = {93},
  doi       = {10.5244/C.29.93},
  file      = {:PDF/paper093.pdf:PDF},
  groups    = {retrieval},
  isbn      = {1-901725-53-7},
  numpages  = {13},
  url       = {https://dx.doi.org/10.5244/C.29.93},
}

@Article{Felzenszwalb2010Discriminatively,
  author   = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Object Detection with Discriminatively Trained Part-Based Models},
  year     = {2010},
  issn     = {1939-3539},
  month    = {Sep.},
  number   = {9},
  pages    = {1627-1645},
  volume   = {32},
  abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
  doi      = {10.1109/TPAMI.2009.167},
  file     = {:PDF/lsvm-pami.pdf:PDF},
  keywords = {Object detection;Deformable models;Support vector machines;Bicycles;Computer vision;Shape;Speech recognition;Computer Society;Iterative algorithms;Lighting;Object recognition;deformable models;pictorial structures;discriminative training;latent SVM.},
}

@Article{Oliva2001ModelingTS,
  author  = {Aude Oliva and Antonio Torralba},
  journal = {International Journal of Computer Vision},
  title   = {Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope},
  year    = {2001},
  pages   = {145-175},
  volume  = {42},
  file    = {:PDF/A_1011139631724.pdf:PDF},
  url     = {https://api.semanticscholar.org/CorpusID:11664336},
}

@Article{Dunning1993Accurate,
  author     = {Dunning, Ted},
  journal    = {Comput. Linguist.},
  title      = {Accurate methods for the statistics of surprise and coincidence},
  year       = {1993},
  issn       = {0891-2017},
  month      = mar,
  number     = {1},
  pages      = {61–74},
  volume     = {19},
  abstract   = {Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
  address    = {Cambridge, MA, USA},
  file       = {:PDF/J93-1003.pdf:PDF},
  issue_date = {March 1993},
  numpages   = {14},
  publisher  = {MIT Press},
}

@Article{Farhadi2013Phrasal,
  author   = {Farhadi, Ali and Sadeghi, Mohammad Amin},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Phrasal Recognition},
  year     = {2013},
  issn     = {1939-3539},
  month    = {Dec},
  number   = {12},
  pages    = {2854-2865},
  volume   = {35},
  abstract = {In this paper, we introduce visual phrases, complex visual composites like "a person riding a horse." Visual phrases often display significantly reduced visual complexity compared to their component objects because the appearance of those objects can change profoundly when they participate in relations. We introduce a dataset suitable for phrasal recognition that uses familiar PASCAL object categories, and demonstrate significant experimental gains resulting from exploiting visual phrases. We show that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects. We argue that any multiclass detection system must decode detector outputs to produce final results; this is usually done with nonmaximum suppression. We describe a novel decoding procedure that can account accurately for local context without solving difficult inference problems. We show this decoding procedure outperforms the state of the art. Finally, we show that decoding a combination of phrasal and object detectors produces real improvements in detector results.},
  doi      = {10.1109/TPAMI.2013.168},
  file     = {:PDF/Phrasal_Recognition.pdf:PDF},
  keywords = {Data visualization;Detectors;Decoding;Object recognition;Image processing;Complexity theory;Visual phrase;phrasal recognition;visual composites;object recognition;object interactions;scene understanding;single image activity recognition;object subcategories},
}

@TechReport{Bach2001,
  author   = {Bach, Francis R. and Jordan, Michael I.},
  title    = {Kernel Independent Component Analysis},
  year     = {2001},
  month    = {Nov},
  number   = {UCB/CSD-01-1166},
  abstract = {We present a class of algorithms for Independent Component Analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.},
  file     = {:PDF/kernelICA-jmlr.pdf:PDF},
  url      = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2001/5721.html},
}

@Article{hardoon2004canonical,
  author    = {Hardoon, David R and Szedmak, Sandor and Shawe-Taylor, John},
  journal   = {Neural computation},
  title     = {Canonical correlation analysis: An overview with application to learning methods},
  year      = {2004},
  number    = {12},
  pages     = {2639--2664},
  volume    = {16},
  file      = {:PDF/tech_report03.pdf:PDF},
  publisher = {MIT Press},
}

@InProceedings{Mason2014nonparametric,
  author    = {Mason, Rebecca and Charniak, Eugene},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Nonparametric method for data-driven image captioning},
  year      = {2014},
  pages     = {592--598},
  file      = {:PDF/P14-2097.pdf:PDF},
  groups    = {retrieval},
}

@Article{Patterson2014Sun,
  author     = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
  journal    = {Int. J. Comput. Vision},
  title      = {The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding},
  year       = {2014},
  issn       = {0920-5691},
  month      = may,
  number     = {1–2},
  pages      = {59–81},
  volume     = {108},
  abstract   = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the "SUN attribute database" on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes.},
  address    = {USA},
  doi        = {10.1007/s11263-013-0695-z},
  file       = {:PDF/Common_Subspace_for_Model_and_Similarity_Phrase_Learning_for_Caption_Generation_from_Images.pdf:PDF},
  groups     = {retrieval},
  issue_date = {May 2014},
  keywords   = {Scene understanding, Scene parsing, Image captioning, Crowdsourcing, Attributes},
  numpages   = {23},
  publisher  = {Kluwer Academic Publishers},
  url        = {https://doi.org/10.1007/s11263-013-0695-z},
}

@InProceedings{Verma2014ImTxtandText2Im,
  author    = {Verma, Yashaswi and Jawahar, C. V.},
  booktitle = {Proceedings of the British Machine Vision Conference},
  title     = {Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval},
  year      = {2014},
  publisher = {BMVA Press},
  doi       = {http://dx.doi.org/10.5244/C.28.97},
  editors   = {Valstar, Michel and French, Andrew and Pridmore, Tony},
  file      = {:PDF/paper089.pdf:PDF},
  groups    = {retrieval},
}

@Article{Socher2014Grounded,
  author    = {Socher, Richard and Karpathy, Andrej and Le, Quoc V. and Manning, Christopher D. and Ng, Andrew Y.},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {Grounded Compositional Semantics for Finding and Describing Images with Sentences},
  year      = {2014},
  pages     = {207--218},
  volume    = {2},
  abstract  = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
  address   = {Cambridge, MA},
  doi       = {10.1162/tacl_a_00177},
  editor    = {Lin, Dekang and Collins, Michael and Lee, Lillian},
  file      = {:PDF/Q14-1017.pdf:PDF},
  groups    = {Earlier Deep Models},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q14-1017},
}

@InProceedings{Chen2015Minds,
  author    = {Chen, Xinlei and Zitnick, C. Lawrence},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Mind's eye: A recurrent visual representation for image caption generation }},
  year      = {2015},
  address   = {Los Alamitos, CA, USA},
  month     = Jun,
  pages     = {2422-2431},
  publisher = {IEEE Computer Society},
  abstract  = {In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. Critical to our approach is a recurrent neural network that attempts to dynamically build a visual representation of the scene as a caption is being generated or read. The representation automatically learns to remember long-term visual concepts. Our model is capable of both generating novel captions given an image, and reconstructing visual features given an image description. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are equal to or preferred by humans 21.0% of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.},
  doi       = {10.1109/CVPR.2015.7298856},
  file      = {:PDF/cvpr15_rnn.pdf:PDF},
  groups    = {multimodal, global CNN features},
  issn      = {1063-6919},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298856},
}

@Article{Donahue2017LongTerm,
  author     = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
  journal    = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title      = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  year       = {2017},
  issn       = {0162-8828},
  month      = apr,
  number     = {4},
  pages      = {677–691},
  volume     = {39},
  abstract   = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.},
  address    = {USA},
  doi        = {10.1109/TPAMI.2016.2599174},
  file       = {:PDF/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf:PDF},
  groups     = {encoder-decoder-lit, global CNN features, Two-layer LSTM},
  issue_date = {April 2017},
  numpages   = {15},
  publisher  = {IEEE Computer Society},
  url        = {https://doi.org/10.1109/TPAMI.2016.2599174},
}

@InProceedings{Jia2015Guiding,
  author    = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
  booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Guiding the Long-Short Term Memory Model for Image Caption Generation},
  year      = {2015},
  address   = {USA},
  pages     = {2407–2415},
  publisher = {IEEE Computer Society},
  series    = {ICCV '15},
  abstract  = {In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search to avoid bias towards short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or better than the current state-of-the-art.},
  doi       = {10.1109/ICCV.2015.277},
  file      = {:PDF/1509.04942v1.pdf:PDF},
  groups    = {encoder-decoder-lit, global CNN features},
  isbn      = {9781467383912},
  numpages  = {9},
  url       = {https://doi.org/10.1109/ICCV.2015.277},
}

@Article{Mao2014DeepCW,
  author  = {Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Alan Loddon Yuille},
  journal = {arXiv: Computer Vision and Pattern Recognition},
  title   = {Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},
  year    = {2014},
  file    = {:PDF/1412.6632v4.pdf:PDF},
  groups  = {multimodal, global CNN features},
  url     = {https://api.semanticscholar.org/CorpusID:3509328},
}

@InProceedings{Gilberto2015Learning,
  author    = {Gilberto Mateos Ortiz, Luis and Wolff, Clemens and Lapata, Mirella},
  booktitle = {Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Learning to Interpret and Describe Abstract Scenes},
  year      = {2015},
  address   = {Denver, Colorado},
  editor    = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
  month     = may # {{--}} # jun,
  pages     = {1505--1515},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/N15-1174},
  file      = {:PDF/N15-1174.pdf:PDF;:PDF/N15-1174.pdf:PDF},
  groups    = {retrieval},
  url       = {https://aclanthology.org/N15-1174},
}

@InProceedings{Lebret2015,
  author    = {R{\'{e}}mi Lebret and Pedro H. O. Pinheiro and Ronan Collobert},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  title     = {Simple Image Description Generator via a Linear Phrase-Based Approach},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/LebretPC14.bib},
  file      = {:PDF/1412.8419v3.pdf:PDF},
  groups    = {retrieval},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  url       = {http://arxiv.org/abs/1412.8419},
}

@InProceedings{Ushiku2015Common,
  author    = {Ushiku, Yoshitaka and Yamaguchi, Masataka and Mukuta, Yusuke and Harada, Tatsuya},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images},
  year      = {2015},
  month     = {Dec},
  pages     = {2668-2676},
  abstract  = {Generating captions to describe images is a fundamental problem that combines computer vision and natural language processing. Recent works focus on descriptive phrases, such as "a white dog" to explain the visual composites of an input image. The phrases can not only express objects, attributes, events, and their relations but can also reduce visual complexity. A caption for an input image can be generated by connecting estimated phrases using a grammar model. However, because phrases are combinations of various words, the number of phrases is much larger than the number of single words. Consequently, the accuracy of phrase estimation suffers from too few training samples per phrase. In this paper, we propose a novel phrase-learning method: Common Subspace for Model and Similarity (CoSMoS). In order to overcome the shortage of training samples, CoSMoS obtains a subspace in which (a) all feature vectors associated with the same phrase are mapped as mutually close, (b) classifiers for each phrase are learned, and (c) training samples are shared among co-occurring phrases. Experimental results demonstrate that our system is more accurate than those in earlier work and that the accuracy increases when the dataset from the web increases.},
  doi       = {10.1109/ICCV.2015.306},
  file      = {:PDF/Common_Subspace_for_Model_and_Similarity_Phrase_Learning_for_Caption_Generation_from_Images.pdf:PDF},
  groups    = {retrieval},
  issn      = {2380-7504},
  keywords  = {Training;Visualization;Learning systems;Neural networks;Grammar;Scalability;Feature extraction},
}

@Article{Qi2020ImageBERTCP,
  author  = {Di Qi and Lin Su and Jianwei Song and Edward Cui and Taroon Bharti and Arun Sacheti},
  journal = {ArXiv},
  title   = {ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data},
  year    = {2020},
  volume  = {abs/2001.07966},
  file    = {:PDF/2001.07966v2.pdf:PDF},
  groups  = {datasets},
  url     = {https://api.semanticscholar.org/CorpusID:210859480},
}

@Misc{Over2003Duc,
  author       = {Over, P. and Yen, J.},
  howpublished = {\url{http://www-nlpir.nist.gov/projects/duc/pubs/2003slides/duc2003intro.pdf}},
  title        = {An Introduction to {DUC} 2003: Intrinsic Evaluation of Generic News Text Summarization Systems},
  year         = {2003},
  added-at     = {2011-03-14T01:05:57.000+0100},
  biburl       = {https://www.bibsonomy.org/bibtex/2f9cfa29d7fd8761529266b686ea74c31/lantiq},
  file         = {:PDF/duc2003intro.pdf:PDF},
  groups       = {public},
  interhash    = {d76270e046732aa34bff1beec17bad23},
  intrahash    = {f9cfa29d7fd8761529266b686ea74c31},
  keywords     = {language summarization NLP},
  timestamp    = {2011-08-28T03:10:47.000+0200},
  username     = {lantiq},
}

@InProceedings{Levy2014WordEmbAsMatrixFactorization,
  author    = {Levy, Omer and Goldberg, Yoav},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  title     = {Neural word embedding as implicit matrix factorization},
  year      = {2014},
  address   = {Cambridge, MA, USA},
  pages     = {2177–2185},
  publisher = {MIT Press},
  series    = {NIPS'14},
  abstract  = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
  location  = {Montreal, Canada},
  numpages  = {9},
}

@Article{Wang2018,
  author     = {Wang, Cheng and Yang, Haojin and Meinel, Christoph},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning},
  year       = {2018},
  issn       = {1551-6857},
  month      = apr,
  number     = {2s},
  volume     = {14},
  abstract   = {Generating a novel and descriptive caption of an image is drawing increasing interests in computer vision, natural language processing, and multimedia communities. In this work, we propose an end-to-end trainable deep bidirectional LSTM (Bi-LSTM (Long Short-Term Memory)) model to address the problem. By combining a deep convolutional neural network (CNN) and two separate LSTM networks, our model is capable of learning long-term visual-language interactions by making use of history and future context information at high-level semantic space. We also explore deep multimodal bidirectional models, in which we increase the depth of nonlinearity transition in different ways to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale, and vertical mirror are proposed to prevent overfitting in training deep models. To understand how our models “translate” image to sentence, we visualize and qualitatively analyze the evolution of Bi-LSTM internal states over time. The effectiveness and generality of proposed models are evaluated on four benchmark datasets: Flickr8K, Flickr30K, MSCOCO, and Pascal1K datasets. We demonstrate that Bi-LSTM models achieve highly competitive performance on both caption generation and image-sentence retrieval even without integrating an additional mechanism (e.g., object detection, attention model). Our experiments also prove that multi-task learning is beneficial to increase model generality and gain performance. We also demonstrate the performance of transfer learning of the Bi-LSTM model significantly outperforms previous methods on the Pascal1K dataset.},
  address    = {New York, NY, USA},
  articleno  = {40},
  doi        = {10.1145/3115432},
  file       = {:PDF/3115432.pdf:PDF},
  groups     = {bez atencji},
  issue_date = {April 2018},
  keywords   = {Deep learning, LSTM, image captioning, multimodal representations, mutli-task learning},
  numpages   = {20},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3115432},
}

@InProceedings{Wu2016WhatValue,
  author    = {Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and Van Den Hengel, Anton},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {What Value Do Explicit High Level Concepts Have in Vision to Language Problems?},
  year      = {2016},
  month     = {June},
  pages     = {203-212},
  abstract  = {Much recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we investigate whether this direct approach succeeds due to, or despite, the fact that it avoids the explicit representation of high-level information. We propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. We achieve the best reported results on both image captioning and VQA on several benchmark datasets, and provide an analysis of the value of explicit high-level concepts in V2L problems.},
  doi       = {10.1109/CVPR.2016.29},
  file      = {:PDF/Wu_What_Value_Do_CVPR_2016_paper.pdf:PDF},
  groups    = {bez atencji, encoder-decoder-lit, global CNN features},
  issn      = {1063-6919},
  keywords  = {Visualization;Knowledge discovery;Feature extraction;Semantics;Vocabulary;Computer vision;Training},
}

@Article{KhangComparative,
  author  = {Khaing, Phyu and Yu, May},
  journal = {International Journal of Image, Graphics and Signal Processing},
  title   = {Attention-Based Deep Learning Model for Image Captioning: A Comparative Study},
  year    = {2019},
  month   = {06},
  pages   = {1-8},
  volume  = {11},
  doi     = {10.5815/ijigsp.2019.06.01},
  file    = {:PDF/IJIGSP-V11-N6-1.pdf:PDF},
  groups  = {review},
}

@Article{Jzefowicz2016ExploringTL,
  author  = {Rafal J{\'o}zefowicz and Oriol Vinyals and Mike Schuster and Noam M. Shazeer and Yonghui Wu},
  journal = {ArXiv},
  title   = {Exploring the Limits of Language Modeling},
  year    = {2016},
  volume  = {abs/1602.02410},
  file    = {:PDF/1602.02410v2.pdf:PDF},
  groups  = {lm},
  url     = {https://api.semanticscholar.org/CorpusID:260422},
}

@InProceedings{Etal2006generating,
  author    = {de Marneffe, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},
  title     = {Generating Typed Dependency Parses from Phrase Structure Parses},
  year      = {2006},
  address   = {Genoa, Italy},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Gangemi, Aldo and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Tapias, Daniel},
  month     = may,
  publisher = {European Language Resources Association (ELRA)},
  abstract  = {This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.},
  file      = {:PDF/440_pdf.pdf:PDF},
  url       = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf},
}

@InProceedings{NIPS2013_7cce53cf,
  author    = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc\textquotesingle Aurelio and Mikolov, Tomas},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {DeViSE: A Deep Visual-Semantic Embedding Model},
  year      = {2013},
  editor    = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {26},
  file      = {:PDF/NIPS-2013-devise-a-deep-visual-semantic-embedding-model-Paper.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/file/7cce53cf90577442771720a370c3c723-Paper.pdf},
}

@InProceedings{Mnih2007Three,
  author    = {Mnih, Andriy and Hinton, Geoffrey},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning},
  title     = {Three new graphical models for statistical language modelling},
  year      = {2007},
  address   = {New York, NY, USA},
  pages     = {641–648},
  publisher = {Association for Computing Machinery},
  series    = {ICML '07},
  abstract  = {The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.},
  doi       = {10.1145/1273496.1273577},
  file      = {:PDF/threenew.pdf:PDF},
  isbn      = {9781595937933},
  location  = {Corvalis, Oregon, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1273496.1273577},
}

@InProceedings{Xu2015ShowAttendTell,
  author    = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  title     = {Show, attend and tell: neural image caption generation with visual attention},
  year      = {2015},
  pages     = {2048–2057},
  publisher = {JMLR.org},
  series    = {ICML'15},
  abstract  = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
  file      = {:PDF/xuc15.pdf:PDF},
  groups    = {Attention Over Grid of CNN Features, single layer LSTM, attention taxonomy},
  location  = {Lille, France},
  numpages  = {10},
}

@Article{Zhou2022Collaborative,
  author  = {Zhou, Dongming and Yang, Jing and Bao, Riqiang},
  journal = {Applied Intelligence},
  title   = {Collaborative strategy network for spatial attention image captioning},
  year    = {2022},
  month   = {06},
  pages   = {1-16},
  volume  = {52},
  doi     = {10.1007/s10489-021-02943-w},
  file    = {:PDF/s10489-021-02943-w.pdf:PDF},
}

@InProceedings{Gan2017Semantic,
  author    = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Semantic Compositional Networks for Visual Captioning }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = Jul,
  pages     = {1141-1150},
  publisher = {IEEE Computer Society},
  abstract  = {A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.},
  doi       = {10.1109/CVPR.2017.127},
  file      = {:PDF/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf:PDF},
  groups    = {global CNN features},
  issn      = {1063-6919},
  keywords  = {Semantics;Visualization;Pediatrics;Feature extraction;Mouth;Tensile stress;Training},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.127},
}

@Article{Jin2015Aligning,
  author     = {Junqi Jin and Kun Fu and Runpeng Cui and Fei Sha and Changshui Zhang},
  journal    = {CoRR},
  title      = {Aligning where to see and what to tell: image caption with region-based attention and scene factorization},
  year       = {2015},
  volume     = {abs/1506.06272},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/JinFCSZ15.bib},
  eprint     = {1506.06272},
  eprinttype = {arXiv},
  file       = {:PDF/1506.06272v1.pdf:PDF},
  timestamp  = {Mon, 21 Feb 2022 19:31:03 +0100},
  url        = {http://arxiv.org/abs/1506.06272},
}

@InProceedings{Tavalkoyi2017PayingAttention,
  author    = {Tavakoliy, Hamed R. and Shetty, Rakshith and Borji, Ali and Laaksonen, Jorma},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Paying Attention to Descriptions Generated by Image Captioning Models},
  year      = {2017},
  month     = {Oct},
  pages     = {2506-2515},
  abstract  = {To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliencyboosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.},
  doi       = {10.1109/ICCV.2017.272},
  file      = {:PDF/Tavakoli_Paying_Attention_to_ICCV_2017_paper.pdf:PDF},
  groups    = {Exploiting human attention, attention},
  issn      = {2380-7504},
  keywords  = {Visualization;Measurement;Data models;Grammar;Computational modeling;Computer science},
}

@Article{Wu2018ICandVQA,
  author   = {Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and Hengel, Anton van den},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Image Captioning and Visual Question Answering Based on Attributes and External Knowledge},
  year     = {2018},
  issn     = {1939-3539},
  month    = {June},
  number   = {6},
  pages    = {1367-1381},
  volume   = {40},
  abstract = {Much of the recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked where the image alone does not contain the information required to select the appropriate answer. Our final model achieves the best reported results for both image captioning and visual question answering on several of the major benchmark datasets.},
  doi      = {10.1109/TPAMI.2017.2708709},
  file     = {:PDF/1603.02814v2.pdf:PDF},
  keywords = {Visualization;Knowledge discovery;Semantics;Computational modeling;Computer vision;Knowledge based systems;Resource description framework;Image captioning;visual question answering;concepts learning;recurrent neural networks;LSTM},
}

@Article{Wang2019Hierarchical,
  author       = {Wang, Weixuan and Chen, Zhihong and Hu, Haifeng},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Hierarchical Attention Network for Image Captioning},
  year         = {2019},
  month        = {Jul.},
  number       = {01},
  pages        = {8957-8964},
  volume       = {33},
  abstractnote = {Recently, attention mechanism has been successfully applied in image captioning, but the existing attention methods are only established on low-level spatial features or high-level text features, which limits richness of captions. In this paper, we propose a Hierarchical Attention Network (HAN) that enables attention to be calculated on pyramidal hierarchy of features synchronously. The pyramidal hierarchy consists of features on diverse semantic levels, which allows predicting different words according to different features. On the other hand, due to the different modalities of features, a Multivariate Residual Module (MRM) is proposed to learn the joint representations from features. The MRM is able to model projections and extract relevant relations among different features. Furthermore, we introduce a context gate to balance the contribution of different features. Compared with the existing methods, our approach applies hierarchical features and exploits several multimodal integration strategies, which can significantly improve the performance. The HAN is verified on benchmark MSCOCO dataset, and the experimental results indicate that our model outperforms the state-of-the-art methods, achieving a BLEU1 score of 80.9 and a CIDEr score of 121.7 in the Karpathy’s test split.},
  doi          = {10.1609/aaai.v33i01.33018957},
  file         = {:PDF/4924-Article Text-7990-1-10-20190709-1.pdf:PDF},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4924},
}

@InProceedings{Zhou2020More,
  author    = {Zhou, Yuanen and Wang, Meng and Liu, Daqing and Hu, Zhenzhen and Zhang, Hanwang},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {More Grounded Image Captioning by Distilling Image-Text Matching Model},
  year      = {2020},
  month     = {June},
  pages     = {4776-4785},
  abstract  = {Visual attention not only improves the performance of image captioners, but also serves as a visual interpretation to qualitatively measure the caption rationality and model transparency. Specifically, we expect that a captioner can fix its attentive gaze on the correct objects while generating the corresponding words. This ability is also known as grounded image captioning. However, the grounding accuracy of existing captioners is far from satisfactory.To improve the grounding accuracy while retaining the captioning quality, it is expensive to collect the word-region alignment as strong supervision.To this end, we propose a Part-of-Speech (POS) enhanced image-text matching model (SCAN): POS-SCAN, as the effective knowledge distillation for more grounded image captioning. The benefits are two-fold: 1) given a sentence and an image, POS-SCAN can ground the objects more accurately than SCAN; 2) POS-SCAN serves as a word-region alignment regularization for the captioner's visual attention module. By showing benchmark experimental results, we demonstrate that conventional image captioners equipped with POS-SCAN can significantly improve the grounding accuracy without strong supervision. Last but not the least, we explore the indispensable Self-Critical Sequence Training (SCST) in the context of grounded image captioning and show that the image-text matching score can serve as a reward for more grounded captioning.},
  doi       = {10.1109/CVPR42600.2020.00483},
  file      = {:PDF/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Visualization;Grounding;Task analysis;Training;Measurement;Computational modeling;Image edge detection},
}

@Article{Yu2020Multimodal,
  author   = {Yu, Jun and Li, Jing and Yu, Zhou and Huang, Qingming},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {Multimodal Transformer With Multi-View Visual Representation for Image Captioning},
  year     = {2020},
  issn     = {1558-2205},
  month    = {Dec},
  number   = {12},
  pages    = {4467-4480},
  volume   = {30},
  abstract = {Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN) based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.},
  doi      = {10.1109/TCSVT.2019.2947482},
  file     = {:PDF/Multimodal_Transformer_With_Multi-View_Visual_Representation_for_Image_Captioning.pdf:PDF},
  keywords = {Visualization;Feature extraction;Hidden Markov models;Adaptation models;Task analysis;Decoding;Computational modeling;Image captioning;multi-view learning;deep learning},
}

@InProceedings{Li2019Entangled,
  author    = {Li, Guang and Zhu, Linchao and Liu, Ping and Yang, Yi},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Entangled Transformer for Image Captioning},
  year      = {2019},
  month     = {Oct},
  pages     = {8927-8936},
  abstract  = {In image captioning, the typical attention mechanisms are arduous to identify the equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as the semantic gap between vision and language. This problem can be overcome by providing semantic attributes that are homologous to language. Thanks to the inherent recurrent nature and gated operating mechanism, Recurrent Neural Network (RNN) and its variants are the dominating architectures in image captioning. However, when designing elaborate attention mechanisms to integrate visual inputs and semantic attributes, RNN-like variants become unflexible due to their complexities. In this paper, we investigate a Transformer-based sequence modeling framework, built only with attention layers and feedforward layers. To bridge the semantic gap, we introduce EnTangled Attention (ETA) that enables the Transformer to exploit semantic and visual information simultaneously. Furthermore, Gated Bilateral Controller (GBC) is proposed to guide the interactions between the multimodal information. We name our model as ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on the MSCOCO image captioning dataset. The ablation studies validate the improvements of our proposed modules.},
  doi       = {10.1109/ICCV.2019.00902},
  file      = {:PDF/Entangled_Transformer_for_Image_Captioning.pdf:PDF},
  groups    = {Early self-attention approaches, Gating mechanisms.},
  issn      = {2380-7504},
  keywords  = {Semantics;Visualization;Decoding;Encoding;Logic gates;Snow;Proposals},
}

@Article{Fan2021TCICTC,
  author  = {Zhihao Fan and Zhongyu Wei and Siyuan Wang and Ruize Wang and Zejun Li and Haijun Shan and Xuanjing Huang},
  journal = {ArXiv},
  title   = {TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning},
  year    = {2021},
  volume  = {abs/2106.10936},
  file    = {:PDF/0091.pdf:PDF},
  groups  = {transformer},
  url     = {https://api.semanticscholar.org/CorpusID:235489710},
}

@Article{Ji2020ImprovingIC,
  author  = {Jiayi Ji and Yunpeng Luo and Xiaoshuai Sun and Fuhai Chen and Gen Luo and Yongjian Wu and Yue Gao and Rongrong Ji},
  journal = {ArXiv},
  title   = {Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network},
  year    = {2020},
  volume  = {abs/2012.07061},
  file    = {:PDF/16258-13-19752-1-2-20210518.pdf:PDF},
  groups  = {Other, Gating mechanisms.},
  url     = {https://api.semanticscholar.org/CorpusID:229152503},
}

@InProceedings{Sariyildiz2020LearnVisRepr,
  author    = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII},
  title     = {Learning Visual Representations with&nbsp;Caption Annotations},
  year      = {2020},
  address   = {Berlin, Heidelberg},
  pages     = {153–170},
  publisher = {Springer-Verlag},
  abstract  = {Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce image-conditioned masked language modeling (ICMLM) – a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: .},
  doi       = {10.1007/978-3-030-58598-3_10},
  file      = {:PDF/2008.01392v1.pdf:PDF},
  isbn      = {978-3-030-58597-6},
  location  = {Glasgow, United Kingdom},
  numpages  = {18},
  url       = {https://doi.org/10.1007/978-3-030-58598-3_10},
}

@Article{Zhu2018CaptTransf,
  author         = {Zhu, Xinxin and Li, Lixiang and Liu, Jing and Peng, Haipeng and Niu, Xinxin},
  journal        = {Applied Sciences},
  title          = {Captioning Transformer with Stacked Attention Modules},
  year           = {2018},
  issn           = {2076-3417},
  number         = {5},
  volume         = {8},
  abstract       = {Image captioning is a challenging task. Meanwhile, it is important for the machine to understand the meaning of an image better. In recent years, the image captioning usually use the long-short-term-memory (LSTM) as the decoder to generate the sentence, and these models show excellent performance. Although the LSTM can memorize dependencies, the LSTM structure has complicated and inherently sequential across time problems. To address these issues, recent works have shown benefits of the Transformer for machine translation. Inspired by their success, we develop a Captioning Transformer (CT) model with stacked attention modules. We attempt to introduce the Transformer to the image captioning task. The CT model contains only attention modules without the dependencies of the time. It not only can memorize dependencies between the sequence but also can be trained in parallel. Moreover, we propose the multi-level supervision to make the Transformer achieve better performance. Extensive experiments are carried out on the challenging MSCOCO dataset and the proposed Captioning Transformer achieves competitive performance compared with some state-of-the-art methods.},
  article-number = {739},
  doi            = {10.3390/app8050739},
  file           = {:PDF/applsci-08-00739.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/8/5/739},
}

@Article{Li2019Boosted,
  author         = {Li, Jiangyun and Yao, Peng and Guo, Longteng and Zhang, Weicun},
  journal        = {Applied Sciences},
  title          = {Boosted Transformer for Image Captioning},
  year           = {2019},
  issn           = {2076-3417},
  number         = {16},
  volume         = {9},
  abstract       = {Image captioning attempts to generate a description given an image, usually taking Convolutional Neural Network as the encoder to extract the visual features and a sequence model, among which the self-attention mechanism has achieved advanced progress recently, as the decoder to generate descriptions. However, this predominant encoder-decoder architecture has some problems to be solved. On the encoder side, without the semantic concepts, the extracted visual features do not make full use of the image information. On the decoder side, the sequence self-attention only relies on word representations, lacking the guidance of visual information and easily influenced by the language prior. In this paper, we propose a novel boosted transformer model with two attention modules for the above-mentioned problems, i.e., “Concept-Guided Attention” (CGA) and “Vision-Guided Attention” (VGA). Our model utilizes CGA in the encoder, to obtain the boosted visual features by integrating the instance-level concepts into the visual features. In the decoder, we stack VGA, which uses the visual information as a bridge to model internal relationships among the sequences and can be an auxiliary module of sequence self-attention. Quantitative and qualitative results on the Microsoft COCO dataset demonstrate the better performance of our model than the state-of-the-art approaches.},
  article-number = {3260},
  doi            = {10.3390/app9163260},
  file           = {:PDF/applsci-09-03260.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/9/16/3260},
}

@Article{Jiang2021Multigate,
  author   = {Jiang, Weitao and Li, Xiying and Hu, Haifeng and Lu, Qiang and Liu, Bohong},
  journal  = {IEEE Access},
  title    = {Multi-Gate Attention Network for Image Captioning},
  year     = {2021},
  issn     = {2169-3536},
  pages    = {69700-69709},
  volume   = {9},
  abstract = {Self-attention mechanism, which has been successfully applied to current encoder-decoder framework of image captioning, is used to enhance the feature representation in the image encoder and capture the most relevant information for the language decoder. However, most existing methods will assign attention weights to all candidate vectors, which implicitly hypothesizes that all vectors are relevant. Moreover, current self-attention mechanisms ignore the intra-object attention distribution, and only consider the inter-object relationships. In this paper, we propose a Multi-Gate Attention (MGA) block, which expands the traditional self-attention by equipping with additional Attention Weight Gate (AWG) module and Self-Gated (SG) module. The former constrains the attention weights to be assigned to the most contributive objects. The latter is adopted to consider the intra-object attention distribution and eliminate the irrelevant information in object feature vector. Furthermore, most current image captioning methods apply the original transformer designed for natural language processing task, to refine image features directly. Therefore, we propose a pre-layernorm transformer to simplify the transformer architecture and make it more efficient for image feature enhancement. By integrating MGA block with pre-layernorm transformer architecture into the image encoder and AWG module into the language decoder, we present a novel Multi-Gate Attention Network (MGAN). The experiments on MS COCO dataset indicate that the MGAN outperforms most of the state-of-the-art, and further experiments on other methods combined with MGA blocks demonstrate the generalizability of our proposal.},
  doi      = {10.1109/ACCESS.2021.3067607},
  file     = {:PDF/Multi-Gate_Attention_Network_for_Image_Captioning.pdf:PDF},
  keywords = {Decoding;Logic gates;Task analysis;Semantics;Visualization;Feature extraction;Proposals;Image captioning;self-attention;transformer;multi-gate attention},
}

@Article{Kumar2022Dual,
  author         = {Kumar, Deepika and Srivastava, Varun and Popescu, Daniela Elena and Hemanth, Jude D.},
  journal        = {Applied Sciences},
  title          = {Dual-Modal Transformer with Enhanced Inter- and Intra-Modality Interactions for Image Captioning},
  year           = {2022},
  issn           = {2076-3417},
  number         = {13},
  volume         = {12},
  abstract       = {Image captioning is oriented towards describing an image with the best possible use of words that can provide a semantic, relatable meaning of the scenario inscribed. Different models can be used to accomplish this arduous task depending on the context and requirement of what needs to be achieved. An encoder–decoder model which uses the image feature vectors as an input to the encoder is often marked as one of the appropriate models to accomplish the captioning process. In the proposed work, a dual-modal transformer has been used which captures the intra- and inter-model interactions in a simultaneous manner within an attention block. The transformer architecture is quantitatively evaluated on a publicly available Microsoft Common Objects in Context (MS COCO) dataset yielding a Bilingual Evaluation Understudy (BLEU)-4 Score of 85.01. The efficacy of the model is evaluated on Flickr 8k, Flickr 30k datasets and MS COCO datasets and results for the same is compared and analysed with the state-of-the-art methods. The results shows that the proposed model outperformed when compared with conventional models, such as the encoder–decoder model and attention model.},
  article-number = {6733},
  doi            = {10.3390/app12136733},
  file           = {:PDF/applsci-12-06733.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/12/13/6733},
}

@InProceedings{Sarto22Retrieval,
  author    = {Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
  title     = {Retrieval-Augmented Transformer for Image Captioning},
  year      = {2022},
  address   = {New York, NY, USA},
  pages     = {1–7},
  publisher = {Association for Computing Machinery},
  series    = {CBMI '22},
  abstract  = {Image captioning models aim at connecting Vision and Language by providing natural language descriptions of input images. In the past few years, the task has been tackled by learning parametric models and proposing visual feature extraction advancements or by modeling better multi-modal connections. In this paper, we investigate the development of an image captioning approach with a kNN memory, with which knowledge can be retrieved from an external corpus to aid the generation process. Our architecture combines a knowledge retriever based on visual similarities, a differentiable encoder, and a kNN-augmented attention layer to predict tokens based on the past context and on text retrieved from the external memory. Experimental results, conducted on the COCO dataset, demonstrate that employing an explicit external memory can aid the generation process and increase caption quality. Our work opens up new avenues for improving image captioning models at larger scale.},
  doi       = {10.1145/3549555.3549585},
  file      = {:PDF/3549555.3549585.pdf:PDF},
  isbn      = {9781450397209},
  keywords  = {vision-and-language., image retrieval, image captioning},
  location  = {Graz, Austria},
  numpages  = {7},
  url       = {https://doi.org/10.1145/3549555.3549585},
}

@InProceedings{Vinyals2015PointerNetworks,
  author    = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Pointer Networks},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {28},
  file      = {:PDF/NIPS-2015-pointer-networks-Paper.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf},
}

@Book{zhang2023dive,
  author    = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  publisher = {Cambridge University Press},
  title     = {Dive into Deep Learning},
  year      = {2023},
  note      = {\url{https://D2L.ai}},
  file      = {:PDF/2106.11342v5.pdf:PDF},
}

@InBook{ElAsnaoui2021AutomatedMethods,
  author    = {El Asnaoui, Khalid and Chawki, Youness and Idri, Ali},
  editor    = {Maleh, Yassine and Baddi, Youssef and Alazab, Mamoun and Tawalbeh, Loai and Romdhani, Imed},
  pages     = {257--284},
  publisher = {Springer International Publishing},
  title     = {Automated Methods for Detection and Classification Pneumonia Based on X-Ray Images Using Deep Learning},
  year      = {2021},
  address   = {Cham},
  isbn      = {978-3-030-74575-2},
  abstract  = {Recently, researchers, specialists, and companies around the world are rolling out deep learning and image processing-based systems that can fastly process hundreds of X-Ray and Computed Tomography (CT) images to accelerate the diagnosis of pneumonia such as SARS, covid-19, etc., and aid in its containment. Medical image analysis is one of the most promising research areas; it provides facilities for diagnosis and making decisions of several diseases such as MERS, covid-19, etc. In this paper, we present a comparison of recent deep convolutional neural network (CNN) architectures for automatic binary classification of pneumonia images based on fined tuned versions of (VGG16, VGG19, DenseNet201, Inception{\_}ResNet{\_}V2, Inception{\_}V3, Resnet50, MobileNet{\_}V2 and Xception) and a retraining of a baseline CNN. The proposed work has been tested using chest X-Ray {\&} CT dataset, which contains 6087 images (4504 pneumonia and 1583 normal). As a result, we can conclude that the fine-tuned version of Resnet50 shows highly satisfactory performance with rate of increase in training and testing accuracy (more than 96{\%} of accuracy).},
  booktitle = {Artificial Intelligence and Blockchain for Future Cybersecurity Applications},
  doi       = {10.1007/978-3-030-74575-2_14},
  file      = {:PDF/2003.14363v1.pdf:PDF},
  url       = {https://doi.org/10.1007/978-3-030-74575-2_14},
}

@InProceedings{Ferraro2015Survey,
  author    = {Ferraro, Francis and Mostafazadeh, Nasrin and Huang, Ting-Hao and Vanderwende, Lucy and Devlin, Jacob and Galley, Michel and Mitchell, Margaret},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  title     = {A Survey of Current Datasets for Vision and Language Research},
  year      = {2015},
  address   = {Lisbon, Portugal},
  editor    = {M{\`a}rquez, Llu{\'\i}s and Callison-Burch, Chris and Su, Jian},
  month     = sep,
  pages     = {207--213},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D15-1021},
  file      = {:PDF/D15-1021.pdf:PDF},
  url       = {https://aclanthology.org/D15-1021},
}

@InProceedings{callison-burch-etal-2007-meta,
  author    = {Callison-Burch, Chris and Fordyce, Cameron and Koehn, Philipp and Monz, Christof and Schroeder, Josh},
  booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
  title     = {(Meta-) Evaluation of Machine Translation},
  year      = {2007},
  address   = {Prague, Czech Republic},
  editor    = {Callison-Burch, Chris and Koehn, Philipp and Fordyce, Cameron Shaw and Monz, Christof},
  month     = jun,
  pages     = {136--158},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/W07-0718.pdf:PDF},
  url       = {https://aclanthology.org/W07-0718},
}

@InProceedings{van-der-lee-etal-2019-best,
  author    = {van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel},
  booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
  title     = {Best practices for the human evaluation of automatically generated text},
  year      = {2019},
  address   = {Tokyo, Japan},
  editor    = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
  month     = oct # {{--}} # nov,
  pages     = {355--368},
  publisher = {Association for Computational Linguistics},
  abstract  = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.},
  doi       = {10.18653/v1/W19-8643},
  file      = {:PDF/W19-8643.pdf:PDF},
  url       = {https://aclanthology.org/W19-8643},
}

@Article{Celikyilmaz2020EvaluationOT,
  author  = {Asli Celikyilmaz and Elizabeth Clark and Jianfeng Gao},
  journal = {ArXiv},
  title   = {Evaluation of Text Generation: A Survey},
  year    = {2020},
  volume  = {abs/2006.14799},
  file    = {:PDF/2006.14799v1.pdf:PDF},
  url     = {https://api.semanticscholar.org/CorpusID:220128348},
}

@InProceedings{Cristianini2000AnIT,
  author = {Nello Cristianini and John Shawe-Taylor},
  title  = {An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
  year   = {2000},
  file   = {:PDF/512565724.pdf:PDF},
  url    = {https://api.semanticscholar.org/CorpusID:60486887},
}

@Article{Reiter2009Investigation,
  author    = {Reiter, Ehud and Belz, Anja},
  journal   = {Computational Linguistics},
  title     = {An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems},
  year      = {2009},
  month     = dec,
  number    = {4},
  pages     = {529--558},
  volume    = {35},
  address   = {Cambridge, MA},
  doi       = {10.1162/coli.2009.35.4.35405},
  file      = {:PDF/J09-4008.pdf:PDF},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/J09-4008},
}

@Article{Bai2018Survey,
  author     = {Bai, Shuang and An, Shan},
  journal    = {Neurocomput.},
  title      = {A survey on automatic image caption generation},
  year       = {2018},
  issn       = {0925-2312},
  month      = oct,
  number     = {C},
  pages      = {291–304},
  volume     = {311},
  address    = {NLD},
  doi        = {10.1016/j.neucom.2018.05.080},
  file       = {:PDF/elsarticle-template-num.pdf:PDF},
  groups     = {review},
  issue_date = {Oct 2018},
  keywords   = {Image captioning, Sentence template, Deep neural networks, Multimodal embedding, Encoder–decoder framework, Attention mechanism},
  numpages   = {14},
  publisher  = {Elsevier Science Publishers B. V.},
  url        = {https://doi.org/10.1016/j.neucom.2018.05.080},
}

@Article{Sarto2024Retrieval,
  author     = {Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Nicolosi, Alessandro and Cucchiara, Rita},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {Towards Retrieval-Augmented Architectures for Image Captioning},
  year       = {2024},
  issn       = {1551-6857},
  month      = jun,
  number     = {8},
  volume     = {20},
  abstract   = {The objective of image captioning models is to bridge the gap between the visual and linguistic modalities by generating natural language descriptions that accurately reflect the content of input images. In recent years, researchers have leveraged deep learning-based models and made advances in the extraction of visual features and the design of multimodal connections to tackle this task. This work presents a novel approach toward developing image captioning models that utilize an external kNN memory to improve the generation process. Specifically, we propose two model variants that incorporate a knowledge retriever component that is based on visual similarities, a differentiable encoder to represent input images, and a kNN-augmented language model to predict tokens based on contextual cues and text retrieved from the external memory. We experimentally validate our approach on COCO and nocaps datasets and demonstrate that incorporating an explicit external memory can significantly enhance the quality of captions, especially with a larger retrieval corpus. This work provides valuable insights into retrieval-augmented captioning models and opens up new avenues for improving image captioning at a larger scale.},
  address    = {New York, NY, USA},
  articleno  = {242},
  doi        = {10.1145/3663667},
  issue_date = {August 2024},
  keywords   = {Image captioning, image retrieval, vision-and-language},
  numpages   = {22},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3663667},
}

@Article{Trisana2024FusionTextRepresentation,
  author         = {Trisna, Komang Wahyu and Huang, Jinjie and Liang, Hengyu and Dharma, Eddy Muntina},
  journal        = {Applied Sciences},
  title          = {Fusion Text Representations to Enhance Contextual Meaning in Sentiment Classification},
  year           = {2024},
  issn           = {2076-3417},
  number         = {22},
  volume         = {14},
  abstract       = {Sentiment classification plays a crucial role in evaluating user feedback. Today, online media users can freely provide their reviews with few restrictions. User reviews on social media are often disorganized and challenging to classify as positive or negative comments. This task becomes even more difficult when dealing with large amounts of data, making sentiment classification necessary. Automating sentiment classification involves text classification processes, commonly performed using deep learning methods. The classification process using deep learning models is closely tied to text representation. This step is critical as it affects the quality of the data being processed by the deep learning model. Traditional text representation methods often overlook the contextual meaning of sentences, leading to potential misclassification by the model. In this study, we propose a novel fusion text representation model, GloWord_biGRU, designed to enhance the contextual understanding of sentences for sentiment classification. Firstly, we combine the advantages of GloVe and Word2Vec to obtain richer and more meaningful word representations. GloVe provides word representations based on global frequency statistics within a large corpus, while Word2Vec generates word vectors that capture local contextual relationships. By integrating these two approaches, we enhance the quality of word representations used in our model. During the classification stage, we employ biGRU, considering the use of fewer parameters, which consequently reduces computational requirements. We evaluate the proposed model using the IMDB dataset. Several scenarios demonstrate that our proposed model achieves superior performance, with an F1 score of 90.21%.},
  article-number = {10420},
  doi            = {10.3390/app142210420},
  file           = {:PDF/applsci-14-10420.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/14/22/10420},
}

@Article{Mars2022FromWordEmbeddings,
  author         = {Mars, Mourad},
  journal        = {Applied Sciences},
  title          = {From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough},
  year           = {2022},
  issn           = {2076-3417},
  number         = {17},
  volume         = {12},
  abstract       = {With the recent advances in deep learning, different approaches to improving pre-trained language models (PLMs) have been proposed. PLMs have advanced state-of-the-art (SOTA) performance on various natural language processing (NLP) tasks such as machine translation, text classification, question answering, text summarization, information retrieval, recommendation systems, named entity recognition, etc. In this paper, we provide a comprehensive review of prior embedding models as well as current breakthroughs in the field of PLMs. Then, we analyse and contrast the various models and provide an analysis of the way they have been built (number of parameters, compression techniques, etc.). Finally, we discuss the major issues and future directions for each of the main points.},
  article-number = {8805},
  doi            = {10.3390/app12178805},
  file           = {:PDF/applsci-12-08805-v3.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/12/17/8805},
}

@Article{apidianaki-2023-word,
  author    = {Apidianaki, Marianna},
  journal   = {Computational Linguistics},
  title     = {From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation},
  year      = {2023},
  month     = jun,
  number    = {2},
  pages     = {465--523},
  volume    = {49},
  abstract  = {Vector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.},
  address   = {Cambridge, MA},
  doi       = {10.1162/coli_a_00474},
  file      = {:PDF/2023.cl-2.7.pdf:PDF},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2023.cl-2.7},
}

@InProceedings{Lochter2020DeOutOfVocab,
  author    = {Lochter, Johannes V. and Silva, Renato M. and Almeida, Tiago A.},
  booktitle = {Intelligent Systems: 9th Brazilian Conference, BRACIS 2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I},
  title     = {Deep Learning Models for Representing Out-of-Vocabulary Words},
  year      = {2020},
  address   = {Berlin, Heidelberg},
  pages     = {418–434},
  publisher = {Springer-Verlag},
  abstract  = {Communication has become increasingly dynamic with the popularization of social networks and applications that allow people to express themselves and communicate instantly. In this scenario, distributed representation models have their quality impacted by new words that appear frequently or that are derived from spelling errors. These words that are unknown by the models, known as out-of-vocabulary (OOV) words, need to be properly handled to not degrade the quality of the natural language processing (NLP) applications, which depend on the appropriate vector representation of the texts. To better understand this problem and finding the best techniques to handle OOV words, in this study, we present a comprehensive performance evaluation of deep learning models for representing OOV words. We performed an intrinsic evaluation using a benchmark dataset and an extrinsic evaluation using different NLP tasks: text categorization, named entity recognition, and part-of-speech tagging. Although the results indicated that the best technique for handling OOV words is different for each task, Comick, a deep learning method that infers the embedding based on the context and the morphological structure of the OOV word, obtained promising results.},
  doi       = {10.1007/978-3-030-61377-8_29},
  file      = {:PDF/2007.07318v2.pdf:PDF},
  isbn      = {978-3-030-61376-1},
  keywords  = {Out-of-vocabulary words, Machine learning, Natural language processing},
  location  = {Rio Grande, Brazil},
  numpages  = {17},
  url       = {https://doi.org/10.1007/978-3-030-61377-8_29},
}

@Article{Sabbeh2023WordEmbedding,
  author         = {Sabbeh, Sahar F. and Fasihuddin, Heba A.},
  journal        = {Electronics},
  title          = {A Comparative Analysis of Word Embedding and Deep Learning for Arabic Sentiment Classification},
  year           = {2023},
  issn           = {2079-9292},
  number         = {6},
  volume         = {12},
  abstract       = {Sentiment analysis on social media platforms (i.e., Twitter or Facebook) has become an important tool to learn about users’ opinions and preferences. However, the accuracy of sentiment analysis is disrupted by the challenges of natural language processing (NLP). Recently, deep learning models have proved superior performance over statistical- and lexical-based approaches in NLP-related tasks. Word embedding is an important layer of deep learning models to generate input features. Many word embedding models have been presented for text representation of both classic and context-based word embeddings. In this paper, we present a comparative analysis to evaluate both classic and contextualized word embeddings for sentiment analysis. The four most frequently used word embedding techniques were used in their trained and pre-trained versions. The selected embedding represents classical and contextualized techniques. Classical word embedding includes algorithms such as GloVe, Word2vec, and FastText. By contrast, ARBERT is used as a contextualized embedding model. Since word embedding is more typically employed as the input layer in deep networks, we used deep learning architectures BiLSTM and CNN for sentiment classification. To achieve these goals, the experiments were applied to a series of benchmark datasets: HARD, Khooli, AJGT, ArSAS, and ASTD. Finally, a comparative analysis was conducted on the results obtained for the experimented models. Our outcomes indicate that, generally, generated embedding by one technique achieves higher performance than its pretrained version for the same technique by around 0.28 to 1.8% accuracy, 0.33 to 2.17% precision, and 0.44 to 2% recall. Moreover, the contextualized transformer-based embedding model BERT achieved the highest performance in its pretrained and trained versions. Additionally, the results indicate that BiLSTM outperforms CNN by approximately 2% in 3 datasets, HARD, Khooli, and ArSAS, while CNN achieved around 2% higher performance in the smaller datasets, AJGT and ASTD.},
  article-number = {1425},
  doi            = {10.3390/electronics12061425},
  file           = {:PDF/electronics-12-01425.pdf:PDF},
  url            = {https://www.mdpi.com/2079-9292/12/6/1425},
}

@Article{Alshattnawi2024,
  author         = {Alshattnawi, Sawsan and Shatnawi, Amani and AlSobeh, Anas M.R. and Magableh, Aws A.},
  journal        = {Applied Sciences},
  title          = {Beyond Word-Based Model Embeddings: Contextualized Representations for Enhanced Social Media Spam Detection},
  year           = {2024},
  issn           = {2076-3417},
  number         = {6},
  volume         = {14},
  article-number = {2254},
  doi            = {10.3390/app14062254},
  file           = {:PDF/applsci-14-02254-v3.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/14/6/2254},
}

@Article{Dessi2021,
  author         = {Dessì, Danilo and Recupero, Diego Reforgiato and Sack, Harald},
  journal        = {Electronics},
  title          = {An Assessment of Deep Learning Models and Word Embeddings for Toxicity Detection within Online Textual Comments},
  year           = {2021},
  issn           = {2079-9292},
  number         = {7},
  volume         = {10},
  abstract       = {Today, increasing numbers of people are interacting online and a lot of textual comments are being produced due to the explosion of online communication. However, a paramount inconvenience within online environments is that comments that are shared within digital platforms can hide hazards, such as fake news, insults, harassment, and, more in general, comments that may hurt someone’s feelings. In this scenario, the detection of this kind of toxicity has an important role to moderate online communication. Deep learning technologies have recently delivered impressive performance within Natural Language Processing applications encompassing Sentiment Analysis and emotion detection across numerous datasets. Such models do not need any pre-defined hand-picked features, but they learn sophisticated features from the input datasets by themselves. In such a domain, word embeddings have been widely used as a way of representing words in Sentiment Analysis tasks, proving to be very effective. Therefore, in this paper, we investigated the use of deep learning and word embeddings to detect six different types of toxicity within online comments. In doing so, the most suitable deep learning layers and state-of-the-art word embeddings for identifying toxicity are evaluated. The results suggest that Long-Short Term Memory layers in combination with mimicked word embeddings are a good choice for this task.},
  article-number = {779},
  doi            = {10.3390/electronics10070779},
  file           = {:PDF/electronics-10-00779-v2.pdf:PDF},
  url            = {https://www.mdpi.com/2079-9292/10/7/779},
}

@Article{Kowsari2019,
  author         = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
  journal        = {Information},
  title          = {Text Classification Algorithms: A Survey},
  year           = {2019},
  issn           = {2078-2489},
  number         = {4},
  volume         = {10},
  abstract       = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
  article-number = {150},
  doi            = {10.3390/info10040150},
  url            = {https://www.mdpi.com/2078-2489/10/4/150},
}

@InBook{Robinson2014,
  author    = {Robinson, John},
  pages     = {3620--3621},
  publisher = {Springer Netherlands},
  title     = {Likert Scale},
  year      = {2014},
  address   = {Dordrecht},
  isbn      = {978-94-007-0753-5},
  booktitle = {Encyclopedia of Quality of Life and Well-Being Research},
  doi       = {10.1007/978-94-007-0753-5_1654},
  url       = {https://doi.org/10.1007/978-94-007-0753-5_1654},
}

@Article{Ondeng,
  author         = {Ondeng, Oscar and Ouma, Heywood and Akuon, Peter},
  journal        = {Applied Sciences},
  title          = {A Review of Transformer-Based Approaches for Image Captioning},
  year           = {2023},
  issn           = {2076-3417},
  number         = {19},
  volume         = {13},
  abstract       = {Visual understanding is a research area that bridges the gap between computer vision and natural language processing. Image captioning is a visual understanding task in which natural language descriptions of images are automatically generated using vision-language models. The transformer architecture was initially developed in the context of natural language processing and quickly found application in the domain of computer vision. Its recent application to the task of image captioning has resulted in markedly improved performance. In this paper, we briefly look at the transformer architecture and its genesis in attention mechanisms. We more extensively review a number of transformer-based image captioning models, including those employing vision-language pre-training, which has resulted in several state-of-the-art models. We give a brief presentation of the commonly used datasets for image captioning and also carry out an analysis and comparison of the transformer-based captioning models. We conclude by giving some insights into challenges as well as future directions for research in this area.},
  article-number = {11103},
  doi            = {10.3390/app131911103},
  file           = {:PDF/applsci-13-11103-v2.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/13/19/11103},
}

@Article{Song2023,
  author   = {Song, Binyang and Zhou, Rui and Ahmed, Faez},
  journal  = {Journal of Computing and Information Science in Engineering},
  title    = {Multi-Modal Machine Learning in Engineering Design: A Review and Future Directions},
  year     = {2023},
  issn     = {1530-9827},
  month    = {11},
  number   = {1},
  pages    = {010801},
  volume   = {24},
  abstract = {In the rapidly advancing field of multi-modal machine learning (MMML), the convergence of multiple data modalities has the potential to reshape various applications. This paper presents a comprehensive overview of the current state, advancements, and challenges of MMML within the sphere of engineering design. The review begins with a deep dive into five fundamental concepts of MMML: multi-modal information representation, fusion, alignment, translation, and co-learning. Following this, we explore the cutting-edge applications of MMML, placing a particular emphasis on tasks pertinent to engineering design, such as cross-modal synthesis, multi-modal prediction, and cross-modal information retrieval. Through this comprehensive overview, we highlight the inherent challenges in adopting MMML in engineering design, and proffer potential directions for future research. To spur on the continued evolution of MMML in engineering design, we advocate for concentrated efforts to construct extensive multi-modal design datasets, develop effective data-driven MMML techniques tailored to design applications, and enhance the scalability and interpretability of MMML models. MMML models, as the next generation of intelligent design tools, hold a promising future to impact how products are designed.},
  doi      = {10.1115/1.4063954},
  eprint   = {https://asmedigitalcollection.asme.org/computingengineering/article-pdf/24/1/010801/7062966/jcise\_24\_1\_010801.pdf},
  file     = {:PDF/2302.10909v2.pdf:PDF},
  group    = {review},
  url      = {https://doi.org/10.1115/1.4063954},
}

@Article{Agarwal2023,
  author  = {Agarwal, Lakshita and Verma, Bindu},
  journal = {Multimedia Tools and Applications},
  title   = {From methods to datasets: A survey on Image-Caption Generators},
  year    = {2023},
  month   = {08},
  pages   = {1-47},
  volume  = {83},
  doi     = {10.1007/s11042-023-16560-x},
  file    = {:PDF/s11042-023-16560-x-1.pdf:PDF},
  groups  = {review},
}

@Article{article,
  author  = {Arshi, Oroos and Dadure, Pankaj},
  journal = {Multimedia Tools and Applications},
  title   = {A comprehensive review of image caption generation},
  year    = {2024},
  month   = {10},
  pages   = {1-53},
  doi     = {10.1007/s11042-024-20095-0},
}

@Article{Ghandi2023,
  author     = {Ghandi, Taraneh and Pourreza, Hamidreza and Mahyar, Hamidreza},
  journal    = {ACM Comput. Surv.},
  title      = {Deep Learning Approaches on Image Captioning: A Review},
  year       = {2023},
  issn       = {0360-0300},
  month      = oct,
  number     = {3},
  volume     = {56},
  abstract   = {Image captioning is a research area of immense importance, aiming to generate natural language descriptions for visual content in the form of still images. The advent of deep learning and more recently vision-language pre-training techniques has revolutionized the field, leading to more sophisticated methods and improved performance. In this survey article, we provide a structured review of deep learning methods in image captioning by presenting a comprehensive taxonomy and discussing each method category in detail. Additionally, we examine the datasets commonly employed in image captioning research, as well as the evaluation metrics used to assess the performance of different captioning models. We address the challenges faced in this field by emphasizing issues such as object hallucination, missing context, illumination conditions, contextual understanding, and referring expressions. We rank different deep learning methods’ performance according to widely used evaluation metrics, giving insight into the current state-of-the-art. Furthermore, we identify several potential future directions for research in this area, which include tackling the information misalignment problem between image and text modalities, mitigating dataset bias, incorporating vision-language pre-training methods to enhance caption generation, and developing improved evaluation tools to accurately measure the quality of image captions.},
  address    = {New York, NY, USA},
  articleno  = {62},
  doi        = {10.1145/3617592},
  groups     = {review},
  issue_date = {March 2024},
  keywords   = {text generation, deep learning, Image captioning},
  numpages   = {39},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3617592},
}

@Article{Wei2020,
  author     = {Wei Chen and Weiping Wang and Li Liu and Michael S. Lew},
  journal    = {CoRR},
  title      = {New Ideas and Trends in Deep Multimodal Content Understanding: {A} Review},
  year       = {2020},
  volume     = {abs/2010.08189},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2010-08189.bib},
  eprint     = {2010.08189},
  eprinttype = {arXiv},
  file       = {:PDF/2010.08189v1.pdf:PDF},
  groups     = {review},
  timestamp  = {Thu, 03 Nov 2022 08:31:05 +0100},
  url        = {https://arxiv.org/abs/2010.08189},
}

@Article{10.1145/3545572,
  author     = {Jabeen, Summaira and Li, Xi and Amin, Muhammad Shoib and Bourahla, Omar and Li, Songyuan and Jabbar, Abdul},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {A Review on Methods and Applications in Multimodal Deep Learning},
  year       = {2023},
  issn       = {1551-6857},
  month      = feb,
  number     = {2s},
  volume     = {19},
  abstract   = {Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.},
  address    = {New York, NY, USA},
  articleno  = {76},
  doi        = {10.1145/3545572},
  file       = {:PDF/2202.09195v1.pdf:PDF},
  groups     = {review},
  issue_date = {April 2023},
  keywords   = {Deep learning, multimedia, multimodal learning, datasets, neural networks, survey},
  numpages   = {41},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3545572},
}

@Article{Ming2022VisualsToText,
  author   = {Ming, Yue and Hu, Nannan and Fan, Chunxiao and Feng, Fan and Zhou, Jiangwan and Yu, Hui},
  journal  = {IEEE/CAA Journal of Automatica Sinica},
  title    = {Visuals to Text: A Comprehensive Review on Automatic Image Captioning},
  year     = {2022},
  issn     = {2329-9274},
  month    = {August},
  number   = {8},
  pages    = {1339-1365},
  volume   = {9},
  abstract = {Image captioning refers to automatic generation of descriptive texts according to the visual content of images. It is a technique integrating multiple disciplines including the computer vision (CV), natural language processing (NLP) and artificial intelligence. In recent years, substantial research efforts have been devoted to generate image caption with impressive progress. To summarize the recent advances in image captioning, we present a comprehensive review on image captioning, covering both traditional methods and recent deep learning-based techniques. Specifically, we first briefly review the early traditional works based on the retrieval and template. Then deep learning-based image captioning researches are focused, which is categorized into the encoder-decoder framework, attention mechanism and training strategies on the basis of model structures and training manners for a detailed introduction. After that, we summarize the publicly available datasets, evaluation metrics and those proposed for specific requirements, and then compare the state of the art methods on the MS COCO dataset. Finally, we provide some discussions on open challenges and future research directions.},
  doi      = {10.1109/JAS.2022.105734},
  file     = {:PDF/A_Comprehensive_Review_on_Automatic_Image_Captioning.pdf:PDF},
  groups   = {review, wazne_wazne},
  keywords = {Measurement;Training;Deep learning;Visualization;Computer vision;Information processing;Natural language processing;Artificial intelligence;attention mechanism;encoder-decoder framework;image captioning;multi-modal understanding;training strategies},
  priority = {prio1},
}

@InProceedings{Hodosh2013Senetence,
  author    = {Hodosh, Micah and Hockenmaier, Julia},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {Sentence-Based Image Description with Scalable, Explicit Models},
  year      = {2013},
  month     = {June},
  pages     = {294-300},
  abstract  = {Associating photographs with complete sentences that describe what is depicted in them is a challenging problem. This paper examines how an approach that is inspired by image tagging techniques which can scale to very large data sets performs on this much harder task, and examines some of the linguistic difficulties that this bag-of-words model faces.},
  doi       = {10.1109/CVPRW.2013.51},
  file      = {:PDF/Hodosh_Sentence-Based_Image_Description_2013_CVPR_paper.pdf:PDF},
  groups    = {retrieval},
  issn      = {2160-7516},
  keywords  = {Training;Kernel;Joints;Computational modeling;Measurement;Visualization;Training data},
}

@InProceedings{Kuznetsova2012Collective,
  author    = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Alexander and Berg, Tamara and Choi, Yejin},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Collective Generation of Natural Image Descriptions},
  year      = {2012},
  address   = {Jeju Island, Korea},
  editor    = {Li, Haizhou and Lin, Chin-Yew and Osborne, Miles and Lee, Gary Geunbae and Park, Jong C.},
  month     = jul,
  pages     = {359--368},
  publisher = {Association for Computational Linguistics},
  file      = {:PDF/P12-1038.pdf:PDF},
  groups    = {retrieval},
  url       = {https://aclanthology.org/P12-1038/},
}

@InProceedings{Gupta2012Choosing,
  author    = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C. V.},
  booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
  title     = {Choosing linguistics over vision to describe images},
  year      = {2012},
  pages     = {606–612},
  publisher = {AAAI Press},
  series    = {AAAI'12},
  abstract  = {In this paper, we address the problem of automatically generating human-like descriptions for unseen images, given a collection of images and their corresponding human-generated descriptions. Previous attempts for this task mostly rely on visual clues and corpus statistics, but do not take much advantage of the semantic information inherent in the available image descriptions. Here, we present a generic method which benefits from all these three sources (i. e. visual clues, corpus statistics and available descriptions) simultaneously, and is capable of constructing novel descriptions. Our approach works on syntactically and linguistically motivated phrases extracted from the human descriptions. Experimental evaluations demonstrate that our formulation mostly generates lucid and semantically correct descriptions, and significantly outperforms the previous methods on automatic evaluation metrics. One of the significant advantages of our approach is that we can generate multiple interesting descriptions for an image. Unlike any previous work, we also test the applicability of our method on a large dataset containing complex images with rich descriptions.},
  file      = {:PDF/8205-Article Text-11732-1-2-20201228-2.pdf:PDF},
  groups    = {retrieval},
  location  = {Toronto, Ontario, Canada},
  numpages  = {7},
}

@InProceedings{Ma2015,
  author    = {Ma, Lin and Lu, Zhengdong and Shang, Lifeng and Li, Hang},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Multimodal Convolutional Neural Networks for Matching Image and Sentence},
  year      = {2015},
  month     = {Dec},
  pages     = {2623-2631},
  abstract  = {In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content and one matching CNN modeling the joint representation of image and sentence. The matching CNN composes different semantic fragments from words and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. More specifically, our proposed m-CNNs significantly outperform the state-of-the-art approaches for bidirectional image and sentence retrieval on the Flickr8K and Flickr30K datasets.},
  doi       = {10.1109/ICCV.2015.301},
  file      = {:PDF/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf:PDF},
  groups    = {Earlier Deep Models},
  issn      = {2380-7504},
  keywords  = {Convolution;Image representation;Semantics;Neural networks;Computer architecture;Natural languages;Grounding},
}

@InProceedings{Yan2015Deep,
  author    = {Yan, Fei and Mikolajczyk, Krystian},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep correlation for matching images and text},
  year      = {2015},
  month     = {June},
  pages     = {3441-3450},
  abstract  = {This paper addresses the problem of matching images and captions in a joint latent space learnt with deep canonical correlation analysis (DCCA). The image and caption data are represented by the outputs of the vision and text based deep neural networks. The high dimensionality of the features presents a great challenge in terms of memory and speed complexity when used in DCCA framework. We address these problems by a GPU implementation and propose methods to deal with overfitting. This makes it possible to evaluate DCCA approach on popular caption-image matching benchmarks. We compare our approach to other recently proposed techniques and present state of the art results on three datasets.},
  doi       = {10.1109/CVPR.2015.7298966},
  file      = {:PDF/Yan_Deep_Correlation_for_2015_CVPR_paper.pdf:PDF},
  groups    = {Earlier Deep Models},
  issn      = {1063-6919},
  keywords  = {Correlation;Yttrium;Graphics processing units;Protocols;Training;Libraries;Visualization},
}

@InProceedings{Yagcioglu2015Distributed,
  author    = {Yagcioglu, Semih and Erdem, Erkut and Erdem, Aykut and Cakici, Ruket},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  title     = {A Distributed Representation Based Query Expansion Approach for Image Captioning},
  year      = {2015},
  address   = {Beijing, China},
  editor    = {Zong, Chengqing and Strube, Michael},
  month     = jul,
  pages     = {106--111},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/P15-2018},
  file      = {:PDF/P15-2018.pdf:PDF},
  groups    = {Earlier Deep Models},
  url       = {https://aclanthology.org/P15-2018/},
}

@InProceedings{Pu2016,
  author    = {Pu, Yunchen and Yuan, Win and Stevens, Andrew and Li, Chunyuan and Carin, Lawrence},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  title     = {A Deep Generative Deconvolutional Image Model},
  year      = {2016},
  address   = {Cadiz, Spain},
  editor    = {Gretton, Arthur and Robert, Christian C.},
  month     = {09--11 May},
  pages     = {741--750},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {51},
  abstract  = {A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic unpooling is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The algorithm is efficiently trained via Monte Carlo expectation-maximization (MCEM), with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.},
  file      = {:PDF/pu16.pdf:PDF},
  groups    = {encoder-decoder-lit},
  pdf       = {http://proceedings.mlr.press/v51/pu16.pdf},
  url       = {https://proceedings.mlr.press/v51/pu16.html},
}

@InProceedings{Gan2017StyleNet,
  author    = {Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {StyleNet: Generating Attractive Visual Captions with Styles},
  year      = {2017},
  month     = {July},
  pages     = {955-964},
  abstract  = {We propose a novel framework named StyleNet to address the task of generating attractive captions for images and videos with different styles. To this end, we devise a novel model component, named factored LSTM, which automatically distills the style factors in the monolingual text corpus. Then at runtime, we can explicitly control the style in the caption generation process so as to produce attractive visual captions with the desired style. Our approach achieves this goal by leveraging two sets of data: 1) factual image/video-caption paired data, and 2) stylized monolingual text data (e.g., romantic and humorous sentences). We show experimentally that StyleNet outperforms existing approaches for generating visual captions with different styles, measured in both automatic and human evaluation metrics on the newly collected FlickrStyle10K image caption dataset, which contains 10K Flickr images with corresponding humorous and romantic captions.},
  doi       = {10.1109/CVPR.2017.108},
  file      = {:PDF/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf:PDF},
  groups    = {encoder-decoder-lit},
  issn      = {1063-6919},
  keywords  = {Visualization;Training;Dogs;Flickr;Logic gates;Semantics},
}

@InProceedings{Hao2018,
  author    = {Hao, Yanlong and Xie, Jiyang and Lin, Zhiqing},
  booktitle = {2018 International Conference on Network Infrastructure and Digital Content (IC-NIDC)},
  title     = {Image Caption via Visual Attention Switch on DenseNet},
  year      = {2018},
  month     = {Aug},
  pages     = {334-338},
  abstract  = {We introduce a novel approach that is used to convert images into the corresponding language descriptions. This method follows the most popular encoder-decoder architecture. The encoder uses the recently proposed densely convolutional neural network (DenseNet) to extract the feature maps. Meanwhile, the decoder uses the long short time memory (LSTM) to parse the feature maps to descriptions. We predict the next word of descriptions by taking the effective combination of feature maps with word embedding of current input word by “visual attention switch”. Finally, we compare the performance of the proposed model with other baseline models and achieve good results.},
  doi       = {10.1109/ICNIDC.2018.8525732},
  file      = {:PDF/Image_Caption_via_Visual_Attention_Switch_on_DenseNet.pdf:PDF},
  groups    = {encoder-decoder-lit},
  issn      = {2575-4955},
  keywords  = {Feature extraction;Decoding;Visualization;Switches;Training;Dictionaries;Data models;Image caption;Visual attention switch;Encoder-decoder architecture;DenseNet;LSTM},
}

@InProceedings{Tran2016,
  author    = {Tran, Kenneth and He, Xiaodong and Zhang, Lei and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Rich Image Captioning in the Wild},
  year      = {2016},
  month     = {June},
  pages     = {434-441},
  abstract  = {We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include generating high quality caption with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out-of-domain datasets. We also make the system publicly accessible as a part of the Microsoft Cognitive Services.},
  doi       = {10.1109/CVPRW.2016.61},
  file      = {:PDF/Tran_Rich_Image_Captioning_CVPR_2016_paper.pdf:PDF},
  groups    = {Compositional architectures},
  issn      = {2160-7516},
  keywords  = {Training;Visualization;Face;Knowledge based systems;Semantics;Data models;Neural networks},
}

@InProceedings{Ma2016,
  author    = {Ma, Shubo and Han, Yahong},
  booktitle = {2016 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Describing images by feeding LSTM with structural words},
  year      = {2016},
  month     = {July},
  pages     = {1-6},
  abstract  = {Generating semantic description draws increasing attention recently. Describing objects with adaptive adjunct words make the sentence more informative. In this paper, we focus on the generation of descriptions for images according to the structural words we have generated such as a tetrad of <;object, attribute, activity, scene>. We propose to use deep machine translation method to generate semantically meaningful descriptions. In particular, the description is composed of objects with appropriate adjunct words, corresponding activities and scene. We propose to use a multi-task method to generate structural words. Taking these words sequence as source language, we train a LSTM encoder-decoder machine translation model to output the target language. Experiments on the benchmark datasets demonstrate our method has better performance than state-of-the-art methods of image caption in terms of language generation metrics.},
  doi       = {10.1109/ICME.2016.7552883},
  file      = {:PDF/07552883.pdf:PDF},
  groups    = {Compositional architectures},
  issn      = {1945-788X},
  keywords  = {Logic gates;Feature extraction;Measurement;Recurrent neural networks;Training;Hidden Markov models;Visualization;Image description;structural words;LSTM;machine translation},
}

@InProceedings{Oruganti2016,
  author    = {Oruganti, Ram Manohar and Sah, Shagan and Pillai, Suhas and Ptucha, Raymond},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  title     = {Image description through fusion based recurrent multi-modal learning},
  year      = {2016},
  month     = {Sep.},
  pages     = {3613-3617},
  abstract  = {Current research in computer vision and machine learning has demonstrated some great abilities at detecting and recognizing objects in natural images. The promising results in these areas have inspired research towards solving more complex multi-modal learning problems in the image/video domains such as automatic annotation, segmentation, labelling, and generic understanding. Although solutions have been provided for one or more of these problems, their approaches have been application specific. This paper introduces an end-to-end trainable Fusion-based Recurrent Multi-Modal (FRMM) model to address multi-modal applications. FRMM allows each input modality to be independent in terms of architecture, parameters and length of input sequences. FRMM image description models seamlessly blend convolutional neural network feature descriptors with sequential language data in a recurrent framework. For training and testing we used the Flickr30K and MSCOCO datasets, demonstrating state-of-the-art description results.},
  doi       = {10.1109/ICIP.2016.7533033},
  file      = {:PDF/Image_description_through_fusion_based_recurrent_multi-modal_learning.pdf:PDF},
  groups    = {Compositional architectures},
  issn      = {2381-8549},
  keywords  = {Decision support systems;Computer vision;Handheld computers;Pattern recognition;Conferences;Indexes;Computational linguistics;Image Captioning;CNN;LSTM;Recurrent Neural Networks},
}

@InProceedings{Wang2016,
  author    = {Wang, Minsi and Song, Li and Yang, Xiaokang and Luo, Chuanfei},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  title     = {A parallel-fusion RNN-LSTM architecture for image caption generation},
  year      = {2016},
  month     = {Sep.},
  pages     = {4448-4452},
  abstract  = {The models based on deep convolutional networks and recurrent neural networks have dominated in recent image caption generation tasks. Performance and complexity are still eternal topic. Inspired by recent work, by combining the advantages of simple RNN and LSTM, we present a novel parallel-fusion RNN-LSTM architecture, which obtains better results than a dominated one and improves the efficiency as well. The proposed approach divides the hidden units of RNN into several same-size parts, and lets them work in parallel. Then, we merge their outputs with corresponding ratios to generate final results. Moreover, these units can be different types of RNNs, for instance, a simple RNN and a LSTM. By training normally using NeuralTalk1 platform on Flickr8k dataset, without additional training data, we get better results than that of dominated structure and particularly, the proposed model surpass GoogleNIC in image caption generation.},
  doi       = {10.1109/ICIP.2016.7533201},
  file      = {:PDF/A_parallel-fusion_RNN-LSTM_architecture_for_image_caption_generation.pdf:PDF},
  groups    = {Compositional architectures},
  issn      = {2381-8549},
  keywords  = {Training;Computational modeling;Feature extraction;Measurement;Data models;Recurrent neural networks;Image captioning;deep neural network;RNN;LSTM},
}

@Article{Fu2017,
  author   = {Fu, Kun and Jin, Junqi and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts},
  year     = {2017},
  issn     = {1939-3539},
  month    = {Dec},
  number   = {12},
  pages    = {2321-2334},
  volume   = {39},
  abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image captioning system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifts among the visual regions—such transitions impose a thread of ordering in visual perception. This alignment characterizes the flow of latent meaning, which encodes what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets, using both automatic evaluation metrics and human evaluation. We show that either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
  doi      = {10.1109/TPAMI.2016.2642953},
  file     = {:PDF/Aligning_Where_to_See_and_What_to_Tell_Image_Captioning_with_Region-Based_Attention_and_Scene-Specific_Contexts.pdf:PDF},
  groups   = {Compositional architectures},
  keywords = {Visualization;Feature extraction;Image classification;Context modeling;Adaptation models;Computational modeling;Data mining;Image captioning;visual attention;scene-specific context;LSTM},
}

@InProceedings{Dai2017Diverse,
  author    = {Dai, Bo and Fidler, Sanja and Urtasun, Raquel and Lin, Dahua},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {{ Towards Diverse and Natural Image Descriptions via a Conditional GAN }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = Oct,
  pages     = {2989-2998},
  publisher = {IEEE Computer Society},
  abstract  = {Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect. Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the “ground-truth” captions, while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity - two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.},
  doi       = {10.1109/ICCV.2017.323},
  file      = {:PDF/Dai_Towards_Diverse_and_ICCV_2017_paper.pdf:PDF},
  groups    = {Other deep learning methods},
  issn      = {2380-7504},
  keywords  = {Measurement;Buildings;Training;Cows;Gallium nitride;Generators;Visualization},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.323},
}

@Misc{zhang2017actorcriticsequencetrainingimage,
  author        = {Li Zhang and Flood Sung and Feng Liu and Tao Xiang and Shaogang Gong and Yongxin Yang and Timothy M. Hospedales},
  title         = {Actor-Critic Sequence Training for Image Captioning},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1706.09601},
  file          = {:PDF/ac_nips2017.pdf:PDF},
  groups        = {Other deep learning methods},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1706.09601},
}

@Misc{Anderson2018partiallysupervisedimagecaptioning,
  author        = {Peter Anderson and Stephen Gould and Mark Johnson},
  title         = {Partially-Supervised Image Captioning},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1806.06004},
  file          = {:PDF/NeurIPS-2018-partially-supervised-image-captioning-Paper.pdf:PDF},
  groups        = {Other deep learning methods},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1806.06004},
}

@Misc{Li2019generatingdiverseaccuratevisual,
  author        = {Dianqi Li and Qiuyuan Huang and Xiaodong He and Lei Zhang and Ming-Ting Sun},
  title         = {Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1804.00861},
  file          = {:PDF/27.pdf:PDF},
  groups        = {Other deep learning methods},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1804.00861},
}

@InProceedings{Li2018SemanticLSTM,
  author    = {Li, Nannan and Chen, Zhenzhong},
  booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  title     = {Image captioning with visual-semantic LSTM},
  year      = {2018},
  pages     = {793–799},
  publisher = {AAAI Press},
  series    = {IJCAI'18},
  abstract  = {In this paper, a novel image captioning approach is proposed to describe the content of images. Inspired by the visual processing of our cognitive system, we propose a visual-semantic LSTM model to locate the attention objects with their low-level features in the visual cell, and then successively extract high-level semantic features in the semantic cell. In addition, a state perturbation term is introduced to the word sampling strategy in the REINFORCE based method to explore proper vocabularies in the training process. Experimental results on MS COCO and Flickr30K validate the effectiveness of our approach when compared to the state-of-the-art methods.},
  file      = {:PDF/0110.pdf:PDF},
  groups    = {Other deep learning methods},
  isbn      = {9780999241127},
  location  = {Stockholm, Sweden},
  numpages  = {7},
}

@InProceedings{Lindh2018Diverse,
  author    = {Lindh, Annika and Ross, Robert J. and Mahalunkar, Abhijit and Salton, Giancarlo and Kelleher, John D.},
  booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2018},
  title     = {Generating Diverse and Meaningful Captions},
  year      = {2018},
  address   = {Cham},
  editor    = {K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  pages     = {176--187},
  publisher = {Springer International Publishing},
  abstract  = {Image Captioning is a task that requires models to acquire a multimodal understanding of the world and to express this understanding in natural language text. While the state-of-the-art for this task has rapidly improved in terms of n-gram metrics, these models tend to output the same generic captions for similar images. In this work, we address this limitation and train a model that generates more diverse and specific captions through an unsupervised training approach that incorporates a learning signal from an Image Retrieval model. We summarize previous results and improve the state-of-the-art on caption diversity and novelty. We make our source code publicly available online (https://github.com/AnnikaLindh/Diverse{\_}and{\_}Specific{\_}Image{\_}Captioning).},
  file      = {:PDF/aics_38.pdf:PDF},
  groups    = {Other deep learning methods},
  isbn      = {978-3-030-01418-6},
}

@InProceedings{Gong2014,
  author    = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {529--545},
  publisher = {Springer International Publishing},
  abstract  = {This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of images that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.},
  file      = {:PDF/yunchao_eccv14_sentence.pdf:PDF},
  groups    = {retrieval},
  isbn      = {978-3-319-10593-2},
}

@InProceedings{Devlin2015Language,
  author    = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  title     = {Language Models for Image Captioning: The Quirks and What Works},
  year      = {2015},
  address   = {Beijing, China},
  editor    = {Zong, Chengqing and Strube, Michael},
  month     = jul,
  pages     = {100--105},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/P15-2017},
  file      = {:PDF/P15-2017.pdf:PDF},
  groups    = {retrieval},
  url       = {https://aclanthology.org/P15-2017/},
}

@InProceedings{1Chen2017,
  author    = {Chen, Fuhai and Ji, Rongrong and Su, Jinsong and Wu, Yongjian and Wu, Yunsheng},
  booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
  title     = {StructCap: Structured Semantic Embedding for Image Captioning},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {46–54},
  publisher = {Association for Computing Machinery},
  series    = {MM '17},
  abstract  = {Image captioning has attracted ever-increasing research attention in multimedia and computer vision. To encode the visual content, existing approaches typically utilize the off-the-shelf deep Convolutional Neural Network (CNN) model to extract visual features, which are sent to Recurrent Neural Network (RNN) based textual generators to output word sequence. Some methods encode visual objects and scene information with attention mechanism more recently. Despite the promising progress, one distinct disadvantage lies in distinguishing and modeling key semantic entities and their relations, which are in turn widely regarded as the important cues for us to describe image content. In this paper, we propose a novel image captioning model, termed StructCap. It parses a given image into key entities and their relations organized in a visual parsing tree, which is transformed and embedded under an encoder-decoder framework via visual attention. We give an end-to-end formulation to facilitate joint training of visual tree parser, structured semantic attention and RNN-based captioning modules. Experimental results on two public benchmarks, Microsoft COCO and Flickr30K, show that the proposed StructCap model outperforms the state-of-the-art approaches under various standard evaluation metrics.},
  doi       = {10.1145/3123266.3123275},
  file      = {:PDF/3123266.3123275.pdf:PDF},
  groups    = {global CNN features},
  isbn      = {9781450349062},
  keywords  = {visual relation, structured learning, image captioning, deep learning},
  location  = {Mountain View, California, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3123266.3123275},
}

@InProceedings{Chen2018GroupCap,
  author    = {Chen, Fuhai and Ji, Rongrong and Sun, Xiaoshuai and Wu, Yongjian and Su, Jinsong},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {GroupCap: Group-Based Image Captioning with Structured Relevance and Diversity Constraints},
  year      = {2018},
  month     = {June},
  pages     = {1345-1353},
  abstract  = {Most image captioning models focus on one-line (single image) captioning, where the correlations like relevance and diversity among group images (e.g., within the same album or event) are simply neglected, resulting in less accurate and diverse captions. Recent works mainly consider imposing the diversity during the online inference only, which neglect the correlation among visual structures in offline training. In this paper, we propose a novel group-based image captioning scheme (termed GroupCap), which jointly models the structured relevance and diversity among group images towards an optimal collaborative captioning. In particular, we first propose a visual tree parser (VP-Tree) to construct the structured semantic correlations within individual images. Then, the relevance and diversity among images are well modeled by exploiting the correlations among their tree structures. Finally, such correlations are modeled as constraints and sent into the LSTM-based captioning generator. We adopt an end-to-end formulation to train the visual tree parser, the structured relevance and diversity constraints, as well as the LSTM based captioning model jointly. To facilitate quantitative evaluation, we further release two group captioning datasets derived from the MS-COCO benchmark, serving as the first of their kind. Quantitative results show that the proposed GroupCap model outperforms the state-of-the-art and alternative approaches.},
  doi       = {10.1109/CVPR.2018.00146},
  file      = {:PDF/GroupCap_Group-Based_Image_Captioning_with_Structured_Relevance_and_Diversity_Constraints.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Visualization;Correlation;Semantics;Feature extraction;Training;Adaptation models;Task analysis},
}

@InProceedings{Ge2019Exploring,
  author    = {Ge, Hongwei and Yan, Zehang and Zhang, Kai and Zhao, Mingde and Sun, Liang},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {{ Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style }},
  year      = {2019},
  address   = {Los Alamitos, CA, USA},
  month     = Nov,
  pages     = {1754-1763},
  publisher = {IEEE Computer Society},
  abstract  = {Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results.},
  doi       = {10.1109/ICCV.2019.00184},
  file      = {:PDF/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf:PDF},
  groups    = {Additive attention over a grid of features, hidden state reconstruction},
  keywords  = {Feature extraction;Semantics;Visualization;Decoding;Training;Computational modeling;Cognition},
  url       = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00184},
}

@InProceedings{Cornia2020SMART,
  author    = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability},
  year      = {2020},
  month     = {May},
  pages     = {1128-1134},
  abstract  = {The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.},
  doi       = {10.1109/ICRA40945.2020.9196653},
  file      = {:PDF/SMArT_Training_Shallow_Memory-aware_Transformers_for_Robotic_Explainability.pdf:PDF},
  groups    = {Self-Attention Encoding},
  issn      = {2577-087X},
  keywords  = {Decoding;Computational modeling;Visualization;Magnetic heads;Robots;Natural languages;Encoding},
}

@Article{Luo2021DualLevelCT,
  author  = {Yunpeng Luo and Jiayi Ji and Xiaoshuai Sun and Liujuan Cao and Yongjian Wu and Feiyue Huang and Chia-Wen Lin and Rongrong Ji},
  journal = {ArXiv},
  title   = {Dual-Level Collaborative Transformer for Image Captioning},
  year    = {2021},
  volume  = {abs/2101.06462},
  file    = {:PDF/16328-13-19822-1-2-20210518.pdf:PDF},
  groups  = {Other, Transformer},
  url     = {https://api.semanticscholar.org/CorpusID:231632752},
}

@InProceedings{Zhang2021RSTNet,
  author    = {Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words},
  year      = {2021},
  month     = {June},
  pages     = {15460-15469},
  abstract  = {Recent progress on visual question answering has explored the merits of grid features for vision language tasks. Meanwhile, transformer-based models have shown remarkable performance in various sequence prediction problems. However, the spatial information loss of grid features caused by flattening operation, as well as the defect of the transformer model in distinguishing visual words and non visual words, are still left unexplored. In this paper, we first propose Grid-Augmented (GA) module, in which relative geometry features between grids are incorporated to enhance visual representations. Then, we build a BERT-based language model to extract language context and propose Adaptive-Attention (AA) module on top of a transformer decoder to adaptively measure the contribution of visual and language cues before making decisions for word prediction. To prove the generality of our proposals, we apply the two modules to the vanilla transformer model to build our Relationship-Sensitive Transformer (RSTNet) for image captioning task. The proposed model is tested on the MSCOCO benchmark, where it achieves new state-of-art results on both the Karpathy test split and the online test server. Source code is available at GitHub 1.},
  doi       = {10.1109/CVPR46437.2021.01521},
  file      = {:PDF/RSTNet_Captioning_with_Adaptive_Attention_on_Visual_and_Non-Visual_Words.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Geometry;Visualization;Adaptation models;Predictive models;Transformers;Time measurement;Servers},
}

@InProceedings{Touvron2021Training,
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Training data-efficient image transformers distillation through attention},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {10347--10357},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
  file      = {:PDF/touvron21a.pdf:PDF},
  groups    = {Vision Transformer.},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
}

@InProceedings{Zhang2021Revisiting,
  author    = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ VinVL: Revisiting Visual Representations in Vision-Language Models }},
  year      = {2021},
  address   = {Los Alamitos, CA, USA},
  month     = Jun,
  pages     = {5575-5584},
  publisher = {IEEE Computer Society},
  abstract  = {This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [20], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL.},
  doi       = {10.1109/CVPR46437.2021.00553},
  file      = {:PDF/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf:PDF},
  groups    = {Early fusion and vision-and-language pre-training., BERT},
  keywords  = {Training;Visualization;Computational modeling;Object detection;Benchmark testing;Feature extraction;Transformers},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00553},
}

@Misc{gao2019maskednonautoregressiveimagecaptioning,
  author        = {Junlong Gao and Xi Meng and Shiqi Wang and Xia Li and Shanshe Wang and Siwei Ma and Wen Gao},
  title         = {Masked Non-Autoregressive Image Captioning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1906.00717},
  file          = {:PDF/1906.00717v1.pdf:PDF},
  groups        = {Non-autoregressive Language Models},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1906.00717},
}

@Misc{fei2019fastimagecaptiongeneration,
  author        = {Zheng-cong Fei},
  title         = {Fast Image Caption Generation with Position Alignment},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1912.06365},
  file          = {:PDF/1912.06365v1.pdf:PDF},
  groups        = {Non-autoregressive Language Models},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1912.06365},
}

@InProceedings{Guo2021NonAutoregresive,
  author    = {Guo, Longteng and Liu, Jing and Zhu, Xinxin and He, Xingjian and Jiang, Jie and Lu, Hanqing},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  title     = {Non-autoregressive image captioning with counterfactuals-critical multi-agent learning},
  year      = {2021},
  series    = {IJCAI'20},
  abstract  = {Most image captioning models are autoregressive, i.e. they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentencelevel consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a Non-Autoregressive Image Captioning (NAIC) model with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAIC as a multi-agent reinforcement learning system where positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. Besides, we propose to utilize massive unlabeled images to boost captioning performance. Extensive experiments on MSCOCO image captioning benchmark show that our NAIC model achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9\texttimes{} decoding speedup.},
  articleno = {107},
  file      = {:PDF/3491440.3491547.pdf:PDF},
  groups    = {Non-autoregressive Language Models},
  isbn      = {9780999241165},
  location  = {Yokohama, Yokohama, Japan},
  numpages  = {7},
}

@InProceedings{Fei2020,
  author    = {Fei, Zhengcong},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  title     = {Iterative Back Modification for Faster Image Captioning},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {3182–3190},
  publisher = {Association for Computing Machinery},
  series    = {MM '20},
  abstract  = {Current state-of-the-art image captioning systems generally produce a sentence from left to right, and every step is conditioned on the given image and previously generated words. Nevertheless, such autoregressive nature makes the inference process difficult to parallelize and leads to high captioning latency. In this paper, we propose a non-autoregressive approach for faster image caption generation. Technically, low-dimension continuous latent variables are shaped to capture semantic information and word dependencies from extracted image features before sentence decoding. Moreover, we develop an iterative back modification inference algorithm, which continuously refines the latent variables with a look back mechanism and parallelly generates the whole sentence based on the updated latent variables in a constant number of steps. Extensive experiments demonstrate that our method achieves competitive performance compared to prevalent autoregressive captioning models while significantly reducing the decoding time on average.},
  doi       = {10.1145/3394171.3413901},
  file      = {:PDF/3394171.3413901.pdf:PDF},
  groups    = {Non-autoregressive Language Models},
  isbn      = {9781450379885},
  keywords  = {image captioning, iterative back modification, latent variables, non-autoregressive generation},
  location  = {Seattle, WA, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3394171.3413901},
}

@Misc{guo2021fastsequencegenerationmultiagent,
  author        = {Longteng Guo and Jing Liu and Xinxin Zhu and Hanqing Lu},
  title         = {Fast Sequence Generation with Multi-Agent Reinforcement Learning},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2101.09698},
  file          = {:PDF/2101.09698v1.pdf:PDF},
  groups        = {Non-autoregressive Language Models},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2101.09698},
}

@InProceedings{Chen2017SCACNN,
  author    = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = Jul,
  pages     = {6298-6306},
  publisher = {IEEE Computer Society},
  abstract  = {Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism - a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.},
  doi       = {10.1109/CVPR.2017.667},
  file      = {:PDF/2101.09698v1.pdf:PDF;:PDF/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf:PDF},
  groups    = {single layer LSTM, Multi-level features},
  issn      = {1063-6919},
  keywords  = {Visualization;Semantics;Feature extraction;Image coding;Neural networks;Detectors},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.667},
}

@InProceedings{Ramanishka2017TopDown,
  author    = {Ramanishka, Vasili and Das, Abir and Zhang, Jianming and Saenko, Kate},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Top-Down Visual Saliency Guided by Captions }},
  year      = {2017},
  address   = {Los Alamitos, CA, USA},
  month     = Jul,
  pages     = {3135-3144},
  publisher = {IEEE Computer Society},
  abstract  = {Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/.},
  doi       = {10.1109/CVPR.2017.334},
  file      = {:PDF/Top-Down_Visual_Saliency_Guided_by_Captions.pdf:PDF},
  groups    = {Exploiting human attention},
  issn      = {1063-6919},
  keywords  = {Visualization;Decoding;Computational modeling;Heating systems;Analytical models;Predictive models},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.334},
}

@Misc{cornia2018payingattentionsaliencyimage,
  author        = {Marcella Cornia and Lorenzo Baraldi and Giuseppe Serra and Rita Cucchiara},
  title         = {Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1706.08474},
  file          = {:PDF/2018_TOMM.pdf:PDF},
  groups        = {Exploiting human attention},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1706.08474},
}

@InProceedings{Jiang2018RecurrentFusion,
  author    = {Jiang, Wenhao and Ma, Lin and Jiang, Yu-Gang and Liu, Wei and Zhang, Tong},
  booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II},
  title     = {Recurrent Fusion Network for Image Captioning},
  year      = {2018},
  address   = {Berlin, Heidelberg},
  pages     = {510–526},
  publisher = {Springer-Verlag},
  abstract  = {Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework employ only one kind of CNNs, e.g., ResNet or Inception-X, which describes the image contents from only one specific view point. Thus, the semantic meaning of the input image cannot be comprehensively understood, which restricts improving the performance. In this paper, to exploit the complementary information from multiple encoders, we propose a novel recurrent fusion network (RFNet) for the image captioning task. The fusion process in our model can exploit the interactions among the outputs of the image encoders and generate new compact and informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.},
  doi       = {10.1007/978-3-030-01216-8_31},
  file      = {:PDF/Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper.pdf:PDF},
  groups    = {Multi-level features},
  isbn      = {978-3-030-01215-1},
  keywords  = {Image captioning, Encoder-decoder framework, Recurrent fusion network (RFNet)},
  location  = {Munich, Germany},
  numpages  = {17},
  url       = {https://doi.org/10.1007/978-3-030-01216-8_31},
}

@InProceedings{Dalal2005HOG,
  author    = {Dalal, N. and Triggs, B.},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title     = {Histograms of oriented gradients for human detection},
  year      = {2005},
  month     = {June},
  pages     = {886-893 vol. 1},
  volume    = {1},
  abstract  = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  doi       = {10.1109/CVPR.2005.177},
  file      = {:PDF/Histograms_of_oriented_gradients_for_human_detection.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
}

@InProceedings{Torralba2003Vi,
  author    = {Torralba and Murphy and Freeman and Rubin},
  booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
  title     = {Context-based vision system for place and object recognition},
  year      = {2003},
  month     = {Oct},
  pages     = {273-280 vol.1},
  abstract  = {While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.},
  doi       = {10.1109/ICCV.2003.1238354},
  file      = {:PDF/Context-based_vision_system_for_place_and_object_recognition.pdf:PDF},
  keywords  = {Machine vision;Object recognition;Layout;Artificial intelligence;Image recognition;Mobile robots;Robot kinematics;Face recognition;Navigation;Image representation},
}


@MISC{Graff2003Gigaword,
  title     = "English Gigaword Fifth Edition",
  author    = "Parker, Robert and Graff, David and Kong, Junbo and Chen, Ke and
               Maeda, Kazuaki",
  abstract  = "Introduction English Gigaword Fifth Edition is a comprehensive
               archive of newswire text data that has been acquired over
               several years by the Linguistic Data Consortiume (LDC). The
               fifth edition includes all of the contents in English Gigaword
               Fourth Edition (LDC2009T13) plus new data covering the 24-month
               period January 2009 through December 2010. The seven distinct
               international sources of English newswire included in this
               edition are the following: Agence France-Presse, English Service
               (afp\_eng) Associated Press Worldstream, English Service
               (apw\_eng) Central News Agency of Taiwan, English Service
               (cna\_eng) Los Angeles Times/Washington Post Newswire Service
               (ltw\_eng) Washington Post/Bloomberg Newswire Service (wpb\_eng)
               New York Times Newswire Service (nyt\_eng) Xinhua News Agency,
               English Service (xin\_eng) The seven letter codes in the
               parentheses above include the three-character source name
               abbreviations and the three-character langauge code (eng)
               separated by an underscore (\_) character. The three-letter
               language code conforms to LDC's internal convention based on the
               ISO 639-3 standard. Data The following table sets forth the
               overall totals for each source. Note that Total-MB refers to the
               quantity of date when unzipped (approximately 26 gigabytes),
               Gzip-MB refers to compressed file sizes as stored on the
               DVD-ROMs and K-wrds refers to the number of whitespace-separated
               tokens (of all types) after all SGML tags are eliminated: Source
               \#Files Gzip-MB Totl-MB K-wrds \#DOCs afp\_eng 146 1732 4937
               738322 2479624 apw\_eng 193 2700 7889 1186955 3107777 cna\_eng
               144 86 261 38491 145317 ltw\_eng 127 651 1694 268088 411032
               nyt\_eng 197 3280 8938 1422670 1962178 wpb\_eng 12 42 111 17462
               26143 xin\_eng 191 834 2518 360714 1744025 TOTAL 1010 9325 26348
               4032686 9876086 Sponsorship This work was supported in part by
               the Defense Advanced Research Projects Agency, GALE Program
               Grant No. HR0011-06-1-0003. The content of this publication does
               not necessarily reflect the position or policy of the
               Government, and no official endorsement should be inferred.
               Samples For an example of the data in this corpus, please review
               this sample. Portions \copyright{} 1994-2010 Agence France
               Presse, \copyright{} 1994-2010 The Associated Press,
               \copyright{} 1997-2010 Central News Agency (Taiwan),
               \copyright{} 1994-1998, 2003-2009 Los Angeles Times-Washington
               Post News Service, Inc., \copyright{} 1994-2010 New York Times,
               \copyright{} 2010 The Washington Post News Service with
               Bloomberg News, \copyright{} 1995-2010 Xinhua News Agency,
               \copyright{} 2003, 2005, 2007, 2009, 2011 Trustees of the
               University of Pennsylvania",
  publisher = "Linguistic Data Consortium",
  year      =  2011
}

@Article{Lowe2004SIFT,
  author   = {Lowe, David G.},
  journal  = {International Journal of Computer Vision},
  title    = {Distinctive Image Features from Scale-Invariant Keypoints},
  year     = {2004},
  issn     = {1573-1405},
  month    = {Nov},
  number   = {2},
  pages    = {91-110},
  volume   = {60},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  day      = {01},
  doi      = {10.1023/B:VISI.0000029664.99615.94},
  file     = {:PDF/B_VISI.0000029664.99615.94-1.pdf:PDF},
  url      = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
}

@InProceedings{Lin1998AnID,
  author    = {Dekang Lin},
  booktitle = {International Conference on Machine Learning},
  title     = {An Information-Theoretic Definition of Similarity},
  year      = {1998},
  file      = {:PDF/document.pdf:PDF},
  url       = {https://api.semanticscholar.org/CorpusID:5659557},
}


@Article{Torralba80Milion,
  author   = {Torralba, Antonio and Fergus, Rob and Freeman, William T.},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition},
  year     = {2008},
  issn     = {1939-3539},
  month    = {Nov},
  number   = {11},
  pages    = {1958-1970},
  volume   = {30},
  abstract = {With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.},
  doi      = {10.1109/TPAMI.2008.128},
  file     = {:PDF/80_Million_Tiny_Images_A_Large_Data_Set_for_Nonparametric_Object_and_Scene_Recognition.pdf:PDF},
  keywords = {Layout;Image recognition;Internet;Image databases;Image sampling;Psychology;Humans;Visual system;Degradation;Image resolution;Computer vision;Object recognition;large datasets;nearest-neighbor methods;Computer vision;Object recognition;large datasets;nearest-neighbor methods},
}

@InProceedings{Viola2001Haar,
  author    = {Viola, P. and Jones, M.},
  booktitle = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
  title     = {Robust real-time face detection},
  year      = {2001},
  month     = {July},
  pages     = {747-747},
  volume    = {2},
  doi       = {10.1109/ICCV.2001.937709},
  file      = {:PDF/Robust_real-time_face_detection.pdf:PDF},
  keywords  = {Robustness;Face detection;Pixel;Detectors;Boosting;Object detection;Laboratories;Video sequences;Color;Information resources},
}

@Article{Liu2011Gabor,
  author   = {Liu, Xiaobai and Lin, Liang and Yan, Shuicheng and Jin, Hai and Tao, Wenbing},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {Integrating Spatio-Temporal Context With Multiview Representation for Object Recognition in Visual Surveillance},
  year     = {2011},
  issn     = {1558-2205},
  month    = {April},
  number   = {4},
  pages    = {393-407},
  volume   = {21},
  abstract = {We present in this paper an integrated solution to rapidly recognizing dynamic objects in surveillance videos by exploring various contextual information. This solution consists of three components. The first one is a multi-view object representation. It contains a set of deformable object templates, each of which comprises an ensemble of active features for an object category in a specific view/pose. The template can be efficiently learned via a small set of roughly aligned positive samples without negative samples. The second component is a unified spatio-temporal context model, which integrates two types of contextual information in a Bayesian way. One is the spatial context, including main surface property (constraints on object type and density) and camera geometric parameters (constraints on object size at a specific location). The other is the temporal context, containing the pixel-level and instance-level consistency models, used to generate the foreground probability map and local object trajectory prediction. We also combine the above spatial and temporal contextual information to estimate the object pose in scene and use it as a strong prior for inference. The third component is a robust sampling-based inference procedure. Taking the spatio-temporal contextual knowledge as the prior model and deformable template matching as the likelihood model, we formulate the problem of object category recognition as a maximum-a-posteriori problem. The probabilistic inference can be achieved by a simple Markov chain Mento Carlo sampler, owing to the informative spatio-temporal context model which is able to greatly reduce the computation complexity and the category ambiguities. The system performance and benefit gain from the spatio-temporal contextual information are quantitatively evaluated on several challenging datasets and the comparison results clearly demonstrate that our proposed algorithm outperforms other state-of-the-art algorithms.},
  doi      = {10.1109/TCSVT.2010.2087570},
  file     = {:PDF/Integrating_Spatio-Temporal_Context_With_Multiview_Representation_for_Object_Recognition_in_Visual_Surveillance.pdf:PDF},
  keywords = {Context;Context modeling;Surveillance;Bismuth;Videos;Object recognition;Computational modeling;Active feature;deformable template;object recognition;spatio-temporal context},
}

@Article{Bach2003Kernel,
  author     = {Bach, Francis R. and Jordan, Michael I.},
  journal    = {J. Mach. Learn. Res.},
  title      = {Kernel independent component analysis},
  year       = {2003},
  issn       = {1532-4435},
  month      = mar,
  number     = {null},
  pages      = {1–48},
  volume     = {3},
  abstract   = {We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.},
  doi        = {10.1162/153244303768966085},
  file       = {:PDF/153244303768966085.pdf:PDF},
  issue_date = {3/1/2003},
  keywords   = {Stiefel manifold, blind source separation, canonical correlations, gram matrices, incomplete Cholesky decomposition, independent component analysis, integral equations, kernel methods, mutual information, semiparametric models},
  numpages   = {48},
  publisher  = {JMLR.org},
  url        = {https://doi.org/10.1162/153244303768966085},
}

@InProceedings{Joachim2002Optimizing,
  author    = {Joachims, Thorsten},
  booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title     = {Optimizing search engines using clickthrough data},
  year      = {2002},
  address   = {New York, NY, USA},
  pages     = {133–142},
  publisher = {Association for Computing Machinery},
  series    = {KDD '02},
  abstract  = {This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.},
  doi       = {10.1145/775047.775067},
  file      = {:PDF/775047.775067.pdf:PDF},
  isbn      = {158113567X},
  location  = {Edmonton, Alberta, Canada},
  numpages  = {10},
  url       = {https://doi.org/10.1145/775047.775067},
}

@InProceedings{Cornia2016Saliency,
  author    = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
  title     = {A deep multi-level network for saliency prediction},
  year      = {2016},
  month     = {Dec},
  pages     = {3488-3493},
  abstract  = {This paper presents a novel deep architecture for saliency prediction. Current state of the art models for saliency prediction employ Fully Convolutional networks that perform a non-linear combination of features extracted from the last convolutional layer to predict saliency maps. We propose an architecture which, instead, combines features extracted at different levels of a Convolutional Neural Network (CNN). Our model is composed of three main blocks: a feature extraction CNN, a feature encoding network, that weights low and high level feature maps, and a prior learning network. We compare our solution with state of the art saliency models on two public benchmarks datasets. Results show that our model outperforms under all evaluation metrics on the SALICON dataset, which is currently the largest public dataset for saliency prediction, and achieves competitive results on the MIT300 benchmark. Code is available at https://github.com/marcellacornia/mlnet.},
  doi       = {10.1109/ICPR.2016.7900174},
  file      = {:PDF/A_deep_multi-level_network_for_saliency_prediction.pdf:PDF},
  keywords  = {Feature extraction;Computer architecture;Measurement;Encoding;Convolutional codes;Benchmark testing;Observers},
}

@Article{Cornia2018HumanFixation,
  author   = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  journal  = {IEEE Transactions on Image Processing},
  title    = {Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model},
  year     = {2018},
  issn     = {1941-0042},
  month    = {Oct},
  number   = {10},
  pages    = {5142-5154},
  volume   = {27},
  abstract = {Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.},
  doi      = {10.1109/TIP.2018.2851672},
  keywords = {Predictive models;Feature extraction;Computer architecture;Computational modeling;Task analysis;Deep learning;Visualization;Saliency;human eye fixations;convolutional neural networks;deep learning},
}

@InProceedings{Szegedy2017InceptionV4,
  author    = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  title     = {Inception-v4, inception-ResNet and the impact of residual connections on learning},
  year      = {2017},
  pages     = {4278–4284},
  publisher = {AAAI Press},
  series    = {AAAI'17},
  abstract  = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  file      = {:PDF/11231-Article Text-14759-1-2-20201228.pdf:PDF},
  location  = {San Francisco, California, USA},
  numpages  = {7},
}

@Article{Barron2017ELU,
  author     = {Jonathan T. Barron},
  journal    = {CoRR},
  title      = {Continuously Differentiable Exponential Linear Units},
  year       = {2017},
  volume     = {abs/1704.07483},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/Barron17a.bib},
  eprint     = {1704.07483},
  eprinttype = {arXiv},
  file       = {:PDF/1704.07483v1.pdf:PDF},
  timestamp  = {Mon, 13 Aug 2018 16:46:02 +0200},
  url        = {http://arxiv.org/abs/1704.07483},
}

@Article{Andrzej2005kultura,
  author  = {Andrzej, Markowski},
  journal = {Teoria. Zagadnienia leksykalne, Wydawnictwo Naukowe PWN, Warszawa},
  title   = {Kultura j{\k{e}}zyka polskiego},
  year    = {2005},
}

@Book{Przepiorkowski2002FormalnyOpis,
  author    = {Adam Przepiórkowski and Anna Kupść and Małgorzata Marciniak and Agnieszka Mykowiecka},
  publisher = {aowe},
  title     = {Formalny opis języka polskiego: Teoria i implementacja},
  year      = {2002},
  address   = {Warszawa},
  file      = {:PDF/hpsg.pdf:PDF},
  groups    = {opis polskiego},
}

@Book{janusz_s_bien_2009_3944137,
  author    = {Janusz S. Bień},
  publisher = {Zenodo},
  title     = {Problemy formalnego opisu składni polskiej},
  year      = {2009},
  month     = sep,
  doi       = {10.5281/zenodo.3944137},
  file      = {:PDF/JSB_PROFOSP_CMYK (1).pdf:PDF},
  groups    = {opis polskiego},
  url       = {https://doi.org/10.5281/zenodo.3944137},
}

@Book{Wolinski2019AutomatycznaAnalizaSkladnikowa,
  author    = {Woliński, Marcin},
  publisher = {Uniwersytet Warszawski},
  title     = {Automatyczna analiza składnikowa języka polskiego},
  year      = {2019},
  address   = {Warszawa},
  isbn      = {978-83-235-3614-7},
  doi       = {https://doi.org/10.31338/uw.9788323536147},
  file      = {:PDF/Automatyczna_analiza_składnikowa_ję.pdf:PDF},
}

@Misc{Bańko_Mirosław._Red._Narodowy,
  author       = {Bańko, Mirosław. Red. and Górski, Rafał L. Red. and Przepiórkowski, Adam. Red. and Lewandowska-Tomaszczyk , Barbara. Red.},
  howpublished = {online},
  title        = {Narodowy Korpus Języka Polskiego},
  file         = {:PDF/NKJP_ksiazka_poprawiony.pdf:PDF},
  groups       = {opis polskiego},
  keywords     = {narodowy korpus polszczyzny, przetwarzanie języków naturalnych},
  language     = {pol},
  publisher    = {Warszawa : Wydawnictwo Naukowe PWN},
  type         = {książka},
}

@InProceedings{Marcinczuk2021Text,
  author    = {Marci{\'n}czuk, Micha{\l} and Gniewkowski, Mateusz and Walkowiak, Tomasz and B{\k{e}}dkowski, Marcin},
  booktitle = {Proceedings of the 11th Global Wordnet Conference},
  title     = {Text Document Clustering: {W}ordnet vs. {TF}-{IDF} vs. Word Embeddings},
  year      = {2021},
  address   = {University of South Africa (UNISA)},
  editor    = {Vossen, Piek and Fellbaum, Christiane},
  month     = jan,
  pages     = {207--214},
  publisher = {Global Wordnet Association},
  abstract  = {In the paper, we deal with the problem of unsupervised text document clustering for the Polish language. Our goal is to compare the modern approaches based on language modeling (doc2vec and BERT) with the classical ones, i.e., TF-IDF and wordnet-based. The experiments are conducted on three datasets containing qualification descriptions. The experiments' results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches.},
  file      = {:PDF/2021.gwc-1.24.pdf:PDF},
  url       = {https://aclanthology.org/2021.gwc-1.24/},
}

@Article{Li2024-zs,
  author   = {Li, Bing and Yang, Peng and Sun, Yuankang and Hu, Zhongjian and Yi, Meng},
  journal  = {Frontiers of Information Technology \& Electronic Engineering},
  title    = {Advances and challenges in artificial intelligence text generation},
  year     = {2024},
  month    = jan,
  number   = {1},
  pages    = {64--83},
  volume   = {25},
  abstract = {Text generation is an essential research area in artificial
              intelligence (AI) technology and natural language processing and
              provides key technical support for the rapid development of
              AI-generated content (AIGC). It is based on technologies such as
              natural language processing, machine learning, and deep learning,
              which enable learning language rules through training models to
              automatically generate text that meets grammatical and semantic
              requirements. In this paper, we sort and systematically summarize
              the main research progress in text generation and review recent
              text generation papers, focusing on presenting a detailed
              understanding of the technical models. In addition, several
              typical text generation application systems are presented.
              Finally, we address some challenges and future directions in AI
              text generation. We conclude that improving the quality,
              quantity, interactivity, and adaptability of generated text can
              help fundamentally advance AI text generation development.},
  file     = {:PDF/download.pdf:PDF},
  groups   = {bledy w SI},
}

@Misc{havlik2024commonerrorsgenerativeai,
  author        = {Denis Havlik and Marcelo Pias},
  title         = {Common errors in Generative AI systems used for knowledge extraction in the climate action domain},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.00830},
  file          = {:PDF/2402.00830v1.pdf:PDF},
  groups        = {bledy w SI},
  primaryclass  = {cs.CY},
  url           = {https://arxiv.org/abs/2402.00830},
}

@Article{Dale2021GPT3,
  author  = {Dale, Robert},
  journal = {Natural Language Engineering},
  title   = {GPT-3: What’s it good for?},
  year    = {2021},
  number  = {1},
  pages   = {113–118},
  volume  = {27},
  doi     = {10.1017/S1351324920000601},
  file    = {:PDF/GPT-3_Whats_it_good_for.pdf:PDF},
  groups  = {bledy w SI},
}

@InProceedings{Sutskever2014Seq2Seq,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
  title     = {Sequence to sequence learning with neural networks},
  year      = {2014},
  address   = {Cambridge, MA, USA},
  pages     = {3104–3112},
  publisher = {MIT Press},
  series    = {NIPS'14},
  abstract  = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  file      = {:PDF/NIPS-2014-sequence-to-sequence-learning-with-neural-networks-Paper.pdf:PDF},
  groups    = {lm},
  location  = {Montreal, Canada},
  numpages  = {9},
}

@Book{Cormen2024WprowadzenieDoAlgorytmow,
  author    = {Cormen, Thomas H},
  editor    = {Diks, Krzysztof and Malinowski, Adam},
  publisher = {PWN},
  title     = {Wprowadzenie do algorytmów},
  year      = {2024},
  edition   = {2},
  isbn      = {9788301229603},
  file      = {:PDF/100000331082_Cormen_WprowadzenieDoAlgorytmow.pdf:PDF},
  place     = {Warszawa},
}

@Article{Maciag2022NLPRekonstrukcja,
  author  = {Maciag, Rafal},
  journal = {Zarządzanie w Kulturze},
  title   = {Zaawansowane procedury NLP jako przesłanka rekonstrukcji idei wiedzy},
  year    = {2022},
  month   = {05},
  pages   = {37-53},
  volume  = {23},
  doi     = {10.4467/20843976ZK.22.003.15869},
  file    = {:PDF/3_Maciag_Zarzadzanie-w-Kulturze_23(1)_2022-2.pdf:PDF},
}

@InProceedings{Tanti2017Role,
  author    = {Tanti, Marc and Gatt, Albert and Camilleri, Kenneth},
  booktitle = {Proceedings of the 10th International Conference on Natural Language Generation},
  title     = {What is the Role of Recurrent Neural Networks ({RNN}s) in an Image Caption Generator?},
  year      = {2017},
  address   = {Santiago de Compostela, Spain},
  editor    = {Alonso, Jose M. and Bugar{\'i}n, Alberto and Reiter, Ehud},
  month     = sep,
  pages     = {51--60},
  publisher = {Association for Computational Linguistics},
  abstract  = {Image captioning has evolved into a core task for Natural Language Generation and has also proved to be an important testbed for deep learning approaches to handling multimodal representations. Most contemporary approaches rely on a combination of a convolutional network to handle image features, and a recurrent network to encode linguistic information. The latter is typically viewed as the primary {\textquotedblleft}generation{\textquotedblright} component. Beyond this high-level characterisation, a CNN+RNN model supports a variety of architectural designs. The dominant model in the literature is one in which visual features encoded by a CNN are {\textquotedblleft}injected{\textquotedblright} as part of the linguistic encoding process, driving the RNN`s linguistic choices. By contrast, it is possible to envisage an architecture in which visual and linguistic features are encoded separately, and merged at a subsequent stage. In this paper, we address two related questions: (1) Is direct injection the best way of combining multimodal information, or is a late merging alternative better for the image captioning task? (2) To what extent should a recurrent network be viewed as actually generating, rather than simply encoding, linguistic information?},
  doi       = {10.18653/v1/W17-3506},
  file      = {:PDF/W17-3506.pdf:PDF},
  url       = {https://aclanthology.org/W17-3506/},
}

@Article{Payandeh2023DeepRepresentationLearning,
  author   = {Payandeh, Amirreza and Baghaei, Kourosh T. and Fayyazsanavi, Pooya and Ramezani, Somayeh Bakhtiari and Chen, Zhiqian and Rahimi, Shahram},
  journal  = {IEEE Access},
  title    = {Deep Representation Learning: Fundamentals, Technologies, Applications, and Open Challenges},
  year     = {2023},
  issn     = {2169-3536},
  pages    = {137621-137659},
  volume   = {11},
  abstract = {Machine learning algorithms have had a profound impact on the field of computer science over the past few decades. The performance of these algorithms heavily depends on the representations derived from the data during the learning process. Successful learning processes aim to produce concise, discrete, meaningful representations that can be effectively applied to various tasks. Recent advancements in deep learning models have proven to be highly effective in capturing high-dimensional, non-linear, and multi-modal characteristics. In this work, we provide a comprehensive overview of the current state-of-the-art in deep representation learning and the principles and developments made in the process of representation learning. Our study encompasses both supervised and unsupervised methods, including popular techniques such as autoencoders, self-supervised methods, and deep neural networks. Furthermore, we explore a wide range of applications, including image recognition and natural language processing. In addition, we discuss recent trends, key issues, and open challenges in the field. This survey endeavors to make a significant contribution to the field of deep representation learning, fostering its understanding and facilitating further advancements.},
  doi      = {10.1109/ACCESS.2023.3335196},
  file     = {:PDF/Deep_Representation_Learning_Fundamentals_Technologies_Applications_and_Open_Challenges.pdf:PDF},
  groups   = {representation learning},
  keywords = {Representation learning;Natural language processing;Task analysis;Generative adversarial networks;Feature extraction;Deep learning;Training;Transfer learning;Computer vision;Representation learning;deep learning;feature extraction;transfer learning;natural language processing;computer vision},
}

@Misc{Bhatnagar2023EnhancingIC,
  author        = {Pooja Bhatnagar and Sai Mrunaal and Sachin Kamnure},
  title         = {Enhancing Image Captioning with Neural Models},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2312.00435},
  file          = {:PDF/2312.00435v1.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2312.00435},
}

@Article{Bengio2013RepresentationLearning,
  author     = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal    = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title      = {Representation Learning: A Review and New Perspectives},
  year       = {2013},
  issn       = {0162-8828},
  month      = aug,
  number     = {8},
  pages      = {1798–1828},
  volume     = {35},
  abstract   = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  address    = {USA},
  doi        = {10.1109/TPAMI.2013.50},
  file       = {:PDF/Representation_Learning_A_Review_and_New_Perspectives (1).pdf:PDF},
  groups     = {representation learning},
  issue_date = {August 2013},
  keywords   = {Abstracts, Boltzmann machine, Deep learning, Feature extraction, Learning systems, Machine learning, Manifolds, Neural networks, Speech recognition, autoencoder, feature learning, neural nets, representation learning, unsupervised learning},
  numpages   = {31},
  publisher  = {IEEE Computer Society},
  url        = {https://doi.org/10.1109/TPAMI.2013.50},
}

@Article{Wang2022TSFeatureEngineering,
  author         = {Wang, Can and Baratchi, Mitra and Bäck, Thomas and Hoos, Holger H. and Limmer, Steffen and Olhofer, Markus},
  journal        = {Engineering Proceedings},
  title          = {Towards Time-Series Feature Engineering in Automated Machine Learning for Multi-Step-Ahead Forecasting},
  year           = {2022},
  issn           = {2673-4591},
  number         = {1},
  volume         = {18},
  abstract       = {Feature engineering is an essential step in the pipelines used for many machine learning tasks, including time-series forecasting. Although existing AutoML approaches partly automate feature engineering, they do not support specialised approaches for applications on time-series data such as multi-step forecasting. Multi-step forecasting is the task of predicting a sequence of values in a time-series. Two kinds of approaches are commonly used for multi-step forecasting. A typical approach is to apply one model to predict the value for the next time step. Then the model uses this predicted value as an input to forecast the value for the next time step. Another approach is to use multi-output models to make the predictions for multiple time steps of each time-series directly. In this work, we demonstrate how automated machine learning can be enhanced with feature engineering techniques for multi-step time-series forecasting. Specifically, we combine a state-of-the-art automated machine learning system, auto-sklearn, with tsfresh, a library for feature extraction from time-series. In addition to optimising machine learning pipelines, we propose to optimise the size of the window over which time-series data are used for predicting future time-steps. This is an essential hyperparameter in time-series forecasting. We propose and compare (i) auto-sklearn with automated window size selection, (ii) auto-sklearn with tsfresh features, and (iii) auto-sklearn with automated window size selection and tsfresh features. We evaluate these approaches with statistical techniques, machine learning techniques and state-of-the-art automated machine learning techniques, on a diverse set of benchmarks for multi-step time-series forecasting, covering 20 synthetic and real-world problems. Our empirical results indicate a significant potential for improving the accuracy of multi-step time-series forecasting by using automated machine learning in combination with automatically optimised feature extraction techniques.},
  article-number = {17},
  doi            = {10.3390/engproc2022018017},
  file           = {:Fuzzy.bib (conflicted copy 2021-09-15 142929).sav:sav;:PDF/engproc-18-00017.pdf:PDF},
  groups         = {representation learning},
  url            = {https://www.mdpi.com/2673-4591/18/1/17},
}

@Article{Uddin2018Proposing,
  author         = {Uddin, Muhammad Fahim and Lee, Jeongkyu and Rizvi, Syed and Hamada, Samir},
  journal        = {Applied Sciences},
  title          = {Proposing Enhanced Feature Engineering and a Selection Model for Machine Learning Processes},
  year           = {2018},
  issn           = {2076-3417},
  number         = {4},
  volume         = {8},
  abstract       = {Machine Learning (ML) requires a certain number of features (i.e., attributes) to train the model. One of the main challenges is to determine the right number and the type of such features out of the given dataset’s attributes. It is not uncommon for the ML process to use dataset of available features without computing the predictive value of each. Such an approach makes the process vulnerable to overfit, predictive errors, bias, and poor generalization. Each feature in the dataset has either a unique predictive value, redundant, or irrelevant value. However, the key to better accuracy and fitting for ML is to identify the optimum set (i.e., grouping) of the right feature set with the finest matching of the feature’s value. This paper proposes a novel approach to enhance the Feature Engineering and Selection (eFES) Optimization process in ML. eFES is built using a unique scheme to regulate error bounds and parallelize the addition and removal of a feature during training. eFES also invents local gain (LG) and global gain (GG) functions using 3D visualizing techniques to assist the feature grouping function (FGF). FGF scores and optimizes the participating feature, so the ML process can evolve into deciding which features to accept or reject for improved generalization of the model. To support the proposed model, this paper presents mathematical models, illustrations, algorithms, and experimental results. Miscellaneous datasets are used to validate the model building process in Python, C#, and R languages. Results show the promising state of eFES as compared to the traditional feature selection process.},
  article-number = {646},
  doi            = {10.3390/app8040646},
  file           = {:PDF/applsci-08-00646.pdf:PDF},
  groups         = {representation learning},
  url            = {https://www.mdpi.com/2076-3417/8/4/646},
}

@InProceedings{Zhu2024FeatureEngineering,
  author    = {Zhu, Qian and Wang, Dakuo and Ma, Shuai and Wang, April Yi and Chen, Zixin and Khurana, Udayan and Ma, Xiaojuan},
  booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
  title     = {Towards Feature Engineering with Human and AI’s Knowledge: Understanding Data Science Practitioners’ Perceptions in Human\&AI-Assisted Feature Engineering Design},
  year      = {2024},
  address   = {New York, NY, USA},
  pages     = {1789–1804},
  publisher = {Association for Computing Machinery},
  series    = {DIS '24},
  abstract  = {As AI technology continues to advance, the importance of human-AI collaboration becomes increasingly evident, with numerous studies exploring its potential in various fields. One vital field is data science, including feature engineering (FE), where both human ingenuity and AI capabilities play pivotal roles. Despite the existence of AI-generated recommendations for FE, there remains a limited understanding of how to effectively integrate and utilize humans’ and AI’s knowledge. To address this gap, we design a readily-usable prototype, human&AI-assisted FE in Jupyter notebooks. It harnesses the strengths of humans and AI to provide feature suggestions to users, seamlessly integrating these recommendations into practical workflows. Using the prototype as a research probe, we conducted an exploratory study to gain valuable insights into data science practitioners’ perceptions, usage patterns, and their potential needs when presented with feature suggestions from both humans and AI. Through qualitative analysis, we discovered that the “Creator” of the feature (i.e., AI or human) significantly influences users’ feature selection, and the semantic clarity of the suggested feature greatly impacts its adoption rate. Furthermore, our findings indicate that users perceive both differences and complementarity between features generated by humans and those generated by AI. Lastly, based on our study results, we derived a set of design recommendations for future human&AI FE design. Our findings show the collaborative potential between humans and AI in the field of FE.},
  doi       = {10.1145/3643834.3661517},
  file      = {:PDF/2405.14107v1.pdf:PDF},
  groups    = {representation learning},
  isbn      = {9798400705830},
  keywords  = {Computational Notebooks, Feature Recommendation, human-AI Collaboration},
  location  = {Copenhagen, Denmark},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3643834.3661517},
}

@InProceedings{Heaton2016Empirical,
  author    = {Heaton, Jeff},
  booktitle = {SoutheastCon 2016},
  title     = {An empirical analysis of feature engineering for predictive modeling},
  year      = {2016},
  month     = mar,
  pages     = {1–6},
  publisher = {IEEE},
  doi       = {10.1109/secon.2016.7506650},
  file      = {:PDF/1701.07852v2.pdf:PDF},
  groups    = {representation learning},
  url       = {http://dx.doi.org/10.1109/SECON.2016.7506650},
}

@Article{Guyon2003Introduction,
  author     = {Guyon, Isabelle and Elisseeff, Andr\'{e}},
  journal    = {J. Mach. Learn. Res.},
  title      = {An introduction to variable and feature selection},
  year       = {2003},
  issn       = {1532-4435},
  month      = mar,
  number     = {null},
  pages      = {1157–1182},
  volume     = {3},
  abstract   = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
  file       = {:PDF/944919.944968.pdf:PDF},
  groups     = {representation learning},
  issue_date = {3/1/2003},
  numpages   = {26},
  publisher  = {JMLR.org},
}

@Misc{aggarwal2019aiautomateendtoenddata,
  author        = {Charu Aggarwal and Djallel Bouneffouf and Horst Samulowitz and Beat Buesser and Thanh Hoang and Udayan Khurana and Sijia Liu and Tejaswini Pedapati and Parikshit Ram and Ambrish Rawat and Martin Wistuba and Alexander Gray},
  title         = {How can AI Automate End-to-End Data Science?},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1910.14436},
  file          = {:PDF/1910.14436v1.pdf:PDF;:PDF/1910.14436v1.pdf:PDF},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/1910.14436},
}

@Article{Borji2023ACA,
  author  = {Ali Borji},
  journal = {ArXiv},
  title   = {A Categorical Archive of ChatGPT Failures},
  year    = {2023},
  volume  = {abs/2302.03494},
  file    = {:PDF/2302.03494v8.pdf:PDF},
  groups  = {bledy w SI},
  url     = {https://api.semanticscholar.org/CorpusID:256627571},
}

@InProceedings{Bender2021StochasticsParrots,
  author    = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ��},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {610–623},
  publisher = {Association for Computing Machinery},
  series    = {FAccT 2021},
  abstract  = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  doi       = {10.1145/3442188.3445922},
  groups    = {bledy w SI},
  isbn      = {9781450383097},
  location  = {Virtual Event, Canada},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3442188.3445922},
}

@Book{Manning1999Foundations,
  author    = {Manning, C. and Schutze, H.},
  publisher = {MIT Press},
  title     = {Foundations of Statistical Natural Language Processing},
  year      = {1999},
  isbn      = {9780262133609},
  series    = {Foundations of Statistical Natural Language Processing},
  file      = {:PDF/Christopher_D._Manning_Hinrich_Schütze_Foundations_Of_Statistical_Natural_Language_Processing.pdf:PDF},
  lccn      = {99021137},
  url       = {https://books.google.pl/books?id=YiFDxbEX3SUC},
}

@InProceedings{Meteor2008Agarwal,
  author    = {Agarwal, Abhaya and Lavie, Alon},
  booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
  title     = {METEOR, M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine translation output},
  year      = {2008},
  address   = {USA},
  pages     = {115–118},
  publisher = {Association for Computational Linguistics},
  series    = {StatMT '08},
  abstract  = {This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments. We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor.},
  file      = {:PDF/WMT12.pdf:PDF},
  isbn      = {9781932432091},
  location  = {Columbus, Ohio},
  numpages  = {4},
}

@Article{Porter1980Stemmer,
  author    = {Porter, M. F.},
  journal   = {Program},
  title     = {An algorithm for suffix stripping},
  year      = {1980},
  issn      = {0033-0337},
  month     = {Jan},
  number    = {3},
  pages     = {130-137},
  volume    = {14},
  abstract  = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.},
  day       = {01},
  doi       = {10.1108/eb046814},
  file      = {:PDF/Porter-1980.pdf:PDF},
  publisher = {MCB UP Ltd},
  url       = {https://doi.org/10.1108/eb046814},
}

@InProceedings{Denkowski2014Meteor,
  author    = {Denkowski, Michael and Lavie, Alon},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  title     = {Meteor Universal: Language Specific Translation Evaluation for Any Target Language},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  editor    = {Bojar, Ond{\v{r}}ej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Monz, Christof and Post, Matt and Specia, Lucia},
  month     = jun,
  pages     = {376--380},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/W14-3348},
  file      = {:PDF/W14-3348.pdf:PDF},
  url       = {https://aclanthology.org/W14-3348/},
}

@InProceedings{Arneffe2014Universal,
  author    = {de Marneffe, Marie-Catherine and Dozat, Timothy and Silveira, Natalia and Haverinen, Katri and Ginter, Filip and Nivre, Joakim and Manning, Christopher D.},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}`14)},
  title     = {Universal {S}tanford dependencies: A cross-linguistic typology},
  year      = {2014},
  address   = {Reykjavik, Iceland},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  month     = may,
  pages     = {4585--4592},
  publisher = {European Language Resources Association (ELRA)},
  abstract  = {Revisiting the now de facto standard Stanford dependency representation, we propose an improved taxonomy to capture grammatical relations across languages, including morphologically rich ones. We suggest a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added. We emphasize the lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology. We show how existing dependency schemes for several languages map onto the universal taxonomy proposed here and close with consideration of practical implications of dependency representation choices for NLP applications, in particular parsing.},
  file      = {:PDF/1062_Paper.pdf:PDF},
  url       = {https://aclanthology.org/L14-1045/},
}

@InProceedings{Schuster2015Generating,
  author    = {Schuster, Sebastian and Krishna, Ranjay and Chang, Angel and Fei-Fei, Li and Manning, Christopher D.},
  booktitle = {Proceedings of the Fourth Workshop on Vision and Language},
  title     = {Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval},
  year      = {2015},
  address   = {Lisbon, Portugal},
  editor    = {Belz, Anja and Coheur, Luisa and Ferrari, Vittorio and Moens, Marie-Francine and Pastra, Katerina and Vuli{\'c}, Ivan},
  month     = sep,
  pages     = {70--80},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/W15-2812},
  file      = {:PDF/W15-2812.pdf:PDF;:PDF/W15-2812.pdf:PDF},
  url       = {https://aclanthology.org/W15-2812/},
}

@InProceedings{Klein2003Accurate,
  author    = {Klein, Dan and Manning, Christopher D.},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  title     = {Accurate Unlexicalized Parsing},
  year      = {2003},
  address   = {Sapporo, Japan},
  month     = jul,
  pages     = {423--430},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/1075096.1075150},
  file      = {:PDF/P03-1054.pdf:PDF},
  url       = {https://aclanthology.org/P03-1054/},
}

@InProceedings{Elliott2014Comparing,
  author    = {Elliott, Desmond and Keller, Frank},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Comparing Automatic Evaluation Measures for Image Description},
  year      = {2014},
  address   = {Baltimore, Maryland},
  editor    = {Toutanova, Kristina and Wu, Hua},
  month     = jun,
  pages     = {452--457},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/P14-2074},
  file      = {:PDF/P14-2074.pdf:PDF},
  url       = {https://aclanthology.org/P14-2074/},
}

@Article{Ryoma2021WMDRe,
  author     = {Ryoma Sato and Makoto Yamada and Hisashi Kashima},
  journal    = {CoRR},
  title      = {Re-evaluating Word Mover's Distance},
  year       = {2021},
  volume     = {abs/2105.14403},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2105-14403.bib},
  eprint     = {2105.14403},
  eprinttype = {arXiv},
  file       = {:PDF/sato22b.pdf:PDF},
  group      = {metryki},
  timestamp  = {Wed, 02 Jun 2021 11:46:42 +0200},
  url        = {https://arxiv.org/abs/2105.14403},
}

@Article{Sutton2012CRF,
  author     = {Sutton, Charles and McCallum, Andrew},
  journal    = {Found. Trends Mach. Learn.},
  title      = {An Introduction to Conditional Random Fields},
  year       = {2012},
  issn       = {1935-8237},
  month      = apr,
  number     = {4},
  pages      = {267–373},
  volume     = {4},
  abstract   = {Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/2200000013},
  file       = {:PDF/1011.4088v1.pdf:PDF},
  issue_date = {April 2012},
  numpages   = {107},
  publisher  = {Now Publishers Inc.},
  url        = {https://doi.org/10.1561/2200000013},
}

@InProceedings{Bartosiewicz2023Combining,
  author    = {Bartosiewicz, Mateusz and Iwanowski, Marcin and Wiszniewska, Martika and Frączak, Karolina and Leśnowolski, Paweł},
  booktitle = {2023 18th Conference on Computer Science and Intelligence Systems (FedCSIS)},
  title     = {On Combining Image Features and Word Embeddings for Image Captioning},
  year      = {2023},
  month     = {Sep.},
  pages     = {355-365},
  abstract  = {Image captioning is the task of generating semantically and grammatically correct caption for a given image. Captioning model usually has an encoder-decoder structure where encoded image is decoded as list of words being a consecutive elements of the descriptive sentence. In this work, we investigate how encoding of the input image and way of coding words affects the result of the training of the encoder-decoder captioning model. We performed experiments with image encoding using 10 all-purpose popular backbones and 2 types of word embeddings. We compared those models using most popular image captioning evaluation metrics. Our research shows that the model's performance highly depends on the optimal combination of the neural image feature extractor and language processing model. The outcome of our research are applicable in all the research works that lead to the developing the optimal encoder-decoder image captioning model.},
  doi       = {10.15439/2023F997},
  file      = {:PDF/On_Combining_Image_Features_and_Word_Embeddings_for_Image_Captioning.pdf:PDF},
  groups    = {moje},
  keywords  = {Training;Measurement;Image coding;Image recognition;Computational modeling;Merging;Predictive models;image captioning;neural image feature extractors;embedding models;LSTM},
}

@Article{Bartosiewicz2024Optimal,
  author         = {Bartosiewicz, Mateusz and Iwanowski, Marcin},
  journal        = {Information},
  title          = {The Optimal Choice of the Encoder–Decoder Model Components for Image Captioning},
  year           = {2024},
  issn           = {2078-2489},
  number         = {8},
  volume         = {15},
  abstract       = {Image captioning aims at generating meaningful verbal descriptions of a digital image. This domain is rapidly growing due to the enormous increase in available computational resources. The most advanced methods are, however, resource-demanding. In our paper, we return to the encoder–decoder deep-learning model and investigate how replacing its components with newer equivalents improves overall effectiveness. The primary motivation of our study is to obtain the highest possible level of improvement of classic methods, which are applicable in less computational environments where most advanced models are too heavy to be efficiently applied. We investigate image feature extractors, recurrent neural networks, word embedding models, and word generation layers and discuss how each component influences the captioning model’s overall performance. Our experiments are performed on the MS COCO 2014 dataset. As a result of our research, replacing components improves the quality of generating image captions. The results will help design efficient models with optimal combinations of their components.},
  article-number = {504},
  doi            = {10.3390/info15080504},
  file           = {:PDF/information-15-00504.pdf:PDF},
  groups         = {moje},
  url            = {https://www.mdpi.com/2078-2489/15/8/504},
}

@Book{Lappin1996LAPTHO,
  editor    = {Shalom Lappin},
  publisher = {Blackwell Reference},
  title     = {The Handbook of Contemporary Semantic Theory},
  year      = {1996},
  address   = {Cambridge, Mass., USA},
  file      = {:PDF/The Handbook of Contemporary Semantic Theory - 2015 - Lappin.pdf:PDF},
}

@InProceedings{Kirillov2019Panoptic,
  author    = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Panoptic Segmentation},
  year      = {2019},
  month     = {June},
  pages     = {9396-9405},
  abstract  = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.},
  doi       = {10.1109/CVPR.2019.00963},
  file      = {:PDF/Panoptic_Segmentation.pdf:PDF},
  issn      = {2575-7075},
  keywords  = {Recognition: Detection;Categorization;Retrieval;Segmentation;Grouping and Shape},
}

@InProceedings{Guler2018DensePose,
  author    = {Güler, Rıza Alp and Neverova, Natalia and Kokkinos, Iasonas},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {DensePose: Dense Human Pose Estimation in the Wild},
  year      = {2018},
  month     = {June},
  file      = {:PDF/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf:PDF},
}

@Book{Bellman1961AdaptiveControlProcesses,
  author      = {Richard E. Bellman},
  publisher   = {Princeton University Press},
  title       = {Adaptive Control Processes},
  year        = {1961},
  address     = {Princeton},
  isbn        = {9781400874668},
  doi         = {doi:10.1515/9781400874668},
  lastchecked = {2025-03-07},
  url         = {https://doi.org/10.1515/9781400874668},
}

@InProceedings{Peters2018Deep,
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  title     = {Deep Contextualized Word Representations},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  editor    = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  month     = jun,
  pages     = {2227--2237},
  publisher = {Association for Computational Linguistics},
  abstract  = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  doi       = {10.18653/v1/N18-1202},
  file      = {:PDF/N18-1202.pdf:PDF},
  url       = {https://aclanthology.org/N18-1202/},
}

@Article{Harris1954DistributionalS,
  author    = {Zellig S. Harris},
  journal   = {WORD},
  title     = {Distributional Structure},
  year      = {1954},
  number    = {2-3},
  pages     = {146--162},
  volume    = {10},
  doi       = {10.1080/00437956.1954.11659520},
  eprint    = {https://doi.org/10.1080/00437956.1954.11659520},
  file      = {:PDF/Distributional Structure-1.pdf:PDF},
  publisher = {Routledge},
  url       = {https://doi.org/10.1080/00437956.1954.11659520},
}



@InBook{Sparck1988TFIDF,
  author    = {Sparck Jones, Karen},
  pages     = {132–142},
  publisher = {Taylor Graham Publishing},
  title     = {A statistical interpretation of term specificity and its application in retrieval},
  year      = {1988},
  address   = {GBR},
  isbn      = {0947568212},
  booktitle = {Document Retrieval Systems},
  file      = {:PDF/lecture1-firth.pdf:PDF},
  numpages  = {11},
}

@InProceedings{Rehurek2010Gensim,
  author       = {Řehůřek, Radim and Sojka, Petr},
  booktitle    = {Proceedings of LREC 2010 workshop New Challenges for NLP Frameworks},
  title        = {Software Framework for Topic Modelling with Large Corpora},
  year         = {2010},
  address      = {Valletta, Malta},
  pages        = {46--50},
  publisher    = {University of Malta},
  file         = {:PDF/lrec2010_final.pdf:PDF},
  howpublished = {paměťový nosič},
  isbn         = {2-9517408-6-7},
  keywords     = {document similarity; NLP; software; vector space model; topical modelling; software framework; topical document similarity; Python; IR; LSA; LDA; gensim; DML-CZ},
  language     = {eng},
  location     = {Valletta, Malta},
  url          = {http://nlp.fi.muni.cz/projekty/gensim/},
}

@InProceedings{Mikolov2013DistributedRO,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  year      = {2013},
  editor    = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {26},
  file      = {:PDF/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf:PDF},
  groups    = {word2vec},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
}

@Article{Yoav2014Word2VecExplained,
  author     = {Yoav Goldberg and Omer Levy},
  journal    = {CoRR},
  title      = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
  year       = {2014},
  volume     = {abs/1402.3722},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/GoldbergL14.bib},
  eprint     = {1402.3722},
  eprinttype = {arXiv},
  file       = {:PDF/1402.3722v1.pdf:PDF},
  groups     = {word2vec},
  timestamp  = {Mon, 13 Aug 2018 16:47:34 +0200},
  url        = {http://arxiv.org/abs/1402.3722},
}

@Article{Rong2014Word2VecExplained,
  author     = {Xin Rong},
  journal    = {CoRR},
  title      = {word2vec Parameter Learning Explained},
  year       = {2014},
  volume     = {abs/1411.2738},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/Rong14.bib},
  eprint     = {1411.2738},
  eprinttype = {arXiv},
  file       = {:PDF/1411.2738v4.pdf:PDF},
  groups     = {word2vec},
  timestamp  = {Mon, 13 Aug 2018 16:45:57 +0200},
  url        = {http://arxiv.org/abs/1411.2738},
}

@Book{Firth1957ASynopsis,
  author    = {Firth, J.},
  publisher = {Longman},
  title     = {A synopsis of linguistic theory 1930-1955},
  year      = {1957},
  series    = {Studies in Linguistic Analysis, Philological},
  added-at  = {2017-11-18T14:22:30.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/2b72c0e9f0c18ca5466a86ad391997923/thoni},
  file      = {:PDF/lecture1-firth.pdf:PDF},
  interhash = {49b3d847512a3bff2a1d77cd5ea5391f},
  intrahash = {b72c0e9f0c18ca5466a86ad391997923},
  keywords  = {context distributional semantics},
  timestamp = {2017-11-18T14:22:30.000+0100},
}

@InProceedings{mikolov10_interspeech,
  author    = {Tomáš Mikolov and Martin Karafiát and Lukáš Burget and Jan Černocký and Sanjeev Khudanpur},
  booktitle = {Interspeech 2010},
  title     = {Recurrent neural network based language model},
  year      = {2010},
  pages     = {1045--1048},
  doi       = {10.21437/Interspeech.2010-343},
  file      = {:PDF/mikolov10_interspeech.pdf:PDF},
  groups    = {word2vec},
  issn      = {2958-1796},
}

@InProceedings{Morin2005Hierarchical,
  author    = {Morin, Frederic and Bengio, Yoshua},
  booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  title     = {Hierarchical Probabilistic Neural Network Language Model},
  year      = {2005},
  editor    = {Cowell, Robert G. and Ghahramani, Zoubin},
  month     = {06--08 Jan},
  note      = {Reissued by PMLR on 30 March 2021.},
  pages     = {246--252},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {R5},
  file      = {:PDF/hierarchical-nnlm-aistats05.pdf:PDF},
  groups    = {Probabilistic Neural Network Language Model},
  pdf       = {http://proceedings.mlr.press/r5/morin05a/morin05a.pdf},
  url       = {https://proceedings.mlr.press/r5/morin05a.html},
}

@Article{Bengio2003ProbablisticLM,
  author     = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
  journal    = {J. Mach. Learn. Res.},
  title      = {A neural probabilistic language model},
  year       = {2003},
  issn       = {1532-4435},
  month      = mar,
  number     = {null},
  pages      = {1137–1155},
  volume     = {3},
  abstract   = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file       = {:PDF/944919.944966.pdf:PDF;:PDF/944919.944966.pdf:PDF},
  groups     = {Probabilistic Neural Network Language Model},
  issue_date = {3/1/2003},
  numpages   = {19},
  publisher  = {JMLR.org},
}

@Article{Bojanowski2017Enriching,
  author   = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal  = {Transactions of the Association for Computational Linguistics},
  title    = {Enriching Word Vectors with Subword Information},
  year     = {2017},
  issn     = {2307-387X},
  month    = {06},
  pages    = {135-146},
  volume   = {5},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful
                    for many natural language processing tasks. Popular models that learn such
                    representations ignore the morphology of words, by assigning a distinct vector
                    to each word. This is a limitation, especially for languages with large
                    vocabularies and many rare words. In this paper, we propose a new approach based
                    on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated
                    to each character n-gram; words being represented
                    as the sum of these representations. Our method is fast, allowing to train
                    models on large corpora quickly and allows us to compute word representations
                    for words that did not appear in the training data. We evaluate our word
                    representations on nine different languages, both on word similarity and analogy
                    tasks. By comparing to recently proposed morphological word representations, we
                    show that our vectors achieve state-of-the-art performance on these tasks.},
  doi      = {10.1162/tacl_a_00051},
  eprint   = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00051/1567442/tacl\_a\_00051.pdf},
  file     = {:PDF/tacl_a_00051.pdf:PDF},
  groups   = {word2vec},
  url      = {https://doi.org/10.1162/tacl\_a\_00051},
}

@InProceedings{Pascanau2013DifficultyRNN,
  author    = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  title     = {On the difficulty of training recurrent neural networks},
  year      = {2013},
  pages     = {III–1310–III–1318},
  publisher = {JMLR.org},
  series    = {ICML'13},
  abstract  = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  file      = {:PDF/pascanu13.pdf:PDF},
  groups    = {rnn},
  location  = {Atlanta, GA, USA},
}

@InProceedings{Le2014Distributed,
  author    = {Le, Quoc and Mikolov, Tomas},
  booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
  title     = {Distributed representations of sentences and documents},
  year      = {2014},
  pages     = {II–1188–II–1196},
  publisher = {JMLR.org},
  series    = {ICML'14},
  abstract  = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  file      = {:PDF/paragraph_vector.pdf:PDF},
  groups    = {Probabilistic Neural Network Language Model},
  location  = {Beijing, China},
}

@InProceedings{Schafer2016Commoncow,
  author    = {Sch{\"a}fer, Roland},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}`16)},
  title     = {{C}ommon{COW}: Massively Huge Web Corpora from {C}ommon{C}rawl Data and a Method to Distribute them Freely under Restrictive {EU} Copyright Laws},
  year      = {2016},
  address   = {Portoro{\v{z}}, Slovenia},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Goggi, Sara and Grobelnik, Marko and Maegaard, Bente and Mariani, Joseph and Mazo, Helene and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  month     = may,
  pages     = {4500--4504},
  publisher = {European Language Resources Association (ELRA)},
  abstract  = {In this paper, I describe a method of creating massively huge web corpora from the CommonCrawl data sets and redistributing the resulting annotations in a stand-off format. Current EU (and especially German) copyright legislation categorically forbids the redistribution of downloaded material without express prior permission by the authors. Therefore, such stand-off annotations (or other derivates) are the only format in which European researchers (like myself) are allowed to re-distribute the respective corpora. In order to make the full corpora available to the public despite such restrictions, the stand-off format presented here allows anybody to locally reconstruct the full corpora with the least possible computational effort.},
  file      = {:PDF/L16-1712.pdf:PDF},
  url       = {https://aclanthology.org/L16-1712/},
}

@Article{Osowski2018SieciNeuronowe,
  author  = {Osowski Stanisław},
  journal = {Przegląd Telekomunikacyjny - Wiadomości Telekomunikacyjne},
  title   = {Głębokie sieci neuronowe i ich zastosowania w eksploracji danych},
  year    = {2018},
  doi     = {https://doi.org/10.15199/59.2018.5.2},
  file    = {:PDF/04 - Stanisław Osowski - Głębokie sieci neuronowe i ich zastosowania w eksploracji danych.pdf:PDF},
  groups  = {cnn},
}

@InProceedings{Han1995Sigmoid,
  author    = {Han, Jun and Moraga, Claudio},
  booktitle = {From Natural to Artificial Neural Computation},
  title     = {The influence of the sigmoid function parameters on the speed of backpropagation learning},
  year      = {1995},
  address   = {Berlin, Heidelberg},
  editor    = {Mira, Jos{\'e} and Sandoval, Francisco},
  pages     = {195--201},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Sigmoid function is the most commonly known function used in feed forward neural networks because of its nonlinearity and the computational simplicity of its derivative. In this paper we discuss a variant sigmoid function with three parameters that denote the dynamic range, symmetry and slope of the function respectively. We illustrate how these parameters influence the speed of backpropagation learning and introduce a hybrid sigmoidal network with different parameter configuration in different layers. By regulating and modifying the sigmoid function parameter configuration in different layers the error signal problem, oscillation problem and asymmetrical input problem can be reduced. To compare the learning capabilities and the learning rate of the hybrid sigmoidal networks with the conventional networks we have tested the two-spirals benchmark that is known to be a very difficult task for backpropagation and their relatives.},
  file      = {:PDF/3-540-59497-3_175.pdf:PDF},
  isbn      = {978-3-540-49288-7},
}

@Article{Dubey2022ActivationDL,
  author     = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  journal    = {Neurocomput.},
  title      = {Activation functions in deep learning: A comprehensive survey and benchmark},
  year       = {2022},
  issn       = {0925-2312},
  month      = sep,
  number     = {C},
  pages      = {92–108},
  volume     = {503},
  address    = {NLD},
  doi        = {10.1016/j.neucom.2022.06.111},
  file       = {:PDF/1-s2.0-S0925231222008426-main.pdf:PDF},
  issue_date = {Sep 2022},
  keywords   = {Activation Functions, Neural networks, Convolutional neural networks, Deep learning, Overview, Recurrent Neural Networks},
  numpages   = {17},
  publisher  = {Elsevier Science Publishers B. V.},
  url        = {https://doi.org/10.1016/j.neucom.2022.06.111},
}

@Book{Osowski2020SN,
  author   = {Osowski, Stanis{\l}aw},
  title    = {Sieci neuronowe do przetwarzania informacji},
  year     = {2020},
  language = {pl},
}

@Book{Osowski2023ML,
  author    = {Osowski, Stanisław and Szmurło, Robert},
  publisher = {Oficyna Wydawnicza Politechniki Warszawskiej},
  title     = {Matematyczne modele uczenia maszynowego w językach MATLAB i PYTHON},
  year      = {2024},
  isbn      = {978-83-8156-598-1},
}

@InProceedings{Wijnhoven2010SGD,
  author    = {Wijnhoven, R.G.J. and de With, P.H.N.},
  booktitle = {2010 20th International Conference on Pattern Recognition},
  title     = {Fast Training of Object Detection Using Stochastic Gradient Descent},
  year      = {2010},
  month     = {Aug},
  pages     = {424-427},
  abstract  = {Training datasets for object detection problems are typically very large and Support Vector Machine (SVM) implementations are computationally complex. As opposed to these complex techniques, we use Stochastic Gradient Descent (SGD) algorithms that use only a single new training sample in each iteration and process samples in a stream-like fashion. We have incorporated SGD optimization in an object detection framework. The object detection problem is typically highly asymmetric, because of the limited variation in object appearance, compared to the background. Incorporating SGD speeds up the optimization process significantly, requiring only a single iteration over the training set to obtain results comparable to state-of-the-art SVM techniques. SGD optimization is linearly scalable in time and the obtained speedup in computation time is two to three orders of magnitude. We show that by considering only part of the total training set, SGD converges quickly to the overall optimum.},
  doi       = {10.1109/ICPR.2010.112},
  file      = {:PDF/Fast_Training_of_Object_Detection_Using_Stochastic_Gradient_Descent.pdf:PDF},
  issn      = {1051-4651},
  keywords  = {Training;Support vector machines;Object detection;Optimization;Feature extraction;Computer vision;Pattern recognition;SVM;stochastic gradient descent;object recognition;detection;classification;histogram of oriented gradients;HOG},
}

@Misc{Werner2020speedingwordmoversdistance,
  author        = {Matheus Werner and Eduardo Laber},
  title         = {Speeding up Word Mover's Distance and its variants via properties of distances between embeddings},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1912.00509},
  file          = {:PDF/888_paper.pdf:PDF},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1912.00509},
}

@Article{Gu2018RecentAdvances,
  author     = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
  journal    = {Pattern Recogn.},
  title      = {Recent advances in convolutional neural networks},
  year       = {2018},
  issn       = {0031-3203},
  month      = may,
  number     = {C},
  pages      = {354–377},
  volume     = {77},
  abstract   = {We give an overview of the basic components of CNN.We discuss the improvements of CNN on different aspects, namely, layer design, activation function, loss function, regularization, optimization and fast computation.We introduce the applications of CNN on various tasks, including image classification, object detection, object tracking, pose estimation, text detection, visual saliency detection, action recognition, scene labeling, speech and natural language processing.We discuss the challenges in CNN and give several future research directions. In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
  address    = {USA},
  doi        = {10.1016/j.patcog.2017.10.013},
  file       = {:PDF/recent-advances-in-convolutional-neural-networks-2017.pdf:PDF},
  groups     = {cnn},
  issue_date = {May 2018},
  keywords   = {Convolutional neural network, Deep learning},
  numpages   = {24},
  publisher  = {Elsevier Science Inc.},
  url        = {https://doi.org/10.1016/j.patcog.2017.10.013},
}

@InProceedings{Iwanowski2021Fuzzy,
  author    = {Iwanowski, Marcin and Bartosiewicz, Mateusz},
  booktitle = {2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
  title     = {Describing images using fuzzy mutual position matrix and saliency-based ordering of predicates},
  year      = {2021},
  month     = {July},
  pages     = {1-8},
  abstract  = {Describing the content based on bounding boxes of objects located within the image has recently gained popularity thanks to the fast development of object detection algorithms based on deep learning. Such description, however, does not contain any information on the mutual relations between objects that may be crucial to understand the scene as a whole. In the paper, a method is proposed that extracts, from the set of bounding boxes, a scene description in the form of a list of predicates containing consecutive objects' position, referring them to previously described ones. To estimate bounding boxes' relative position, a fuzzy mutual position matrix is proposed. It contains the complete information on the scene composition stored in fuzzy 2-D position descriptors extracted from fuzzified relative bounding box coordinates by a two-stage fuzzy reasoning process. The descriptors of non-zero membership function values are next considered as potential predicates related to the image content. Their list is ordered using the saliency-based criteria to select the most relevant ones, explaining best the scene composition. From the ordered list, the algorithm extracts the final list of predicates. It contains complete and concise information on the composition of objects within the scene. Some examples of the proposed method illustrate the paper.},
  doi       = {10.1109/FUZZ45933.2021.9494549},
  file      = {:PDF/Describing_images_using_fuzzy_mutual_position_matrix_and_saliency-based_ordering_of_predicates.pdf:PDF},
  groups    = {moje},
  issn      = {1558-4739},
  keywords  = {Deep learning;Image retrieval;Object detection;Manuals;Real-time systems;Table lookup;Labeling;image description;object detection;fuzzy reasoning},
}

@InProceedings{Cohen2019Beam,
  author    = {Cohen, Eldan and Beck, Christopher},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09--15 Jun},
  pages     = {1290--1299},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.},
  groups    = {beam},
  pdf       = {http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf},
  url       = {https://proceedings.mlr.press/v97/cohen19a.html},
}

@Article{Siino2024Preprocessing,
  author   = {Marco Siino and Ilenia Tinnirello and Marco {La Cascia}},
  journal  = {Information Systems},
  title    = {Is text preprocessing still worth the time? A comparative survey on the influence of popular preprocessing methods on Transformers and traditional classifiers},
  year     = {2024},
  issn     = {0306-4379},
  pages    = {102342},
  volume   = {121},
  abstract = {With the advent of the modern pre-trained Transformers, the text preprocessing has started to be neglected and not specifically addressed in recent NLP literature. However, both from a linguistic and from a computer science point of view, we believe that even when using modern Transformers, text preprocessing can significantly impact on the performance of a classification model. We want to investigate and compare, through this study, how preprocessing impacts on the Text Classification (TC) performance of modern and traditional classification models. We report and discuss the preprocessing techniques found in the literature and their most recent variants or applications to address TC tasks in different domains. In order to assess how much the preprocessing affects classification performance, we apply the three top referenced preprocessing techniques (alone or in combination) to four publicly available datasets from different domains. Then, nine machine learning models – including modern Transformers – get the preprocessed text as input. The results presented show that an educated choice on the text preprocessing strategy to employ should be based on the task as well as on the model considered. Outcomes in this survey show that choosing the best preprocessing technique – in place of the worst – can significantly improve accuracy on the classification (up to 25%, as in the case of an XLNet on the IMDB dataset). In some cases, by means of a suitable preprocessing strategy, even a simple Naïve Bayes classifier proved to outperform (i.e., by 2% in accuracy) the best performing Transformer. We found that Transformers and traditional models exhibit a higher impact of the preprocessing on the TC performance. Our main findings are: (1) also on modern pre-trained language models, preprocessing can affect performance, depending on the datasets and on the preprocessing technique or combination of techniques used, (2) in some cases, using a proper preprocessing strategy, simple models can outperform Transformers on TC tasks, (3) similar classes of models exhibit similar level of sensitivity to text preprocessing.},
  doi      = {https://doi.org/10.1016/j.is.2023.102342},
  file     = {:PDF/1-s2.0-S0306437923001783-main.pdf:PDF},
  groups   = {preprocessing},
  keywords = {Text preprocessing, Natural Language Processing, Fake news, SVM, Bayes, Transformers, Deep learning, LSTM, Convolutional neural networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S0306437923001783},
}

@Book{Chapman2010Handbook,
  author    = {Indurkhya, Nitin and Damerau, Fred J.},
  publisher = {Chapman \& Hall/CRC},
  title     = {Handbook of Natural Language Processing},
  year      = {2010},
  edition   = {2nd},
  isbn      = {1420085921},
  abstract  = {The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition Greater prominence of statistical approaches New applications section Broader multilingual scope to include Asian and European languages, along with English An actively maintained wiki (http://handbookofnlp.cse.unsw.edu.au) that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems.},
  file      = {:PDF/Handbook Of Natural Language Processing, Second Edition Chapman and Hall Crc Machine Learning and Pattern Recognition 2010.pdf:PDF;:PDF/Handbook of Natural Language Processing 2nd edition 2010.pdf:PDF},
  groups    = {preprocessing},
}

@Book{Matlab2021,
  author    = {MATLAB},
  publisher = {The MathWorks Inc.},
  title     = {version 7.10.0 (R2010a)},
  year      = {2021},
  address   = {Natick, Massachusetts},
}

@InProceedings{Zeiler2014ZFNet,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Visualizing and Understanding Convolutional Networks},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {818--833},
  publisher = {Springer International Publishing},
  abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  file      = {:PDF/978-3-319-10590-1.pdf:PDF},
  isbn      = {978-3-319-10590-1},
}

@Misc{Hirst2017neural,
  author    = {Hirst, G and Goldberg, Y},
  title     = {Neural Network Methods for Natural Language Processing},
  year      = {2017},
  file      = {:PDF/Neural Network Methods in Natural Language Processing-Morgan and Claypool Publishers (2017) - Yoav Goldberg, Graeme Hirst.pdf:PDF},
  groups    = {lm},
  publisher = {San Rafael: Morgan \& Claypool Publishers},
}

@Article{Goodman2001Progress,
  author    = {Joshua Goodman},
  journal   = {CoRR},
  title     = {A Bit of Progress in Language Modeling},
  year      = {2001},
  volume    = {cs.CL/0108005},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/cs-CL-0108005.bib},
  file      = {:PDF/0108005v1.pdf:PDF},
  groups    = {LM},
  timestamp = {Thu, 29 Apr 2021 15:44:29 +0200},
  url       = {https://arxiv.org/abs/cs/0108005},
}

@Article{Chen1998SmoothingTech,
  author  = {Chen, Stanley F. and Joshua Goodman},
  journal = {Harvard Computer Science Group Technical Report TR-10-98.},
  title   = {. An Empirical Study of Smoothing Techniquesfor Language Modeling.},
  year    = {1998},
  file    = {:PDF/tr-10-98.pdf:PDF},
  groups  = {lm},
}

@InProceedings{Ponte1998LMInformationRetrieval,
  author    = {Ponte, Jay M. and Croft, W. Bruce},
  booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {A language modeling approach to information retrieval},
  year      = {1998},
  address   = {New York, NY, USA},
  pages     = {275–281},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '98},
  doi       = {10.1145/290941.291008},
  file      = {:PDF/290941.291008.pdf:PDF},
  groups    = {lm},
  isbn      = {1581130155},
  location  = {Melbourne, Australia},
  numpages  = {7},
  url       = {https://doi.org/10.1145/290941.291008},
}

@InProceedings{Song1999GeneralLM,
  author    = {Song, Fei and Croft, W. Bruce},
  booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
  title     = {A general language model for information retrieval},
  year      = {1999},
  address   = {New York, NY, USA},
  pages     = {316–321},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '99},
  abstract  = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
  doi       = {10.1145/319950.320022},
  file      = {:PDF/319950.320022.pdf:PDF},
  groups    = {lm},
  isbn      = {1581131461},
  keywords  = {statistical language modeling, model combinations, good-turing estimate, curve-fitting functions},
  location  = {Kansas City, Missouri, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/319950.320022},
}

@Article{Almutiri2022Markov,
  author    = {Talal Almutiri, Farrukh Nadeem},
  journal   = {Int. J. Inf. Technol. Comput. Sci.},
  title     = {Markov Models applications in natural language processing: A survey},
  year      = {2022},
  month     = apr,
  number    = {2},
  pages     = {1--16},
  volume    = {14},
  abstract  = {Markov models are one of the widely used techniques in machine
               learning to process natural language. Markov Chains and Hidden
               Markov Models are stochastic techniques employed for modeling
               systems that are dynamic and where the future state relies on
               the current state. The Markov chain, which generates a sequence
               of words to create a complete sentence, is frequently used in
               generating natural language. The hidden Markov model is employed
               in named-entity recognition and the tagging of parts of speech,
               which tries to predict hidden tags based on observed words. This
               paper reviews Markov models' use in three applications of
               natural language processing (NLP): natural language generation,
               named-entity recognition, and parts of speech tagging. Nowadays,
               researchers try to reduce dependence on lexicon or annotation
               tasks in NLP. In this paper, we have focused on Markov Models as
               a stochastic approach to process NLP. A literature review was
               conducted to summarize research attempts with focusing on
               methods/techniques that used Markov Models to process NLP, their
               advantages, and disadvantages. Most NLP research studies apply
               supervised models with the improvement of using Markov models to
               decrease the dependency on annotation tasks. Some others
               employed unsupervised solutions for reducing dependence on a
               lexicon or labeled datasets.},
  file      = {:PDF/IJITCS-V14-N2-1.pdf:PDF},
  groups    = {lm},
  publisher = {MECS Publisher},
}

@Article{Qi2025Markov,
  author    = {Qi, Zhenhan},
  journal   = {Theoretical and Natural Science},
  title     = {An analysis of Markov models applications},
  year      = {2025},
  month     = mar,
  number    = {1},
  pages     = {82--87},
  volume    = {92},
  abstract  = {Markov models are indispensable in numerous scientific and
               technological domains due to their capacity to forecast future
               states based just on present conditions, rather than historical
               ones. The paper provides an analysis of the application of
               Markov models in different fields, emphasizing their importance
               in modern science and technology. The paper explores the basic
               concepts of Markov Processes, Markov Chains, Markov Decision
               Processes (MDPs), and Hidden Markov Models (HMMs) and insights
               into their foundations. Properties of Markov chains, including
               steady-state distributions, ergodicity, and absorbing states,
               are discussed, along with their implications for long-term
               behavior and convergence. The paper also examines in detail the
               application of Markov models to natural language processing,
               human resource prediction, and personalized recommender systems,
               showing how they work in word labeling, text generation, machine
               translation, labor supply prediction, and adapting to changes in
               user preferences. At last, the paper discusses challenges such
               as data sparsity, model scalability, and nonlinearity, and
               suggests future research directions, including combining deep
               learning with Markov models and leveraging big data.},
  file      = {:PDF/An_Analysis_of_Markov_Models_Applications.pdf:PDF},
  groups    = {lm},
  publisher = {EWA Publishing},
}

@InProceedings{Cho2014Properties,
  author    = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  title     = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  year      = {2014},
  address   = {Doha, Qatar},
  editor    = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
  month     = oct,
  pages     = {103--111},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/W14-4012},
  file      = {:PDF/W14-4012.pdf:PDF},
  groups    = {lm},
  url       = {https://aclanthology.org/W14-4012/},
}

@Article{Elman1990Cognitive,
  author   = {Jeffrey L. Elman},
  journal  = {Cognitive Science},
  title    = {Finding structure in time},
  year     = {1990},
  issn     = {0364-0213},
  number   = {2},
  pages    = {179-211},
  volume   = {14},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  doi      = {https://doi.org/10.1016/0364-0213(90)90002-E},
  url      = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
}

@Article{Shao2021Sequence,
  author   = {Shao, Chenze and Feng, Yang and Zhang, Jinchao and Meng, Fandong and Zhou, Jie},
  journal  = {Computational Linguistics},
  title    = {Sequence-Level Training for Non-Autoregressive Neural Machine Translation},
  year     = {2021},
  issn     = {0891-2017},
  month    = {12},
  number   = {4},
  pages    = {891-925},
  volume   = {47},
  abstract = {In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency applications. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup by generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not optimal because the output of NAT cannot be properly evaluated due to the multimodality problem. In this article, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. First, we propose training NAT models to optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperform the conventional method by reducing the variance of gradient estimation. Second, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-N-grams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated efficiently without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En↔De, WMT16 En↔Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable performance on all translation tasks. The source code is available at https://github.com/ictnlp/Seq-NAT.},
  doi      = {10.1162/coli_a_00421},
  eprint   = {https://direct.mit.edu/coli/article-pdf/47/4/891/1979393/coli\_a\_00421.pdf},
  file     = {:PDF/coli_a_00421.pdf:PDF},
  url      = {https://doi.org/10.1162/coli\_a\_00421},
}

@Misc{Cahyono2024automatedimagecaptioningcnns,
  author        = {Joshua Adrian Cahyono and Jeremy Nathan Jusuf},
  title         = {Automated Image Captioning with CNNs and Transformers},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2412.10511},
  file          = {:PDF/2412.10511v1.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2412.10511},
}

@InProceedings{Gao2019SCST,
  author    = {Gao, Junlong and Wang, Shiqi and Wang, Shanshe and Ma, Siwei and Gao, Wen},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Self-Critical N-Step Training for Image Captioning }},
  year      = {2019},
  address   = {Los Alamitos, CA, USA},
  month     = Jun,
  pages     = {6293-6301},
  publisher = {IEEE Computer Society},
  abstract  = {Existing methods for image captioning are usually trained by cross entropy loss, which leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. Recently it has been shown that these two issues can be addressed by incorporating techniques from reinforcement learning, where one of the popular techniques is the advantage actor-critic algorithm that calculates per-token advantage by estimating state value with a parametrized estimator at the cost of introducing estimation bias. In this paper, we estimate state value without using a parametrized value estimator. With the properties of image captioning, namely, the deterministic state transition function and the sparse reward, state value is equivalent to its preceding state-action value, and we reformulate advantage function by simply replacing the former with the latter. Moreover, the reformulated advantage is extended to n-step, which can generally increase the absolute value of the mean of reformulated advantage while lowering variance. Then two kinds of rollout are adopted to estimate state-action value, which we call self-critical n-step training. Empirically we find that our method can obtain better performance compared to the state-of-the-art methods that use the sequence level advantage and parametrized estimator respectively on the widely used MSCOCO benchmark.},
  doi       = {10.1109/CVPR.2019.00646},
  file      = {:PDF/a20191230181.pdf:PDF},
  keywords  = {Training;Measurement;Monte Carlo methods;Computational modeling;Training data;Reinforcement learning;Benchmark testing},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00646},
}

@Article{Mienye2024GRU,
  author         = {Mienye, Ibomoiye Domor and Swart, Theo G. and Obaido, George},
  journal        = {Information},
  title          = {Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications},
  year           = {2024},
  issn           = {2078-2489},
  number         = {9},
  volume         = {15},
  abstract       = {Recurrent neural networks (RNNs) have significantly advanced the field of machine learning (ML) by enabling the effective processing of sequential data. This paper provides a comprehensive review of RNNs and their applications, highlighting advancements in architectures, such as long short-term memory (LSTM) networks, gated recurrent units (GRUs), bidirectional LSTM (BiLSTM), echo state networks (ESNs), peephole LSTM, and stacked LSTM. The study examines the application of RNNs to different domains, including natural language processing (NLP), speech recognition, time series forecasting, autonomous vehicles, and anomaly detection. Additionally, the study discusses recent innovations, such as the integration of attention mechanisms and the development of hybrid models that combine RNNs with convolutional neural networks (CNNs) and transformer architectures. This review aims to provide ML researchers and practitioners with a comprehensive overview of the current state and future directions of RNN research.},
  article-number = {517},
  doi            = {10.3390/info15090517},
  file           = {:PDF/information-15-00517.pdf:PDF},
  groups         = {GRU},
  url            = {https://www.mdpi.com/2078-2489/15/9/517},
}

@Misc{Ralf2019LSTM,
  author        = {Ralf C. Staudemeyer and Eric Rothstein Morris},
  title         = {Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1909.09586},
  file          = {:PDF/1909.09586v1.pdf:PDF},
  groups        = {LSTM},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1909.09586},
}

@Article{Haller2023LSTMPerformance,
  author         = {Bolboacă, Roland and Haller, Piroska},
  journal        = {Mathematics},
  title          = {Performance Analysis of Long Short-Term Memory Predictive Neural Networks on Time Series Data},
  year           = {2023},
  issn           = {2227-7390},
  number         = {6},
  volume         = {11},
  abstract       = {Long short-term memory neural networks have been proposed as a means of creating accurate models from large time series data originating from various fields. These models can further be utilized for prediction, control, or anomaly-detection algorithms. However, finding the optimal hyperparameters to maximize different performance criteria remains a challenge for both novice and experienced users. Hyperparameter optimization algorithms can often be a resource-intensive and time-consuming task, particularly when the impact of the hyperparameters on the performance of the neural network is not comprehended or known. Teacher forcing denotes a procedure that involves feeding the ground truth output from the previous time-step as input to the current time-step during training, while during testing feeding back the predicted values. This paper presents a comprehensive examination of the impact of hyperparameters on long short-term neural networks, with and without teacher forcing, on prediction performance. The study includes testing long short-term memory neural networks, with two variations of teacher forcing, in two prediction modes, using two configurations (i.e., multi-input single-output and multi-input multi-output) on a well-known chemical process simulation dataset. Furthermore, this paper demonstrates the applicability of a long short-term memory neural network with a modified teacher forcing approach in a process state monitoring system. Over 100,000 experiments were conducted with varying hyperparameters and in multiple neural network operation modes, revealing the direct impact of each tested hyperparameter on the training and testing procedures.},
  article-number = {1432},
  doi            = {10.3390/math11061432},
  file           = {:PDF/mathematics-11-01432-v2.pdf:PDF},
  groups         = {lstm},
  url            = {https://www.mdpi.com/2227-7390/11/6/1432},
}

@Misc{Wang2022endtoendtransformerbasedmodel,
  author        = {Yiyu Wang and Jungang Xu and Yingfei Sun},
  title         = {End-to-End Transformer Based Model for Image Captioning},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2203.15350},
  file          = {:PDF/20160-13-24173-1-2-20220628.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2203.15350},
}

@InProceedings{Nguyen2022GRIT,
  author    = {Nguyen, Van-Quang and Suganuma, Masanori and Okatani, Takayuki},
  booktitle = {Computer Vision -- ECCV 2022},
  title     = {GRIT: Faster and Better Image Captioning Transformer Using Dual Visual Features},
  year      = {2022},
  address   = {Cham},
  editor    = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  pages     = {167--184},
  publisher = {Springer Nature Switzerland},
  abstract  = {Current state-of-the-art methods for image captioning employ region-based features, as they provide object-level information that is essential to describe the content of images; they are usually extracted by an object detector such as Faster R-CNN. However, they have several issues, such as lack of contextual information, the risk of inaccurate detection, and the high computational cost. The first two could be resolved by additionally using grid-based features. However, how to extract and fuse these two types of features is uncharted. This paper proposes a Transformer-only neural architecture, dubbed GRIT (Grid- and Region-based Image captioning Transformer), that effectively utilizes the two visual features to generate better captions. GRIT replaces the CNN-based detector employed in previous methods with a DETR-based one, making it computationally faster. Moreover, its monolithic design consisting only of Transformers enables end-to-end training of the model. This innovative design and the integration of the dual visual features bring about significant performance improvement. The experimental results on several image captioning benchmarks show that GRIT outperforms previous methods in inference accuracy and speed.},
  file      = {:PDF/136960165.pdf:PDF},
  isbn      = {978-3-031-20059-5},
}

@Misc{bonillasalvador2024pixloredatasetdrivenapproachrich,
  author        = {Diego Bonilla-Salvador and Marcelino Martínez-Sober and Joan Vila-Francés and Antonio José Serrano-López and Pablo Rodríguez-Belenguer and Fernando Mateo},
  title         = {PixLore: A Dataset-driven Approach to Rich Image Captioning},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2312.05349},
  file          = {:PDF/2312.05349v2.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2312.05349},
}

@InProceedings{Jiang2013Salient,
  author    = {Jiang, Huaizu and Wang, Jingdong and Yuan, Zejian and Wu, Yang and Zheng, Nanning and Li, Shipeng},
  booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Salient Object Detection: A Discriminative Regional Feature Integration Approach},
  year      = {2013},
  month     = {June},
  pages     = {2083-2090},
  abstract  = {Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional background ness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, background ness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.},
  doi       = {10.1109/CVPR.2013.271},
  file      = {:PDF/2312.05349v2.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {Image segmentation;Vectors;Image color analysis;Feature extraction;Object detection;Histograms;Training},
}

@Article{Ma2023Grid,
  author   = {Yiwei Ma and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Rongrong Ji},
  journal  = {Pattern Recognition},
  title    = {Towards local visual modeling for image captioning},
  year     = {2023},
  issn     = {0031-3203},
  pages    = {109420},
  volume   = {138},
  abstract = {In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty of local object recognition during captioning. LSF is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. With these two novel designs, the proposed LSTNet can model the local visual information of grid features to improve the captioning quality. To validate LSTNet, we conduct extensive experiments on the competitive MS-COCO benchmark. The experimental results show that LSTNet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is also verified on the Flickr8k and Flickr30k datasets. The source code is available on GitHub: https://www.github.com/xmu-xiaoma666/LSTNet.},
  doi      = {https://doi.org/10.1016/j.patcog.2023.109420},
  file     = {:PDF/1-s2.0-S0031320323001218-main.pdf:PDF},
  keywords = {Image captioning, Attention mechanism, Local visual modeling},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320323001218},
}

@Article{Samar2023Enhanced,
  author   = {Samar Elbedwehy and T. Medhat and Taher Hamza and Mohammed F. Alrahmawy},
  journal  = {Computer Systems Science and Engineering},
  title    = {Enhanced Image Captioning Using Features Concatenation and Efficient Pre-Trained Word Embedding},
  year     = {2023},
  number   = {3},
  pages    = {3637--3652},
  volume   = {46},
  abstract = {One of the issues in Computer Vision is the automatic development of descriptions for images, sometimes known as image captioning. Deep Learning techniques have made significant progress in this area. The typical architecture of image captioning systems consists mainly of an image feature extractor subsystem followed by a caption generation lingual subsystem. This paper aims to find optimized models for these two subsystems. For the image feature extraction subsystem, the research tested eight different concatenations of pairs of vision models to get among them the most expressive extracted feature vector of the image. For the caption generation lingual subsystem, this paper tested three different pre-trained language embedding models: Glove (Global Vectors for Word Representation), BERT (Bidirectional Encoder Representations from Transformers), and TaCL (Token-aware Contrastive Learning), to select from them the most accurate pre-trained language embedding model. Our experiments showed that building an image captioning system that uses a concatenation of the two Transformer based models SWIN (Shifted window) and PVT (Pyramid Vision Transformer) as an image feature extractor, combined with the TaCL language embedding model is the best result among the other combinations.},
  doi      = {10.32604/csse.2023.038376},
  file     = {:PDF/1-s2.0-S0031320323001218-main.pdf:PDF},
  url      = {http://www.techscience.com/csse/v46n3/52216},
}

@InProceedings{Luo2024unleashing,
  author    = {Luo, Jianjie and Chen, Jingwen and Li, Yehao and Pan, Yingwei and Feng, Jianlin and Chao, Hongyang and Yao, Ting},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title     = {Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning},
  year      = {2024},
  file      = {:PDF/2501.00437v1.pdf:PDF},
}

@Article{Pengpeng2022S2,
  author    = {Pengpeng Zeng and Haonan Zhang and Jingkuan Song and Lianli Gao},
  title     = {S2 Transformer for Image Captioning},
  year      = {2022},
  pages     = {1608--1614},
  booktitle = {IJCAI},
  file      = {:PDF/0224.pdf:PDF;:PDF/0224.pdf:PDF},
}

@Article{Khan2022SI,
  author       = {Khan, Rashid and Islam, M Shujah and Kanwal, Khadija and Iqbal, Mansoor and Hossain, Md Imran and Ye, Zhongfu},
  journal      = {arXiv [cs.CL]},
  title        = {A deep neural framework for image caption generation using {GRU-based} attention mechanism},
  year         = {2022},
  abstract     = {Image captioning is a fast-growing research field of computer
                  vision and natural language processing that involves creating
                  text explanations for images. This study aims to develop a
                  system that uses a pre-trained convolutional neural network
                  (CNN) to extract features from an image, integrates the
                  features with an attention mechanism, and creates captions
                  using a recurrent neural network (RNN). To encode an image
                  into a feature vector as graphical attributes, we employed
                  multiple pre-trained convolutional neural networks. Following
                  that, a language model known as GRU is chosen as the decoder
                  to construct the descriptive sentence. In order to increase
                  performance, we merge the Bahdanau attention model with GRU
                  to allow learning to be focused on a specific portion of the
                  image. On the MSCOCO dataset, the experimental results
                  achieve competitive performance against state-of-the-art
                  approaches.},
  file         = {:PDF/2203.01594v1.pdf:PDF},
  primaryclass = {cs.CL},
  publisher    = {arXiv},
}

@Article{Lv2019Local,
  author         = {Lv, Yafei and Zhang, Xiaohan and Xiong, Wei and Cui, Yaqi and Cai, Mi},
  journal        = {Remote Sensing},
  title          = {An End-to-End Local-Global-Fusion Feature Extraction Network for Remote Sensing Image Scene Classification},
  year           = {2019},
  issn           = {2072-4292},
  number         = {24},
  volume         = {11},
  abstract       = {Remote sensing image scene classification (RSISC) is an active task in the remote sensing community and has attracted great attention due to its wide applications. Recently, the deep convolutional neural networks (CNNs)-based methods have witnessed a remarkable breakthrough in performance of remote sensing image scene classification. However, the problem that the feature representation is not discriminative enough still exists, which is mainly caused by the characteristic of inter-class similarity and intra-class diversity. In this paper, we propose an efficient end-to-end local-global-fusion feature extraction (LGFFE) network for a more discriminative feature representation. Specifically, global and local features are extracted from channel and spatial dimensions respectively, based on a high-level feature map from deep CNNs. For the local features, a novel recurrent neural network (RNN)-based attention module is first proposed to capture the spatial layout information and context information across different regions. Gated recurrent units (GRUs) is then exploited to generate the important weight of each region by taking a sequence of features from image patches as input. A reweighed regional feature representation can be obtained by focusing on the key region. Then, the final feature representation can be acquired by fusing the local and global features. The whole process of feature extraction and feature fusion can be trained in an end-to-end manner. Finally, extensive experiments have been conducted on four public and widely used datasets and experimental results show that our method LGFFE outperforms baseline methods and achieves state-of-the-art results.},
  article-number = {3006},
  doi            = {10.3390/rs11243006},
  file           = {:PDF/remotesensing-11-03006.pdf:PDF},
  url            = {https://www.mdpi.com/2072-4292/11/24/3006},
}

@Article{Yang2024image,
  author  = {Yang, Zhen and Zhou, Ziwei and Wang, Chaoyang and Xu, Liang},
  journal = {IAENG International Journal of Computer Science},
  title   = {Image Guidance Encoder-Decoder Model in Image Captioning and Its Application.},
  year    = {2024},
  number  = {9},
  volume  = {51},
  file    = {:PDF/IJCS_51_9_17.pdf:PDF},
}

@Article{e26100876,
  author         = {Zhao, Fengzhi and Yu, Zhezhou and Wang, Tao and Lv, Yi},
  journal        = {Entropy},
  title          = {Image Captioning Based on Semantic Scenes},
  year           = {2024},
  issn           = {1099-4300},
  number         = {10},
  volume         = {26},
  abstract       = {With the development of artificial intelligence and deep learning technologies, image captioning has become an important research direction at the intersection of computer vision and natural language processing. The purpose of image captioning is to generate corresponding natural language descriptions by understanding the content of images. This technology has broad application prospects in fields such as image retrieval, autonomous driving, and visual question answering. Currently, many researchers have proposed region-based image captioning methods. These methods generate captions by extracting features from different regions of an image. However, they often rely on local features of the image and overlook the understanding of the overall scene, leading to captions that lack coherence and accuracy when dealing with complex scenes. Additionally, image captioning methods are unable to extract complete semantic information from visual data, which may lead to captions with biases and deficiencies. Due to these reasons, existing methods struggle to generate comprehensive and accurate captions. To fill this gap, we propose the Semantic Scenes Encoder (SSE) for image captioning. It first extracts a scene graph from the image and integrates it into the encoding of the image information. Then, it extracts a semantic graph from the captions and preserves semantic information through a learnable attention mechanism, which we refer to as the dictionary. During the generation of captions, it combines the encoded information of the image and the learned semantic information to generate complete and accurate captions. To verify the effectiveness of the SSE, we tested the model on the MSCOCO dataset. The experimental results show that the SSE improves the overall quality of the captions. The improvement in scores across multiple evaluation metrics further demonstrates that the SSE possesses significant advantages when processing identical images.},
  article-number = {876},
  doi            = {10.3390/e26100876},
  file           = {:PDF/entropy-26-00876-v2-2.pdf:PDF},
  pubmedid       = {39451952},
  url            = {https://www.mdpi.com/1099-4300/26/10/876},
}

@Article{jimaging9080162,
  author         = {Hu, Wenjin and Qiao, Lang and Kang, Wendong and Shi, Xinyue},
  journal        = {Journal of Imaging},
  title          = {Thangka Image Captioning Based on Semantic Concept Prompt and Multimodal Feature Optimization},
  year           = {2023},
  issn           = {2313-433X},
  number         = {8},
  volume         = {9},
  abstract       = {Thangka images exhibit a high level of diversity and richness, and the existing deep learning-based image captioning methods generate poor accuracy and richness of Chinese captions for Thangka images. To address this issue, this paper proposes a Semantic Concept Prompt and Multimodal Feature Optimization network (SCAMF-Net). The Semantic Concept Prompt (SCP) module is introduced in the text encoding stage to obtain more semantic information about the Thangka by introducing contextual prompts, thus enhancing the richness of the description content. The Multimodal Feature Optimization (MFO) module is proposed to optimize the correlation between Thangka images and text. This module enhances the correlation between the image features and text features of the Thangka through the Captioner and Filter to more accurately describe the visual concept features of the Thangka. The experimental results demonstrate that our proposed method outperforms baseline models on the Thangka dataset in terms of BLEU-4, METEOR, ROUGE, CIDEr, and SPICE by 8.7%, 7.9%, 8.2%, 76.6%, and 5.7%, respectively. Furthermore, this method also exhibits superior performance compared to the state-of-the-art methods on the public MSCOCO dataset.},
  article-number = {162},
  doi            = {10.3390/jimaging9080162},
  file           = {:PDF/jimaging-09-00162.pdf:PDF},
  pubmedid       = {37623694},
  url            = {https://www.mdpi.com/2313-433X/9/8/162},
}

@Article{sym13071184,
  author         = {Tian, Peng and Mo, Hongwei and Jiang, Laihao},
  journal        = {Symmetry},
  title          = {Image Caption Generation Using Multi-Level Semantic Context Information},
  year           = {2021},
  issn           = {2073-8994},
  number         = {7},
  volume         = {13},
  abstract       = {Object detection, visual relationship detection, and image captioning, which are the three main visual tasks in scene understanding, are highly correlated and correspond to different semantic levels of scene image. However, the existing captioning methods convert the extracted image features into description text, and the obtained results are not satisfactory. In this work, we propose a Multi-level Semantic Context Information (MSCI) network with an overall symmetrical structure to leverage the mutual connections across the three different semantic layers and extract the context information between them, to solve jointly the three vision tasks for achieving the accurate and comprehensive description of the scene image. The model uses a feature refining structure to mutual connections and iteratively updates the different semantic features of the image. Then a context information extraction network is used to extract the context information between the three different semantic layers, and an attention mechanism is introduced to improve the accuracy of image captioning while using the context information between the different semantic layers to improve the accuracy of object detection and relationship detection. Experiments on the VRD and COCO datasets demonstrate that our proposed model can leverage the context information between semantic layers to improve the accuracy of those visual tasks generation.},
  article-number = {1184},
  doi            = {10.3390/sym13071184},
  file           = {:PDF/symmetry-13-01184-v2.pdf:PDF},
  url            = {https://www.mdpi.com/2073-8994/13/7/1184},
}

@InProceedings{Gu2018,
  author    = {Gu, Jiayuan and Hu, Han and Wang, Liwei and Wei, Yichen and Dai, Jifeng},
  booktitle = {Computer Vision -- ECCV 2018},
  title     = {Learning Region Features for Object Detection},
  year      = {2018},
  address   = {Cham},
  editor    = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  pages     = {392--406},
  publisher = {Springer International Publishing},
  abstract  = {While most steps in the modern object detection methods are learnable, the region feature extraction step remains largely hand-crafted, featured by RoI pooling methods. This work proposes a general viewpoint that unifies existing region feature extraction methods and a novel method that is end-to-end learnable. The proposed method removes most heuristic choices and outperforms its RoI pooling counterparts. It moves further towards fully learnable object detection.},
  file      = {:PDF/978-3-030-01258-8.pdf:PDF},
  isbn      = {978-3-030-01258-8},
}

@Article{Cai2020PanOptic,
  author         = {Cai, Wenjie and Xiong, Zheng and Sun, Xianfang and Rosin, Paul L. and Jin, Longcun and Peng, Xinyi},
  journal        = {Applied Sciences},
  title          = {Panoptic Segmentation-Based Attention for Image Captioning},
  year           = {2020},
  issn           = {2076-3417},
  number         = {1},
  volume         = {10},
  abstract       = {Image captioning is the task of generating textual descriptions of images. In order to obtain a better image representation, attention mechanisms have been widely adopted in image captioning. However, in existing models with detection-based attention, the rectangular attention regions are not fine-grained, as they contain irrelevant regions (e.g., background or overlapped regions) around the object, making the model generate inaccurate captions. To address this issue, we propose panoptic segmentation-based attention that performs attention at a mask-level (i.e., the shape of the main part of an instance). Our approach extracts feature vectors from the corresponding segmentation regions, which is more fine-grained than current attention mechanisms. Moreover, in order to process features of different classes independently, we propose a dual-attention module which is generic and can be applied to other frameworks. Experimental results showed that our model could recognize the overlapped objects and understand the scene better. Our approach achieved competitive performance against state-of-the-art methods. We made our code available.},
  article-number = {391},
  doi            = {10.3390/app10010391},
  file           = {:PDF/applsci-10-00391.pdf:PDF},
  url            = {https://www.mdpi.com/2076-3417/10/1/391},
}

@Misc{Jegham2025yoloevolutioncomprehensivebenchmark,
  author        = {Nidhal Jegham and Chan Young Koh and Marwan Abdelatti and Abdeltawab Hendawi},
  title         = {YOLO Evolution: A Comprehensive Benchmark and Architectural Review of YOLOv12, YOLO11, and Their Previous Versions},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2411.00201},
  file          = {:PDF/2411.00201v2.pdf:PDF},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2411.00201},
}

@Article{Terven2023Yolo,
  author   = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
  journal  = {Machine Learning and Knowledge Extraction},
  title    = {A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS},
  year     = {2023},
  issn     = {2504-4990},
  number   = {4},
  pages    = {1680--1716},
  volume   = {5},
  abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO’s evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO’s development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  doi      = {10.3390/make5040083},
  file     = {:PDF/make-05-00083-v2.pdf:PDF},
  url      = {https://www.mdpi.com/2504-4990/5/4/83},
}

@Article{Biswas2020,
  author   = {Biswas, Rajarshi and Barz, Michael and Sonntag, Daniel},
  journal  = {KI - K{\"u}nstliche Intelligenz},
  title    = {Towards Explanatory Interactive Image Captioning Using Top-Down and Bottom-Up Features, Beam Search and Re-ranking},
  year     = {2020},
  issn     = {1610-1987},
  month    = {Dec},
  number   = {4},
  pages    = {571-584},
  volume   = {34},
  abstract = {Image captioning is a challenging multimodal task. Significant improvements could be obtained by deep learning. Yet, captions generated by humans are still considered better, which makes it an interesting application for interactive machine learning and explainable artificial intelligence methods. In this work, we aim at improving the performance and explainability of the state-of-the-art method Show, Attend and Tell by augmenting their attention mechanism using additional bottom-up features. We compute visual attention on the joint embedding space formed by the union of high-level features and the low-level features obtained from the object specific salient regions of the input image. We embed the content of bounding boxes from a pre-trained Mask R-CNN model. This delivers state-of-the-art performance, while it provides explanatory features. Further, we discuss how interactive model improvement can be realized through re-ranking caption candidates using beam search decoders and explanatory features. We show that interactive re-ranking of beam search candidates has the potential to outperform the state-of-the-art in image captioning.},
  day      = {01},
  doi      = {10.1007/s13218-020-00679-2},
  url      = {https://doi.org/10.1007/s13218-020-00679-2},
}

@Article{Fox1989StopWords,
  author     = {Fox, Christopher},
  journal    = {SIGIR Forum},
  title      = {A stop list for general text},
  year       = {1989},
  issn       = {0163-5840},
  month      = sep,
  number     = {1–2},
  pages      = {19–21},
  volume     = {24},
  abstract   = {A stop list, or negative dictionary is a device used in automatic indexing to filter out words that would make poor index terms. Traditionally stop lists are supposed to have included only the most frequently occurring words. In practice, however, stop lists have tended to include infrequently occurring words, and have not included many frequently occurring words. Infrequently occurring words seem to have been included because stop list compilers have not, for whatever reason, consulted empirical studies of word frequencies. Frequently occurring words seem to have been left out for the same reason, and also because many of them might still be important as index terms.This paper reports an exercise in generating a stop list for general text based on the Brown corpus of 1,014,000 words drawn from a broad range of literature in English. We start with a list of tokens occurring more than 300 times in the Brown corpus. From this list of 278 words, 32 are culled on the grounds that they are too important as potential index terms. Twenty-six words are then added to the list in the belief that they may occur very frequently in certain kinds of literature. Finally, 149 words are added to the list because the finite state machine based filter in which this list is intended to be used is able to filter them at almost no cost. The final product is a list of 421 stop words that should be maximally efficient and effective in filtering the most frequently occurring and semantically neutral words in general literature in English.},
  address    = {New York, NY, USA},
  doi        = {10.1145/378881.378888},
  file       = {:PDF/378881.378888.pdf:PDF},
  issue_date = {Fall 89/Winter 90},
  numpages   = {3},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/378881.378888},
}

@Article{Sarica2021StopWords,
  author    = {Sarica, Serhad AND Luo, Jianxi},
  journal   = {PLOS ONE},
  title     = {Stopwords in technical language processing},
  year      = {2021},
  month     = {08},
  number    = {8},
  pages     = {1-13},
  volume    = {16},
  abstract  = {There are increasing applications of natural language processing techniques for information retrieval, indexing, topic modelling and text classification in engineering contexts. A standard component of such tasks is the removal of stopwords, which are uninformative components of the data. While researchers use readily available stopwords lists that are derived from non-technical resources, the technical jargon of engineering fields contains their own highly frequent and uninformative words and there exists no standard stopwords list for technical language processing applications. Here we address this gap by rigorously identifying generic, insignificant, uninformative stopwords in engineering texts beyond the stopwords in general texts, based on the synthesis of alternative statistical measures such as term frequency, inverse document frequency, and entropy, and curating a stopwords dataset ready for technical language processing applications.},
  doi       = {10.1371/journal.pone.0254937},
  publisher = {Public Library of Science},
  url       = {https://doi.org/10.1371/journal.pone.0254937},
}

@InProceedings{He2019HumanAtt,
  author    = {He, Sen and Tavakoli, Hamed Rezazadegan and Borji, Ali and Pugeault, Nicolas},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Human Attention in Image Captioning: Dataset and Analysis},
  year      = {2019},
  month     = {Oct},
  pages     = {8528-8537},
  abstract  = {In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human atten- tion and language constructs during perception and sen- tence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image caption- ing. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Hu- mans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between de- scribed objects and attended objects (97% of the described objects are being attended), (3) a convolutional neural net- work as feature encoder accounts for human-attended re- gions during image captioning to a great extent (around 78%), (4) soft-attention mechanism differs from human at- tention, both spatially and temporally, and there is low correlation between caption scores and attention consis- tency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model's performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/ Human-Attention-in-Image-Captioning.},
  doi       = {10.1109/ICCV.2019.00862},
  file      = {:PDF/Human_Attention_in_Image_Captioning_Dataset_and_Analysis.pdf:PDF},
  groups    = {attention},
  issn      = {2380-7504},
  keywords  = {Task analysis;Visualization;Data collection;Cows;Computer vision;Computational modeling;Adaptation models},
}

@Article{Ho2021EncDec,
  author   = {Seoung-Ho Choi and Seoung Yeon Jo and Sung Hoon Jung},
  journal  = {ICT Express},
  title    = {Component based comparative analysis of each module in image captioning},
  year     = {2021},
  issn     = {2405-9595},
  number   = {1},
  pages    = {121-125},
  volume   = {7},
  abstract = {Image captioning is a task to generate a new caption using the training data of the image and caption. Since existing deep learning is a black-box model, it is crucial to analyze the influence on each module for understanding the model. In this paper, we analyze the impact of the five modules and do a comparative analysis according to three losses and two optimizations using two datasets. From extensive experiments, the best component of each module has been identified as an improved method.},
  doi      = {https://doi.org/10.1016/j.icte.2020.08.004},
  file     = {:PDF/1-s2.0-S2405959520301429-main.pdf:PDF},
  groups   = {attention},
  keywords = {Image captioning, Comparative analysis},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405959520301429},
}

@InProceedings{NagaDurga2022AtttBasedComparisonOfImgCapt,
  author    = {NagaDurga, Cheboyina Sindhu and Anuradha, T.},
  booktitle = {Advances in Micro-Electronics, Embedded Systems and IoT},
  title     = {Attention-Based Comparison of Automatic Image Caption Generation Encoders},
  year      = {2022},
  address   = {Singapore},
  editor    = {Chakravarthy, V. V. S. S. S. and Flores-Fuentes, Wendy and Bhateja, Vikrant and Biswal, B.N.},
  pages     = {157--167},
  publisher = {Springer Nature Singapore},
  abstract  = {Generating captions to images has still been a challenging task. Image captioning is a combination of both computer vision and natural language processing (NLP) which has many applications in social networking and is advantageous to people who are impaired visually. There are different encoders (CNN) for feature extraction from the input image and decoders (RNN) for the language model and attention mechanisms which concentrate on relevant data to improve the model's performance. In this paper, for the comparison of encoders, VGG19 and ResNet152 are used and LSTM as a decoder to generate captions. Along with the decoder, visual attention mechanism is used which allows the human or a system to concentrate on the essential parts from the input data. Visual attention mechanism is also widely used in video analytics. The proposed work uses the MSCOCO dataset for both architectures. The generated captions are then compared with the actual captions using the BLEU score. From the proposed models, the generated captions are 80 per cent accurate.},
  isbn      = {978-981-16-8550-7},
}

@InProceedings{Miltenburg2016Negation,
  author = {Miltenburg, Emiel and Morante, Roser and Elliott, Desmond},
  title  = {Pragmatic Factors in Image Description: The Case of Negations},
  year   = {2016},
  month  = {06},
  pages  = {54-59},
  doi    = {10.18653/v1/W16-3207},
  file   = {:PDF/W16-3207.pdf:PDF},
  groups = {negacja},
}

@PhdThesis{Miltenburg2019Pragmatic,
  author   = {{van Miltenburg}, Emiel},
  school   = {Vrije Universiteit Amsterdam},
  title    = {Pragmatic factors in (automatic) image description},
  year     = {2019},
  month    = oct,
  day      = {14},
  file     = {:PDF/phdthesis.pdf:PDF},
  language = {English},
}

@Book{Horn2012Matrix,
  author    = {Horn, Roger A and Johnson, Charles R},
  publisher = {Cambridge university press},
  title     = {Matrix analysis},
  year      = {2012},
  file      = {:PDF/Roger_A.Horn. _Matrix_Analysis_2nd_edition(BookSee.org).pdf:PDF},
}

@Article{Kam2025BenchmarkingAttention,
  author        = {Hemanth Teja Yanambakkam and Rahul Chinthala},
  title         = {Beyond RNNs: Benchmarking Attention-Based Image Captioning Models},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2502.18734},
  file          = {:PDF/2502.18734v1.pdf:PDF},
  groups        = {attention},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2502.18734},
}

@Article{Szalkiewicz2013Lematyzacja,
  author       = {Szałkiewicz, Ł.},
  journal      = {Polonica},
  title        = {Lematyzacja w ręcznej anotacji milionowego podkorpusu Narodowego Korpusu Języka Polskiego — ciekawe przypadki},
  year         = {2013},
  month        = {grudz.},
  pages        = {133–156},
  volume       = {33},
  file         = {:PDF/lematyzacja_szalkiewicz.pdf:PDF},
  groups       = {opis polskiego},
  url          = {https://polonica.ijppan.pl/index.php/polonica/article/view/663},
}

@Article{Bartosiewicz2024Improving,
  author = {Mateusz Bartosiewicz and Marcin Iwanowski and Piotr Szczepanski and Karol Zielinski and Albert Ziolkiewicz},
  title  = {Improving the efficiency of "Show and Tell" encoder decoder image captioning model},
  year   = {2024},
  file   = {:PDF/Captioning_ICCVG.pdf:PDF},
  groups = {moje},
}

@inproceedings{French1993CatastrophicInference,
author = {French, Robert M.},
title = {Catastrophic interference in connectionist networks: can it be predicted, can it be prevented?},
year = {1993},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Catastrophic forgetting occurs when connectionist networks learn new information, and by so doing, forget all previously learned information. This workshop focused primarily on the causes of catastrophic interference, the techniques that have been developed to reduce it, the effect of these techniques on the networks' ability to generalize, and the degree to which prediction of catastrophic forgetting is possible. The speakers were Robert French, Phil Hetherington (Psychology Department, McGill University, het@blaise.psych.mcgill.ca), and Stephan Lewandowsky (Psychology Department, University of Oklahoma, lewan@constellation.ecn.uoknor.edu).},
booktitle = {Proceedings of the 7th International Conference on Neural Information Processing Systems},
pages = {1176–1177},
numpages = {2},
location = {Denver, Colorado},
series = {NIPS'93}
}

@Book{Manning2008IntroductionToInformationRetrieval,
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  publisher = {Cambridge University Press},
  title     = {Introduction to Information Retrieval},
  year      = {2008},
  address   = {USA},
  isbn      = {0521865719},
  abstract  = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
  file      = {:PDF/irbookprint.pdf:PDF},
  groups    = {lm},
}

@InCollection{Gordon1989Catastrophic,
  author    = {Michael McCloskey and Neal J. Cohen},
  publisher = {Academic Press},
  title     = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  year      = {1989},
  editor    = {Gordon H. Bower},
  pages     = {109-165},
  series    = {Psychology of Learning and Motivation},
  volume    = {24},
  abstract  = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
  doi       = {https://doi.org/10.1016/S0079-7421(08)60536-8},
  issn      = {0079-7421},
  url       = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
}

@Misc{Luo2025EmpiricalEtudyCatastrophicForgetting,
  author        = {Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
  title         = {An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2308.08747},
  file          = {:PDF/2308.08747v5.pdf:PDF},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2308.08747},
}

@Article{Li1994MarkovMRFinCV,
  author    = {Li, S. Z.},
  title     = {Markov random field models in computer vision},
  year      = {1994},
  pages     = {361--370},
  abstract  = {A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles.},
  address   = {Berlin, Heidelberg},
  booktitle = {Computer Vision --- ECCV '94},
  editor    = {Eklundh, Jan-Olof},
  file      = {:PDF/BFb0028368.pdf:PDF},
  isbn      = {978-3-540-48400-4},
  publisher = {Springer Berlin Heidelberg},
}

@Article{wittgenstein1997traktat,
  author  = {Wittgenstein, Ludwig},
  journal = {Prze{\l}. i wst{\k{e}}pem opatrzy{\l} Bogus{\l}aw Wolniewicz. Warszawa. Wydawnictwo Naukowe PWN},
  title   = {Traktat logiczno-filozoficzny},
  year    = {1997},
  file    = {:PDF/filozofia-ludwig-wittgenstein-traktatus-logico-philosophicus.pdf:PDF},
  groups  = {wstep},
}

@Book{floridi2011philosophy,
  author    = {Floridi, L.},
  publisher = {OUP Oxford},
  title     = {The Philosophy of Information},
  year      = {2011},
  isbn      = {9780199232383},
  comment   = {Dla Floridiego, Wittgensteinowska idea, że zdanie sensowne musi mieć precyzyjnie określone warunki prawdziwości, jest kluczowa dla zrozumienia, jak systemy informatyczne przetwarzają dane. Bada on, jak „niedorzeczność" (w sensie Wittgensteina) pojawia się w systemach AI jako błąd semantyczny -- gdy system otrzymuje dane, dla których nie ma zdefiniowanej interpretacji.},
  file      = {:PDF/[Luciano_Floridi]_The_Philosophy_of_Information-0199232385.pdf:PDF},
  groups    = {wstep},
  lccn      = {2010940315},
  url       = {https://books.google.pl/books?id=4d0TDAAAQBAJ},
}

@Book{winograd1986understanding,
  author    = {Winograd, T. and Flores, F.},
  publisher = {Ablex Publishing Corporation},
  title     = {Understanding Computers and Cognition: A New Foundation for Design},
  year      = {1986},
  isbn      = {9780893910501},
  series    = {Language and being},
  file      = {:PDF/Understanding Computers and Cognition_A New Foundation for Design - Terry Winograd.pdf:PDF},
  groups    = {wstep},
  lccn      = {lc85003856},
  url       = {https://books.google.pl/books?id=2sRC8vcDYNEC},
}

@Book{lacey2002dictionary,
  author    = {Lacey, A.},
  publisher = {Taylor \& Francis},
  title     = {Dictionary of Philosophy},
  year      = {1996},
  edition   = {3},
  isbn      = {9781134785858},
  file      = {:PDF/1539210339 - [Alan_Lacey]_A_Dictionary_Of_Philosophy(Bookos.org).pdf:PDF},
  pages     = {64-65},
  url       = {https://staffsites.sohag-univ.edu.eg/uploads/818/1539210339%20-%20%5BAlan_Lacey%5D_A_Dictionary_Of_Philosophy(Bookos.org).pdf},
}

@Article{Kaplan2020ScalingLF,
  author  = {Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
  journal = {ArXiv},
  title   = {Scaling Laws for Neural Language Models},
  year    = {2020},
  volume  = {abs/2001.08361},
  file    = {:PDF/2001.08361v1.pdf:PDF},
  groups  = {wstep},
  url     = {https://api.semanticscholar.org/CorpusID:210861095},
}

@Misc{henighan2020scalinglawsautoregressivegenerative,
  author        = {Tom Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish},
  title         = {Scaling Laws for Autoregressive Generative Modeling},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2010.14701},
  file          = {:PDF/2010.14701v2.pdf:PDF},
  groups        = {wstep},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2010.14701},
}

@Article{Yang2017ImageCW,
  author     = {Zhongliang Yang and Yu{-}Jin Zhang and Sadaqat ur Rehman and Yongfeng Huang},
  journal    = {CoRR},
  title      = {Image Captioning with Object Detection and Localization},
  year       = {2017},
  volume     = {abs/1706.02430},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/YangZRH17.bib},
  eprint     = {1706.02430},
  eprinttype = {arXiv},
  file       = {:PDF/Image Captioning with Object Detection and Localization.pdf:PDF},
  timestamp  = {Tue, 09 Mar 2021 14:31:00 +0100},
  url        = {http://arxiv.org/abs/1706.02430},
}

@InProceedings{Simonyan15VeryDeep,
  author    = {Karen Simonyan and Andrew Zisserman},
  booktitle = {International Conference on Learning Representations},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year      = {2015},
  file      = {:PDF/simonyan15.pdf:PDF},
}

@InProceedings{Macleod2017Understanding,
  author    = {MacLeod, Haley and Bennett, Cynthia L and Morris, Meredith Ringel and Cutrell, Edward},
  booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  title     = {Understanding blind people's experiences with computer-generated captions of social media images},
  year      = {2017},
  pages     = {5988--5999},
}

@InProceedings{Yang2016,
  author    = {Yang, Zhilin and Yuan, Ye and Wu, Yuexin and Cohen, William W. and Salakhutdinov, Ruslan R.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  title     = {Review networks for caption generation},
  year      = {2016},
  address   = {Red Hook, NY, USA},
  pages     = {2369–2377},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'16},
  abstract  = {We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.},
  file      = {:PDF/nips-2016.pdf:PDF},
  groups    = {single layer LSTM, Review networks},
  isbn      = {9781510838819},
  location  = {Barcelona, Spain},
  numpages  = {9},
  url       = {https://proceedings.neurips.cc/paper/2016/file/9996535e07258a7bbfd8b132435c5962-Paper.pdf},
}

@Article{Beyer2020Transformers,
  author     = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal    = {CoRR},
  title      = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year       = {2020},
  volume     = {abs/2010.11929},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  eprint     = {2010.11929},
  eprinttype = {arXiv},
  file       = {:PDF/2010.11929v2.pdf:PDF},
  groups     = {Vision Transformer., attention taxonomy},
  timestamp  = {Fri, 20 Nov 2020 14:04:05 +0100},
  url        = {https://arxiv.org/abs/2010.11929},
}

@InProceedings{Gu2018StackCaptioning,
  author    = {Gu, Jiuxiang and Cai, Jianfei and Wang, Gang and Chen, Tsuhan},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  title     = {Stack-captioning: coarse-to-fine learning for image captioning},
  year      = {2018},
  publisher = {AAAI Press},
  series    = {AAAI'18/IAAI'18/EAAI'18},
  abstract  = {The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.},
  articleno = {837},
  file      = {:PDF/3504035.3504872.pdf:PDF},
  groups    = {Other deep learning methods, Additive attention over a grid of features, Multi-stage generation, attention taxonomy},
  isbn      = {978-1-57735-800-8},
  location  = {New Orleans, Louisiana, USA},
  numpages  = {8},
}

@InProceedings{Koehn2005Europarl,
  author    = {Koehn, Philipp},
  booktitle = {Proceedings of Machine Translation Summit X: Papers},
  title     = {{E}uroparl: A Parallel Corpus for Statistical Machine Translation},
  year      = {2005},
  address   = {Phuket, Thailand},
  month     = sep # { 13-15},
  pages     = {79--86},
  abstract  = {We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.},
  file      = {:PDF/2005.mtsummit-papers.11.pdf:PDF},
  url       = {https://aclanthology.org/2005.mtsummit-papers.11},
}

@InProceedings{7410648,
  author    = {Mao, Junhua and Wei, Xu and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images},
  year      = {2015},
  month     = {Dec},
  pages     = {2533-2541},
  abstract  = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on [38] with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task, and are publicly available on the project page. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is: www.stat.ucla.edu/junhua. mao/projects/child_learning.html.},
  doi       = {10.1109/ICCV.2015.291},
  file      = {:PDF/Learning_Like_a_Child_Fast_Novel_Visual_Concept_Learning_from_Sentence_Descriptions_of_Images.pdf:PDF},
  groups    = {review},
  issn      = {2380-7504},
  keywords  = {Adaptation models;Visualization;Semantics;Dictionaries;Computer vision;Computational modeling;Training},
}

@InProceedings{Grave2018WordWectors,
  author    = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  title     = {Learning Word Vectors for 157 Languages},
  year      = {2018},
  address   = {Miyazaki, Japan},
  editor    = {Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu},
  month     = may,
  publisher = {European Language Resources Association (ELRA)},
  file      = {:PDF/L18-1550.pdf:PDF},
  url       = {https://aclanthology.org/L18-1550/},
}

@Book{Goodfellow2018DL,
  author   = {Goodfellow, Ian},
  title    = {Deep Learning Wspolczesne systemy uczace sie},
  year     = {2018},
  language = {pl},
}

@Article{Chai_2023,
  author  = {Chai, Christine P.},
  journal = {Natural Language Engineering},
  title   = {Comparison of text preprocessing methods},
  year    = {2023},
  number  = {3},
  pages   = {509–553},
  volume  = {29},
  doi     = {10.1017/S1351324922000213},
  groups  = {preprocessing},
}

@Article{Kilimci2020Sentiment,
  author       = {Kilimci, Zeynep Hilal},
  journal      = {International Journal of Intelligent Systems and Applications in Engineering},
  title        = {Sentiment Analysis Based Direction Prediction in Bitcoin using Deep Learning Algorithms and Word Embedding Models},
  year         = {2020},
  month        = {Jun.},
  number       = {2},
  pages        = {60–65},
  volume       = {8},
  abstractnote = {Sentiment analysis is a considerable research field to analyze huge amount of information and specify user opinions on many things and is summarized as the extraction of users’ opinions from the text. Like sentiment analysis, Bitcoin which is a digital cryptocurrency also attracts the researchers considerably in the fields of economics, cryptography, and computer science. The purpose of this study is to forecast the direction of Bitcoin price by analysing user opinions in social media such as Twitter. To our knowledge, this is the very first attempt which estimates the direction of Bitcoin price fluctuations by using deep learning and word embedding models in the state-of-the-art studies. For the purpose of estimating the direction of Bitcoin, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long-short term memory networks (LSTMs) are used as deep learning architectures and Word2Vec, GloVe, and FastText are employed as word embedding models in the experiments. In order to demonstrate the contibution of our work, experiments are carried out on English Twitter dataset. Experiment results show that the usage of FastText model as a word embedding model outperforms other models with 89.13% accuracy value to estimate the direction of Bitcoin price.},
  doi          = {10.18201/ijisae.2020261585},
  file         = {:Fuzzy.bib (conflicted copy 2021-09-15 142929).sav:sav;:PDF/Sentiment Analysis Based DirectionPrediction in Bitcoin using Deep Learning Algorithms and Word Embedding Models.pdf:PDF},
  url          = {https://ijisae.org/index.php/IJISAE/article/view/1062},
}

@Article{Vinyals2017ShowTellLessonsLearned,
  author    = {O. Vinyals and A. Toshev and S. Bengio and D. Erhan},
  journal   = {IEEE Transactions on Pattern Analysis Machine Intelligence},
  title     = {Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},
  year      = {2017},
  issn      = {1939-3539},
  month     = {apr},
  number    = {04},
  pages     = {652-663},
  volume    = {39},
  abstract  = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/TPAMI.2016.2587640},
  file      = {:PDF/07505636.pdf:PDF},
  keywords  = {logic gates;training;recurrent neural networks;visualization;computer vision;computational modeling;natural languages},
  publisher = {IEEE Computer Society},
}

@Article{Mazur2024Poprawnosc,
  author       = {Mazur, Rafał},
  journal      = {LingVaria},
  title        = {O poprawności językowej tekstów generowanych przez SI na przykładzie ChatuGPT},
  year         = {2024},
  month        = {maj},
  number       = {1(37)},
  pages        = {119–138},
  volume       = {19},
  abstractnote = {LANGUAGE ACCURACY OF TEXTS GENERATED BY AI: A CASE STUDY OF CHATGPT The article aims to assess the linguistic accuracy of texts generated by ChatGPT, based on queries that mirror the instructions found in the 2023 secondary school final exam in Polish language and literature. The analysis focused on identifying and classifying language errors and evaluating their frequency. The results of the analysis revealed a varied level of accuracy in AI-generated texts. Most errors occurred in texts containing more complex constructions, where correct formulation required a comprehensive understanding of grammatical rules. The conclusions drawn from the analysis offer insights into the limitations of AI, which users should be mindful of when editing texts using chatbots.},
  doi          = {10.12797/LV.19.2024.37.08},
  file         = {:PDF/09-Mazur-O-poprawnosci-jezykowej-tekstow-generowanych-przez-SI-na-przykladzie-chatuGPT.pdf:PDF},
  groups       = {bledy w SI},
  url          = {https://journals.akademicka.pl/lv/article/view/5756},
}

@Article{Ogrodniczuk2017Lingwistyka,
  author       = {Ogrodniczuk, Maciej},
  journal      = {Język Polski},
  title        = {Lingwistyka komputerowa dla języka polskiego: dziś i jutro},
  year         = {2017},
  month        = {mar.},
  number       = {1},
  pages        = {18–28},
  volume       = {97},
  abstractnote = {Tekst jest publicystyczną próbą nakreślenia dalszych kierunków prac nad komputerowym przetwarzaniem polszczyzny w obliczu intensywnego rozwoju cyfrowych narzędzi i zasobów dla języka polskiego oraz zacieśniającej się współpracy między polskimi ośrodkami badawczymi zajmującymi się lingwistyką komputerową. Za najważniejszy temat autor uważa wznowienie prac nad korpusem narodowym, który jako zasób podstawowy dla językoznawstwa polskiego wymaga stałego poszerzania bazy materiałowej i opisu lingwistycznego, włączenia podkorpusów diachronicznych, gwarowych i równoległych. W sferze technologii językowej autor postuluje wzbogacenie formalnego opisu polszczyzny o głęboki poziom składniowy, semantykę i dyskurs oraz zwraca uwagę na konieczność stałego poprawiania jakości dostępnych narzędzi i zasobów metodą współpracy środowiska językoznawczego z informatycznym.},
  doi          = {10.31286/JP.97.1.3},
  file         = {:PDF/Lingwistyka_komputerowa_dla_jezyka_polskiego_dzis_.pdf:PDF},
  groups       = {opis polskiego},
  url          = {https://jezyk-polski.pl/index.php/jp/article/view/449},
}

@Article{Kieraś_Woliński_2017,
  author       = {Kieraś, Witold and Woliński, Marcin},
  journal      = {Język Polski},
  title        = {Morfeusz 2 – analizator i generator fleksyjny dla języka polskiego},
  year         = {2017},
  month        = {mar.},
  number       = {1},
  pages        = {75–83},
  volume       = {97},
  abstractnote = {Morfeusz jest aplikacją znaną polskim badaczom z kręgu językoznawstwa komputerowego od ponad 10 lat. W artykule przedstawiamy jego nową wersję, skupiając się na zmianach, które pojawiły się od poprzedniej wersji programu.},
  doi          = {10.31286/JP.97.1.7},
  file         = {:PDF/[1_2017]77.pdf:PDF},
  groups       = {opis polskiego},
  url          = {https://jezyk-polski.pl/index.php/jp/article/view/453},
}

@Article{Urzędowska2024PoprawnoscJezykowa,
  author       = {Urzędowska, Aleksandra},
  journal      = {Annales Universitatis Paedagogicae Cracoviensis | Studia de Cultura},
  title        = {Sztuczna inteligencja a inteligencja językowa. Eksperyment poprawnościowy czatu GPT-4},
  year         = {2024},
  month        = {cze.},
  number       = {2},
  pages        = {71–84},
  volume       = {16},
  abstractnote = {Tekst podejmuje próbę oceny poprawności językowej algorytmów lingwistycznych czatu GPT-4. Przeprowadzone i opisane eksperymenty odnoszą się do opisu funkcjonalności czatu jako wysoko inteligentnej pod względem języka w perspektywie poprawności, sprawności i kultury językowej. Przez odniesienie do gardnerowskiego rozumienia inteligencji językowej oraz jego krytyków opisano umiejętności korzystania ze znormalizowanego języka przez sztuczną inteligencję, a także oznaczono błędy w danych i procesie kodowania modeli językowych w czacie GPT-4.},
  doi          = {10.24917/20837275.16.2.5},
  file         = {:PDF/3442188.3445922.pdf:PDF},
  groups       = {bledy w SI},
  url          = {https://studiadecultura.uken.krakow.pl/article/view/11305},
}

@InProceedings{He2015Deep,
  author    = {K. He and X. Zhang and S. Ren and J. Sun},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {770-778},
  publisher = {IEEE Computer Society},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  doi       = {10.1109/CVPR.2016.90},
  file      = {:PDF/ResNet_He2015.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {training;degradation;complexity theory;image recognition;neural networks;visualization;image segmentation},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:embeddings\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:inception\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:relations\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:graf\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:resnet\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:review\;0\;0\;0x8a8a8aff\;\;\;;
2 StaticGroup:wazne_wazne\;0\;1\;0x7c7c7cff\;\;\;;
1 StaticGroup:vgg16\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:word2vec\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:transformer\;0\;1\;\;\;\;;
1 StaticGroup:datasets\;0\;1\;\;\;\;;
1 StaticGroup:metryki\;0\;1\;\;\;\;;
1 StaticGroup:literature review 2\;0\;0\;\;\;\;;
2 StaticGroup:retrieval\;0\;0\;\;\;\;;
2 StaticGroup:template\;0\;1\;\;\;\;;
2 StaticGroup:Encoding\;0\;1\;\;\;\;;
3 StaticGroup:global CNN features\;0\;1\;\;\;\;;
3 StaticGroup:Attention Over Grid of CNN Features\;0\;1\;\;\;\;;
4 StaticGroup:Additive attention over a grid of features\;0\;0\;\;\;\;;
4 StaticGroup:Exploiting human attention\;0\;1\;\;\;\;;
4 StaticGroup:Multi-level features\;0\;0\;\;\;\;;
4 StaticGroup:Review networks\;0\;1\;\;\;\;;
3 StaticGroup:Attention Over Visual Regions\;0\;0\;\;\;\;;
4 StaticGroup:Visual Policy\;0\;1\;\;\;\;;
4 StaticGroup:Geometric Transforms\;0\;1\;\;\;\;;
3 StaticGroup:Graph-based Encoding\;0\;0\;\;\;\;;
4 StaticGroup:Spatial and semantic graphs.\;0\;1\;\;\;\;;
4 StaticGroup:Scene graphs\;0\;1\;\;\;\;;
4 StaticGroup:Hierarchical trees\;0\;1\;\;\;\;;
3 StaticGroup:Self-Attention Encoding\;0\;0\;\;\;\;;
4 StaticGroup:Early self-attention approaches\;0\;0\;\;\;\;;
4 StaticGroup:Other\;0\;1\;\;\;\;;
4 StaticGroup:Vision Transformer.\;0\;0\;\;\;\;;
4 StaticGroup:Early fusion and vision-and-language pre-training.\;0\;0\;\;\;\;;
2 StaticGroup:Language Model\;0\;1\;\;\;\;;
3 StaticGroup:LSTM\;0\;1\;\;\;\;;
4 StaticGroup:single layer LSTM\;0\;0\;\;\;\;;
5 StaticGroup:Visual sentinel\;0\;1\;\;\;\;;
5 StaticGroup:hidden state reconstruction\;0\;0\;\;\;\;;
5 StaticGroup:Multi-stage generation\;0\;1\;\;\;\;;
4 StaticGroup:Two-layer LSTM\;0\;0\;\;\;\;;
5 StaticGroup:Two-layers and additive attention\;0\;1\;\;\;\;;
5 StaticGroup:Reflective attention\;0\;1\;\;\;\;;
4 StaticGroup:Boosting LSTM with Self-Attention\;0\;0\;\;\;\;;
4 StaticGroup:Neural Architecture Search for RNN\;0\;1\;\;\;\;;
3 StaticGroup:CNN language models\;0\;0\;\;\;\;;
3 StaticGroup:Transformer\;0\;0\;\;\;\;;
4 StaticGroup:Gating mechanisms.\;0\;1\;\;\;\;;
3 StaticGroup:BERT\;0\;1\;\;\;\;;
3 StaticGroup:Non-autoregressive Language Models\;0\;1\;\;\;\;;
2 StaticGroup:attention taxonomy\;0\;0\;\;\;\;;
1 StaticGroup:bez atencji\;0\;0\;\;\;\;;
1 StaticGroup:literature review\;0\;0\;\;\;\;;
2 StaticGroup:Earlier Deep Models\;0\;1\;\;\;\;;
2 StaticGroup:multimodal\;0\;1\;\;\;\;;
2 StaticGroup:encoder-decoder-lit\;0\;1\;\;\;\;;
2 StaticGroup:Compositional architectures\;0\;0\;\;\;\;;
2 StaticGroup:Describing novel objects\;0\;1\;\;\;\;;
2 StaticGroup:Other deep learning methods\;0\;0\;\;\;\;;
1 StaticGroup:opis polskiego\;0\;0\;\;\;\;;
1 StaticGroup:bledy w SI\;0\;0\;\;\;\;;
1 StaticGroup:representation learning\;0\;0\;\;\;\;;
1 StaticGroup:moje\;0\;1\;\;\;\;;
1 StaticGroup:Probabilistic Neural Network Language Model\;0\;0\;\;\;\;;
1 StaticGroup:rnn\;0\;1\;\;\;\;;
2 StaticGroup:lstm\;0\;1\;\;\;\;;
1 StaticGroup:cnn\;0\;1\;\;\;\;;
1 StaticGroup:mix\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:beam\;0\;1\;\;\;\;;
1 StaticGroup:preprocessing\;0\;1\;\;\;\;;
1 StaticGroup:lm\;0\;1\;\;\;\;;
1 StaticGroup:attention\;0\;1\;\;\;\;;
1 StaticGroup:negacja\;0\;1\;\;\;\;;
1 StaticGroup:wstep\;0\;1\;\;\;\;;
}
